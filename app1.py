import os
import uuid
import re
import json
import string
import random
from fastapi import FastAPI, UploadFile, File, Request, Query, HTTPException
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import openai
import requests
import io
import base64
import edge_tts
from PyPDF2 import PdfReader
import tiktoken
from runware import Runware, IImageInference
import urllib.parse
import httpx
import aiohttp
import pprint
import asyncio
from werkzeug.utils import secure_filename
from PIL import Image
import pytesseract
from fastapi import FastAPI, HTTPException, Path
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Literal
from google.cloud import firestore


# Inline RAG helper functions (formerly in backend/rag_text_response_image56.py)
try:
    import firebase_admin
    from firebase_admin import credentials, firestore, storage
except ImportError as e:
    raise RuntimeError(
        "Missing 'firebase_admin' dependency: please install via `pip install firebase-admin google-cloud-firestore google-cloud-storage`"
    ) from e
import shutil
import time
from pathlib import Path as FilePath
import base64
import logging
from concurrent.futures import ThreadPoolExecutor
import boto3
from botocore.exceptions import ClientError
def get_firebase_secret_from_aws(secret_name, region_name):
    """Fetch Firebase credentials JSON from AWS Secrets Manager."""
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    try:
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    except ClientError as e:
        raise RuntimeError(f"Error retrieving secret: {e}")
    secret = get_secret_value_response['SecretString']
    return json.loads(secret)

# LangChain & FAISS imports for curriculum RAG
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredImageLoader
from langchain_community.vectorstores import FAISS

# Initialize logging
logging.basicConfig(level=logging.DEBUG)

# Firebase setup (ensure service account JSON is present alongside app1.py)
# Firebase setup (loads from FIREBASE_JSON env var if available, else from file)


if not firebase_admin._apps:
    FIREBASE_JSON = os.getenv("FIREBASE_JSON")
    if FIREBASE_JSON:
        print("[INFO] Loading Firebase credentials from FIREBASE_JSON environment variable.")
        cred = credentials.Certificate(json.loads(FIREBASE_JSON))
    else:
        # Fetch from AWS Secrets Manager
        print("[INFO] Loading Firebase credentials from AWS Secrets Manager.")
        secret_name = "firebase-service-account-json"    # The secret name you used
        region_name = "ap-south-1"                       # Your AWS region
        firebase_creds = get_firebase_secret_from_aws(secret_name, region_name)
        cred = credentials.Certificate(firebase_creds)
    firebase_admin.initialize_app(cred, {"storageBucket": "aischool-ba7c6.appspot.com"})

# Firestore & Storage clients
db = firestore.client()
bucket = storage.bucket()

# ThreadPoolExecutor for any blocking calls
executor = ThreadPoolExecutor()

# In-memory FAISS vector cache per curriculum
curriculum_vectors = {}

async def get_curriculum_url(curriculum):
    """Fetch curriculum PDF URL from Firestore."""
    doc = await asyncio.to_thread(lambda: db.collection('curriculum').document(curriculum).get())
    if not doc.exists:
        raise ValueError(f"No curriculum found with ID: {curriculum}")
    return doc.to_dict().get('url')

async def fetch_chat_detail(chat_id):
    """Fetch and format the last 3 QA pairs of chat history."""
    chat_ref = db.collection('chat_detail').document(chat_id)
    chat_doc = chat_ref.get()
    if not chat_doc.exists:
        return ""
    data = chat_doc.to_dict()
    history = data.get('history', [])
    users = [e for e in history if e.get('role') in ['2','3']]
    bots = [e for e in history if e.get('role') == 'assistant']
    pairs = list(zip(users, bots))
    recent = pairs[-3:]
    out = ''
    for u,a in recent:
        out += f"My Question: {u.get('content','')}\nBot Response: {a.get('content','')}\n"
    return out

async def get_chat_history(chat_id):
    """Alias to fetch formatted chat history for RAG context."""
    return await fetch_chat_detail(chat_id)

async def get_user_name(user_id: str) -> str:
    """
    Fetch the user's display name from Firestore users collection.
    Returns empty string if not found or on error.
    """
    try:
        doc = await asyncio.to_thread(lambda: db.collection('users').document(user_id).get())
        if doc.exists:
            return doc.to_dict().get('name', '') or ''
    except Exception:
        pass
    return ''

async def check_index_in_bucket(curriculum):
    blob = bucket.blob(f'users/KnowledgeBase/faiss_index_{curriculum}/index.faiss')
    return blob.exists()

async def download_index_from_bucket(curriculum):
    dest = FilePath(f'faiss/faiss_index_{curriculum}')
    dest.mkdir(parents=True, exist_ok=True)
    await asyncio.to_thread(
        bucket.blob(f'users/KnowledgeBase/faiss_index_{curriculum}/index.faiss').download_to_filename,
        str(dest / 'index.faiss')
    )
    await asyncio.to_thread(
        bucket.blob(f'users/KnowledgeBase/faiss_index_{curriculum}/index.pkl').download_to_filename,
        str(dest / 'index.pkl')
    )

async def upload_index_to_bucket(curriculum):
    await asyncio.to_thread(
        bucket.blob(f'users/KnowledgeBase/faiss_index_{curriculum}/index.faiss').upload_from_filename,
        f'faiss/faiss_index_{curriculum}/index.faiss'
    )
    await asyncio.to_thread(
        bucket.blob(f'users/KnowledgeBase/faiss_index_{curriculum}/index.pkl').upload_from_filename,
        f'faiss/faiss_index_{curriculum}/index.pkl'
    )

async def download_file(url, curriculum):
    ext = url.split('.')[-1]
    path = f'curriculum_{curriculum}.{ext}'
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            if resp.status == 200:
                with open(path, 'wb') as f:
                    f.write(await resp.read())
            else:
                raise IOError(f"Download failed: {resp.status}")
    return path

async def vector_embedding(curriculum, file_url):
    embeddings = OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY'))
    idx_dir = f'faiss/faiss_index_{curriculum}'
    if await check_index_in_bucket(curriculum):
        await download_index_from_bucket(curriculum)
        return FAISS.load_local(idx_dir, embeddings, allow_dangerous_deserialization=True)
    file_path = await download_file(file_url, curriculum)
    ext = FilePath(file_path).suffix.lower()
    if ext == '.pdf':
        loader = PyPDFLoader(file_path)
    elif ext in ('.doc', '.docx'):
        loader = Docx2txtLoader(file_path)
    elif ext in ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'):
        loader = UnstructuredImageLoader(file_path)
    else:
        raise ValueError(f"Unsupported file type for RAG: {ext}")
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=MAX_TOKENS_PER_CHUNK, chunk_overlap=0)
    final_docs = splitter.split_documents(docs)
    vectors = FAISS.from_documents(final_docs, embeddings)
    vectors.save_local(idx_dir)
    await upload_index_to_bucket(curriculum)
    os.remove(file_path)
    return vectors

async def get_or_load_vectors(curriculum, pdf_url):
    """Optimized FAISS vector caching and retrieval."""
    if curriculum in curriculum_vectors:
        return curriculum_vectors[curriculum]
    idx_dir = f'faiss/faiss_index_{curriculum}'
    if os.path.exists(idx_dir) and os.path.exists(f"{idx_dir}/index.faiss"):
        vectors = await asyncio.to_thread(
            FAISS.load_local,
            idx_dir,
            OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY')),
            allow_dangerous_deserialization=True,
        )
    else:
        vectors = await vector_embedding(curriculum, pdf_url)
    curriculum_vectors[curriculum] = vectors
    return vectors

async def retrieve_documents(vectorstore, query: str, max_tokens: int = 7000, k: int = 10):
    """Fetch and trim top-k docs by token count."""
    docs = await vectorstore.asimilarity_search(query, k=k)
    total = 0
    out = []
    encoder = tiktoken.encoding_for_model('gpt-4')
    for d in docs:
        nt = len(encoder.encode(d.page_content))
        if total + nt <= max_tokens:
            out.append(d)
            total += nt
        else:
            break
    return out

async def update_chat_history_speech(user_id, question, answer):
    """Append QA pair to Firestore speech history."""
    ref = db.collection('history_chat_backend_speech').document(user_id)
    doc = ref.get()
    hist = doc.to_dict().get('history', []) if doc.exists else []
    hist.append({'question': question, 'answer': answer})
    ref.set({'history': hist})

# --- Matplotlib for graphing ---
import matplotlib
from dotenv import load_dotenv
load_dotenv()
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import shutil

# --- System-level LaTeX prohibition (always enforced) ---
SYSTEM_LATEX_BAN = (
    "STRICT RULE: Do NOT generate any LaTeX code or markup (\\frac, \\left, \\right, $...$). "
    "Only use plain-text stacked fractions when required.\n"
)


# OpenAI-style TTS voice names mapped to Edge TTS voices
OPENAI_TO_EDGE_VOICE = {
    "alloy": "en-US-DavisNeural",
    "shimmer": "en-US-JennyNeural",
    "nova": "en-US-GuyNeural",
    "echo": "en-GB-RyanNeural",
    "fable": "en-AU-NatashaNeural",
    "onyx": "en-US-ChristopherNeural",
    # Arabic mappings (see below)
    "alloy-arabic": "ar-SA-HamedNeural",   # Closest to alloy for Arabic (male, clear, neutral)
    "shimmer-arabic": "ar-SA-ZariyahNeural"  # For shimmer in Arabic (female)
}


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
MAX_TOKENS_PER_CHUNK = 100_000
RUNWARE_API_KEY = os.getenv("RUNWARE_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
openai.api_key = OPENAI_API_KEY
tokenizer = tiktoken.encoding_for_model("gpt-4")
AUDIO_DIR = "audio_sents"
os.makedirs(AUDIO_DIR, exist_ok=True)





# üîµ IMAGE GENERATION KEYWORDS
IMAGE_SYNONYMS = [
    "*GENERATION*", "*Generation*", "*ÿ¨ŸäŸÑ*", "*ÿ•ŸÜÿ¥ÿßÿ°*"
]

# üü¢ GRAPH GENERATION KEYWORDS
GRAPH_SYNONYMS = [

    '"GRAPH"',
    '"PLOT"',
    '"GRAPH_CALCULATOR"',
    '"GRAPHING"',
    '"ÿ±ÿ≥ŸÖ ÿ®ŸäÿßŸÜŸä"',
    '"ÿ¢ŸÑÿ© ÿßŸÑÿ±ÿ≥ŸÖ"',
    '"ÿ≠ÿßÿ≥ÿ®ÿ© ÿßŸÑÿ±ÿ≥ŸàŸÖ"',
    "GRAPH",
    "PLOT",
    "GRAPH_CALCULATOR",
    "GRAPHING",
    "ÿ±ÿ≥ŸÖ ÿ®ŸäÿßŸÜŸä",
    "ÿ¢ŸÑÿ© ÿßŸÑÿ±ÿ≥ŸÖ",
    "ÿ≠ÿßÿ≥ÿ®ÿ© ÿßŸÑÿ±ÿ≥ŸàŸÖ"
]

# üü† WEB/WEBLINK KEYWORDS
WEB_SYNONYMS = [
    "*internet*", "*web*", "*ÿ•ŸÜÿ™ÿ±ŸÜÿ™*", "*ÿßŸÑŸàŸäÿ®*"
]

app = FastAPI()

# --- CORS middleware setup ---
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/audio", StaticFiles(directory=AUDIO_DIR), name="audio")
app.mount("/frontend", StaticFiles(directory="frontend", html=True), name="frontend")
# --- Graphs directory setup and static mount ---
GRAPHS_DIR = "graphs"
os.makedirs(GRAPHS_DIR, exist_ok=True)
app.mount("/graphs", StaticFiles(directory=GRAPHS_DIR), name="graphs")
# --- Uploads directory setup and static mount for serving user-uploaded files ---
os.makedirs("uploads", exist_ok=True)
app.mount("/uploads", StaticFiles(directory="uploads"), name="uploads")
# Base URL where uploaded files (PDF, image, audio) will be served
UPLOADS_BASE_URL = "http://51.20.81.94:8000/uploads"
# Base URL where generated graphs will be served
GRAPHS_BASE_URL = "http://51.20.81.94:8000/graphs"

def local_path_from_image_url(image_url):
    """
    If image_url points under our /uploads/ static mount, return the local file path;
    otherwise return None.
    """
    if image_url and image_url.startswith(UPLOADS_BASE_URL + "/"):
        filename = image_url.split(UPLOADS_BASE_URL + "/", 1)[1]
        local_path = os.path.join("uploads", filename)
        if os.path.exists(local_path):
            return local_path
    return None
def generate_matplotlib_graph(prompt):
    """
    Tries to extract a function or equation from the prompt, plots it with matplotlib,
    saves to a file in graphs/ with a UUID, and returns the URL to the static file.
    """
    import re
    import uuid
    import numpy as np
    import matplotlib.pyplot as plt
    import os

    # Basic function extraction: look for y = ... or plot f(x) = ...
    expr = None
    # Try to find y = ... or f(x) = ... or "plot ..." or "draw ..."
    patterns = [
        r'y\s*=\s*([^\s,;]+)',             # y = ...
        r'f\s*\(\s*x\s*\)\s*=\s*([^\s,;]+)', # f(x) = ...
        r'plot\s+([^\s,;]+)',              # plot ...
        r'draw\s+([^\s,;]+)',              # draw ...
        r'graph\s+([^\s,;]+)',             # graph ...
    ]
    for pat in patterns:
        m = re.search(pat, prompt, re.IGNORECASE)
        if m:
            expr = m.group(1)
            break

    # If not found, look for something that looks like a math expr
    if expr is None:
        # Try to find something like "sin(x)", "x^2+3*x-2", etc.
        m = re.search(r'([a-zA-Z0-9_\+\-\*/\^\.]+\(x\)[^\s,;]*)', prompt)
        if m:
            expr = m.group(1)
    if expr is None:
        # fallback: try to find something like "x^2", "sin(x)", etc.
        m = re.search(r'([a-zA-Z0-9_\+\-\*/\^\.]+)', prompt)
        if m:
            expr = m.group(1)

    # Clean up expr, replace ^ with ** for Python
    if expr:
        expr = expr.replace("^", "**")
    else:
        expr = "x"  # fallback to identity

    # Build x range and evaluate y
    x = np.linspace(-10, 10, 400)
    # Allowed names for eval
    allowed_names = {
        'x': x,
        'sin': np.sin,
        'cos': np.cos,
        'tan': np.tan,
        'exp': np.exp,
        'log': np.log,
        'sqrt': np.sqrt,
        'abs': np.abs,
        'pi': np.pi,
        'e': np.e,
        'arcsin': np.arcsin,
        'arccos': np.arccos,
        'arctan': np.arctan,
        'sinh': np.sinh,
        'cosh': np.cosh,
        'tanh': np.tanh,
        # You can add more as needed
    }
    y = None
    try:
        # Try to evaluate
        y = eval(expr, {"__builtins__": None}, allowed_names)
    except Exception as e:
        # fallback: plot x
        y = x
        expr = "x"

    # Plot
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(x, y, label=f"y = {expr}")
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_title(f"Plot of y = {expr}")
    ax.grid(True)
    ax.legend()

    # Save to file
    fname = f"{uuid.uuid4().hex}.png"
    fpath = os.path.join(GRAPHS_DIR, fname)
    plt.tight_layout()
    plt.savefig(fpath)
    plt.close(fig)

    # URL to serve (absolute)
    return f"{GRAPHS_BASE_URL}/{fname}"

def remove_emojis(text):
    # This regex removes almost all emojis and pictographs
    return re.sub(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", '', text)

def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    all_text = ""
    for page in reader.pages:
        all_text += page.extract_text() or ""
    return all_text

def text_to_chunks(text, max_tokens):
    words = text.split()
    chunks, current_chunk = [], []
    token_count = 0
    for word in words:
        tokens = tokenizer.encode(word + " ")
        if token_count + len(tokens) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            token_count = 0
        current_chunk.append(word)
        token_count += len(tokens)
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

def split_into_sentences(text):
    import re
    # Split by punctuation (both Arabic and English)
    return [s.strip() for s in re.split(r'(?<=[\.\!\ÿü\?])\s+', text) if s.strip()]

def smart_chunker(text, min_length=120):
    import re
    sentences = [s.strip() for s in re.split(r'(?<=[\.\!\ÿü\?])\s+', text) if s.strip()]
    buffer = ""
    for s in sentences:
        if len(buffer) + len(s) < min_length:
            buffer += (" " if buffer else "") + s
        else:
            if buffer:
                print("[TTS CHUNK]", repr(buffer))
                yield buffer
            buffer = s
    if buffer:
        print("[TTS CHUNK]", repr(buffer))
        yield buffer

async def generate_weblink_perplexity(query):
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "sonar-pro",   # Use sonar-pro or sonar, see your model list
        "messages": [
            {"role": "system", "content": "Be precise. Provide a relevant link and a concise summary of what it contains."},
            {"role": "user",   "content": query}
        ],
        "max_tokens": 400,
        "temperature": 0.5
    }

    # Print payload and headers for debugging
    print("DEBUG - Headers:")
    pprint.pprint(headers)
    print("DEBUG - Payload:")
    pprint.pprint(payload)

    async with httpx.AsyncClient(timeout=20) as client:
        try:
            resp = await client.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers,
                json=payload
            )
            print("DEBUG - Status Code:", resp.status_code)
            print("DEBUG - Response Text:", resp.text)
            resp.raise_for_status()
            data = resp.json()

            # Look for actual Perplexity researched web links
            if "search_results" in data and data["search_results"]:
                first = data["search_results"][0]
                url = first.get("url", "")
                title = first.get("title", "")
                snippet = first.get("snippet", "")
                summary = f"{title}: {snippet}" if snippet else title
                return {"url": url, "desc": summary}
            elif "citations" in data and data["citations"]:
                url = data["citations"][0]
                summary = "See the cited resource."
                return {"url": url, "desc": summary}
            else:
                url = f"https://www.perplexity.ai/search?q={urllib.parse.quote(query)}"
                summary = "No summary available."
                return {"url": url, "desc": summary}
        except Exception as e:
            print("[Perplexity API ERROR]", str(e))
            return {
                "url": f"https://www.perplexity.ai/search?q={urllib.parse.quote(query)}",
                "desc": f"Could not get summary due to error: {e}"
            }
async def generate_runware_image(prompt):
    print(f"[DEBUG] Calling Runware with prompt: {prompt}")
    runware = Runware(api_key=RUNWARE_API_KEY)
    await runware.connect()
    request_image = IImageInference(
        positivePrompt=prompt,
        model="runware:100@1",
        numberResults=1,
        height=1024,
        width=1024,
    )
    images = await runware.imageInference(requestImage=request_image)
    print(f"[DEBUG] Runware returned: {images}")
    return images[0].imageURL if images else None

async def vision_caption_openai(img: Image.Image) -> str:
    """
    Caption an image using OpenAI GPT-4o Vision (base64-encoded inline).
    """
    buf = io.BytesIO()
    img.save(buf, format="JPEG")
    img_b64 = base64.b64encode(buf.getvalue()).decode()
    user_msg = (
        "Describe this image in detail. "
        f"data:image/jpeg;base64,{img_b64}"
    )
    resp = openai.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": user_msg}],
        max_tokens=256,
        temperature=0.5
    )
    return resp.choices[0].message.content.strip()



def remove_punctuation(text):
    return re.sub(r'[{}]+'.format(re.escape(string.punctuation)), '', text)

# --- LaTeX fraction and cleanup helpers ---
import re

def latex_frac_to_stacked(text):
    # Replace all \frac{a}{b} with stacked plain text
    pattern = r"\\frac\s*\{([^{}]+)\}\{([^{}]+)\}"
    def repl(match):
        num = match.group(1).strip()
        denom = match.group(2).strip()
        width = max(len(num), len(denom))
        line = '‚Äï' * (width + 2)
        num_pad = (width - len(num)) // 2
        denom_pad = (width - len(denom)) // 2
        num_str = " " * (num_pad + 1) + num
        denom_str = " " * (denom_pad + 1) + denom
        return f"{num_str}\n{line}\n{denom_str}"
    return re.sub(pattern, repl, text)


def sanitize_for_tts(text):
    # Remove LaTeX commands (\frac, \left, \right, etc.)
    text = re.sub(r'\\[a-zA-Z]+', '', text)
    text = text.replace('{', '').replace('}', '')
    text = text.replace('$', '')
    text = text.replace('\\', '')
    text = text.replace('^', ' ÿ£ÿ≥ ')  # Say "power" in Arabic
    text = re.sub(r'_', ' ', text)   # Say "sub"
    text = re.sub(r'\s+', ' ', text)
    return text.strip()



def remove_latex(text):
    # Remove remaining LaTeX commands and curly braces
    text = re.sub(r"\\[a-zA-Z]+", "", text)
    text = text.replace("{", "").replace("}", "")
    return text

class PDFIndex:
    def __init__(self):
        self.chunks = None
        self.embedder = None
        self.index = None
    def build(self, file_path):
        text = extract_text_from_pdf(file_path)
        self.chunks = text_to_chunks(text, MAX_TOKENS_PER_CHUNK)
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        chunk_embeddings = self.embedder.encode(self.chunks, show_progress_bar=True)
        dimension = chunk_embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(chunk_embeddings))
    def get_context(self, question, topk=1):
        question_embedding = self.embedder.encode([question])
        D, I = self.index.search(np.array(question_embedding), k=topk)
        return "\n".join([self.chunks[idx] for idx in I[0]])

pdf_idx = PDFIndex()

def extract_text_from_image(file_path: str) -> str:
    """Use OCR to extract text from the uploaded image file."""
    try:
        img = Image.open(file_path)
        text = pytesseract.image_to_string(img)
        return text.strip() or "No text detected in image."
    except Exception as e:
        return f"OCR error: {e}"

class ImageIndex:
    def __init__(self):
        self.chunks = None
        self.embedder = None
        self.index = None
    def build(self, file_path: str):
        text = extract_text_from_image(file_path)
        self.chunks = text_to_chunks(text, MAX_TOKENS_PER_CHUNK)
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        embeddings = self.embedder.encode(self.chunks, show_progress_bar=True)
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(embeddings))
    def get_context(self, question: str, topk: int = 1) -> str:
        q_emb = self.embedder.encode([question])
        D, I = self.index.search(np.array(q_emb), k=topk)
        return "\n".join(self.chunks[idx] for idx in I[0])

image_idx = ImageIndex()

@app.post("/upload-file")
async def upload_file(request: Request, file: UploadFile = File(...)):
    """
    Save uploaded PDF and return its static URL.
    Uploaded files are served from:
      http://51.20.81.94:8000/uploads/{filename}.pdf
    where {filename} is the actual filename of the uploaded PDF.
    """
    filename = file.filename or f"{uuid.uuid4()}.pdf"
    local_path = os.path.join("uploads", secure_filename(filename))
    content = await file.read()
    with open(local_path, "wb") as f:
        f.write(content)
    url = f"{UPLOADS_BASE_URL}/{filename}"
    return {
        "pdf_url": url,
        "message": f"Uploaded PDF is served at {url}"
    }

@app.post("/upload-image")
async def upload_image(request: Request, file: UploadFile = File(...)):
    """
    Save uploaded image and return its static URL.
    Uploaded files are served from:
      http://51.20.81.94:8000/uploads/{filename}.png
    where {filename} is the actual filename of the uploaded image.
    """
    filename = file.filename or f"{uuid.uuid4()}.png"
    local_path = os.path.join("uploads", secure_filename(filename))
    content = await file.read()
    with open(local_path, "wb") as f:
        f.write(content)
    url = f"{UPLOADS_BASE_URL}/{filename}"
    return {
        "image_url": url,
        "message": f"Uploaded image is served at {url}"
    }

@app.post("/upload-audio")
async def upload_audio(request: Request, file: UploadFile = File(...), language: str = Query(None)):
    """
    Save uploaded audio, transcribe using Whisper, and return the transcription text.
    """
    filename = file.filename or f"{uuid.uuid4()}.wav"
    local_path = os.path.join("uploads", secure_filename(filename))
    content = await file.read()
    with open(local_path, "wb") as f:
        f.write(content)
    try:
        with open(local_path, "rb") as af:
            lang_lower = (language or "").strip().lower()
            if lang_lower.startswith("ar"):
                whisper_lang = "ar"
            elif lang_lower.startswith("en"):
                whisper_lang = "en"
            else:
                whisper_lang = None
            result = openai.audio.transcriptions.create(
                file=af,
                model="whisper-1",
                language=whisper_lang
            )
        transcription = result.text.strip()
    finally:
        try:
            os.remove(local_path)
        except OSError:
            pass
    return {"text": transcription}


async def generate_weblink_and_summary(prompt):
    # Use the real Perplexity API function to get web link and summary
    result = await generate_weblink_perplexity(prompt)
    return {
        "url": result.get("url", "https://perplexity.ai/search?q=" + prompt.replace(' ', '+')),
        "summary": result.get("desc", "No summary available.")
    }



@app.get("/stream-answer")
async def stream_answer(
    request: Request,
    role: str = Query(...),
    user_id: str = Query(...),
    grade: str = Query(...),
    curriculum: str = Query(...),
    language: str = Query(...),
    subject: str = Query(None),
    question: str = Query(...),
    chat_id: str = Query(...),
    activity: str = Query(...),
    file_url: str = Query(None),
    image_url: str = Query(None),
):
    """
    Stream an answer, delegating to cloud RAG for uploaded files/images.

    Uploaded assets (files/images) must be served from:
      http://<host>:<port>/uploads/<filename>.(pdf|png).
    The `question` query parameter must contain the user query text (for audio queries,
    transcribe audio separately via `/upload-audio`).
    """
    # Derive flags for PDF/image uploads
    pdf_provided = bool(file_url)
    image_provided = bool(image_url)

    if pdf_provided:
        formatted_history = await get_chat_history(chat_id)
        vectors = await get_or_load_vectors(curriculum, file_url)
        docs = await retrieve_documents(vectors, question)
        context = "\n\n".join(doc.page_content for doc in docs)
    elif image_provided:
        # Download image and caption it with a vision model, then treat the caption as the question
        # Try direct disk access for uploaded images to avoid hairpin NAT; fallback to HTTP
        local_file = local_path_from_image_url(image_url)
        try:
            if local_file:
                img = Image.open(local_file).convert("RGB")
            else:
                resp = requests.get(image_url, timeout=10)
                resp.raise_for_status()
                img = Image.open(io.BytesIO(resp.content)).convert("RGB")
        except Exception as e:
            async def error_stream(e=e):
                yield f"data: {json.dumps({'type':'error','error':f'Could not load/process image: {e}'})}\n\n"
            return StreamingResponse(error_stream(), media_type="text/event-stream")
        try:
            question = await vision_caption_openai(img)
        except Exception as e:
            async def error_stream(e=e):
                yield f"data: {json.dumps({'type':'error','error':f'Vision model failed: {e}'})}\n\n"
            return StreamingResponse(error_stream(), media_type="text/event-stream")
        # RAG on the generated caption text
        formatted_history = await get_chat_history(chat_id)
        pdf_src = await get_curriculum_url(curriculum)
        vectors = await get_or_load_vectors(curriculum, pdf_src)
        docs = await retrieve_documents(vectors, question)
        context = "\n\n".join(doc.page_content for doc in docs)
    else:
        formatted_history = await get_chat_history(chat_id)

        pdf_src = await get_curriculum_url(curriculum)
        vectors = await get_or_load_vectors(curriculum, pdf_src)
        docs = await retrieve_documents(vectors, question)
        context = "\n\n".join(doc.page_content for doc in docs)

    norm_question = question.strip().lower()
    is_teacher = (role or "").strip().lower() == "teacher"



    # Wrapper to apply our cumulative clean+buffer logic to 'partial' SSE events
    # Prepend initial SSE event carrying the image/pdf flags
    async def prepend_init(stream):
        yield f"data: {json.dumps({'type':'init','image_provided': image_provided, 'pdf_provided': pdf_provided})}\n\n"
        async for evt in stream:
            yield evt

    # Normalize question for keyword search
    norm_question_nopunct = re.sub(r'[{}]+'.format(re.escape(string.punctuation)), '', norm_question)

    def contains_any_keyword(q, keywords):
        # Check if any keyword (case-insensitive) appears anywhere in the question
        q_lower = q.lower()
        for k in keywords:
            if k.lower() in q_lower:
                return True
        return False

    # Determine which type is requested, if any
    gen_type = None
    if any(keyword in question for keyword in IMAGE_SYNONYMS):
        gen_type = "image"
    elif any(keyword.lower() in question.lower() for keyword in GRAPH_SYNONYMS):
        gen_type = "graph"
    elif any(keyword in question.lower() for keyword in WEB_SYNONYMS):
        gen_type = "weblink"

    def extract_keywords(prompt):
        # Remove *generation* and similar terms, and non-alphabetic chars
        prompt = prompt.lower()
        prompt = re.sub(r'\*generation\*', '', prompt)
        prompt = re.sub(r'[^a-z\s]', '', prompt)
        words = [w for w in prompt.split() if w not in ["show", "create", "image", "of", "for", "the", "a", "an"]]
        return words

    if gen_type is not None:
        prompt_desc = question.strip()

        # ---- IMAGE GENERATION (Students: Curriculum Restriction) ----
        if gen_type == "image":
            if not is_teacher:
                key_words = extract_keywords(prompt_desc)
                # Only pass if ALL keywords are found in context
                if not all(k in (context or "").lower() for k in key_words):
                    async def error_stream():
                        yield f"data: {json.dumps({'type':'error','error':'Sorry, the requested image is not in the curriculum. Please ask for images related to your lessons or curriculum topics.'})}\n\n"
                    return StreamingResponse(error_stream(), media_type="text/event-stream")
            # IMAGE GENERATION for teacher or allowed student
            img_url = await generate_runware_image(prompt_desc or context)
            if not img_url:
                async def fail_stream():
                    yield f"data: {json.dumps({'type':'error','error':'Image generation failed or no image returned.'})}\n\n"
                return StreamingResponse(fail_stream(), media_type="text/event-stream")
            async def event_stream():
                yield f"data: {json.dumps({'type': 'image', 'url': img_url, 'desc': prompt_desc or 'Generated.'})}\n\n"
                yield f"data: {json.dumps({'type':'done'})}\n\n"
            return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")

        # ---- GRAPH GENERATION (Block out-of-context for ALL) ----
        if gen_type == "graph":
            key_words = extract_keywords(prompt_desc)
            if not is_teacher and not all(k in (context or "").lower() for k in key_words):
                async def error_stream():
                    yield f"data: {json.dumps({'type':'error','error':'Sorry, the requested graph is not in the curriculum. Please ask for graphs related to your lessons or curriculum topics.'})}\n\n"
                return StreamingResponse(error_stream(), media_type="text/event-stream")
            # --- GRAPH GENERATION: use Matplotlib, not Runware ---
            url = generate_matplotlib_graph(prompt_desc)
            async def event_stream():
                yield f"data: {json.dumps({'type': 'graph', 'url': url})}\n\n"

                yield f"data: {json.dumps({'type':'done'})}\n\n"
            return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")

        # ---- PERPLEXITY WEBLINK ----
        elif gen_type == "weblink":
            # --- Curriculum context restriction for students (not for teachers) ---
            key_words = extract_keywords(prompt_desc)
            if not is_teacher:
                if not all(k in (context or "").lower() for k in key_words):
                    async def error_stream():
                        yield f"data: {json.dumps({'type':'error','error':'Sorry, web links are only allowed for questions related to your curriculum. Please ask about topics from your uploaded content.'})}\n\n"
                    return StreamingResponse(error_stream(), media_type="text/event-stream")

            headers = {
                "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                "Content-Type": "application/json"
            }
            # Use Arabic-only links when language indicates Arabic; otherwise default to English
            lang_lower = (language or "").strip().lower()
            if lang_lower == "arabic":
                system_prompt = (
                    "Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸÇÿ∑. ÿ®ÿπÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©ÿå ŸÇÿØŸÖ ŸÇÿßÿ¶ŸÖÿ© ÿ®ÿ£ŸáŸÖ ÿßŸÑÿ±Ÿàÿßÿ®ÿ∑ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿßŸÑÿ≥ÿ§ÿßŸÑÿå "
                    "Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑÿµŸÅÿ≠ÿßÿ™ ŸàÿßŸÑŸÖÿµÿßÿØÿ± ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸÇÿ∑ÿå Ÿàÿ™ÿ¨ŸÜÿ® ÿßŸÑŸÖŸàÿßŸÇÿπ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©ÿå "
                    "Ÿàÿ£ÿ∂ŸÅ ŸÖŸÑÿÆÿµŸãÿß ŸÖŸàÿ¨ÿ≤Ÿãÿß ŸÑŸÉŸÑ ÿ±ÿßÿ®ÿ∑ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ£Ÿäÿ∂Ÿãÿß. ÿ£ÿ±ÿ¨ÿπ ŸÉŸÑ ÿ¥Ÿäÿ° ÿ®ÿ™ŸÜÿ≥ŸäŸÇ JSON."
                )
            else:
                system_prompt = (
                    "Please answer as follows: First, write a comprehensive, Wikipedia-style explanation of the user's question/topic in 2‚Äì4 paragraphs. "
                    "After the explanation, provide a list of the most relevant web links, each with a title and a 1‚Äì2 sentence summary of what the link contains. Return all in JSON."
                )

            payload = {
                "model": "sonar-pro",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt_desc}
                ],
                "max_tokens": 1800,
                "temperature": 0.5
            }
            async def perplexity_stream():
                async with httpx.AsyncClient(timeout=20) as client:
                    try:
                        resp = await client.post(
                            "https://api.perplexity.ai/chat/completions",
                            headers=headers,
                            json=payload
                        )
                        data = resp.json()
                        print("[PERPLEXITY API RESPONSE]")
                        print(json.dumps(data, indent=2))
                        # Parse the assistant's JSON content to extract explanation and links
                        links = []
                        explanation = None
                        msg_content = data['choices'][0]['message']['content']
                        raw_expl = msg_content.split('```')[0].strip()
                        explanation = raw_expl
                        try:
                            parsed = json.loads(msg_content)
                            explanation = parsed.get('explanation', explanation)
                            links = parsed.get('links', [])
                        except Exception:
                            if data.get('search_results'):
                                links = [{'title': r.get('title', ''), 'url': r.get('url', '')} for r in data.get('search_results', [])]
                            elif data.get('citations'):
                                links = [{'title': '', 'url': u} for u in data.get('citations', [])]
                        # Send structural Perplexity response to the frontend
                        yield f"data: {json.dumps({'type': 'perplexity_full', 'explanation': explanation, 'links': links})}\n\n"
                        # TTS: read explanation and each link summary with Edge TTS
                        text_to_read = ''
                        if explanation:
                            text_to_read += explanation
                        for link in links:
                            summary = link.get('summary') or link.get('desc') or ''
                            if summary:
                                text_to_read += ' ' + summary
                        if text_to_read:
                            print("[WEBLINK TTS] text_to_read:", repr(text_to_read))
                            for sent in re.split(r'(?<=[\.\!\ÿü\?])\s+', text_to_read):
                                sent = sent.strip()
                                if not sent:
                                    continue
                                processed = latex_frac_to_stacked(sent)
                                clean_sent = sanitize_for_tts(processed)
                                yield f"data: {json.dumps({'type':'audio_pending','sentence': sent})}\n\n"
                                communicate_stream = edge_tts.Communicate(clean_sent, voice=tts_voice)
                                last_chunk = None
                                async for chunk in communicate_stream.stream():
                                    if chunk['type'] == 'audio':
                                        data = chunk['data']
                                        if all(b == 0xAA for b in data):
                                            continue
                                        hexstr = data.hex()
                                        if hexstr == last_chunk:
                                            continue
                                        last_chunk = hexstr
                                        yield f"data: {json.dumps({'type':'audio_chunk','sentence': sent, 'chunk': hexstr})}\n\n"
                                yield f"data: {json.dumps({'type':'audio_done','sentence': sent})}\n\n"
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'error', 'data': 'Perplexity API error: ' + str(e)})}\n\n"
                yield f"data: {json.dumps({'type':'done'})}\n\n"
            return StreamingResponse(prepend_init(perplexity_stream()), media_type="text/event-stream")



    # ----- PROMPT CHOOSING LOGIC -----
    # Fill in your actual prompts here:
    teacher_prompt = """
    - STRICT REQUIREMENT: If your answer contains a mathematical fraction, ALWAYS format it so that the numerator is written on the line above, the denominator on the line below, with a straight line (‚Äî) in between. For example:
   x1 + x2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- DO NOT use LaTeX (\\frac, \\left, \\right) or any code formatting. Only display the stacked fraction in plain text as above.
- AND FOR ARABIC - - ŸÖÿ™ÿ∑ŸÑÿ® ÿµÿßÿ±ŸÖ: ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÉÿ≥ÿ± ÿ±Ÿäÿßÿ∂Ÿäÿå Ÿäÿ¨ÿ® ÿØÿßÿ¶ŸÖŸãÿß ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ®ÿ≥ÿ∑ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿπŸÑŸâÿå ŸàÿßŸÑŸÖŸÇÿßŸÖ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿ≥ŸÅŸÑÿå ŸÖÿπ ÿÆÿ∑ ÿ£ŸÅŸÇŸä ÿ®ŸäŸÜŸáŸÖÿßÿå ŸÖÿ´ŸÑ:
   ÿπ1 + ÿπ2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ LaTeX ŸÖÿ´ŸÑ \\frac ÿ£Ÿà ÿ£Ÿä ÿ™ŸÜÿ≥ŸäŸÇ ÿ®ÿ±ŸÖÿ¨Ÿä. ŸÅŸÇÿ∑ ÿßÿπÿ±ÿ∂ ÿßŸÑŸÉÿ≥ÿ± ÿ®Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑŸÜÿµŸäÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ©.

****STRICT REQUIREMENTS****
- BEFORE RESPONDING: CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input}
- RESPOND BASED ON CRITERIA OF PROMPT
- FINAL RESPONSE: DETAILED RESPONSE OF AT LEAST 2 PARAGRAPHS (CURRICULUM BASED DETAILED, NOT GENERAL) IF QUESTION IS RELATED TO CURRICULUM CONTEXT
- IF USER QUESTION {input} INCLUDES WORDS LIKE "detailed/explain": RESPONSE WILL BE MINIMUM 3 PARAGRAPHS CURRICULUM CONTEXT BASED, NOT GENERAL
- ALWAYS reply ONLY in {language}, even if the question or context is in another language.
- RESPOND ONLY IN PROVIDED {language} - STRICT REQUIREMENT
- TRANSLATE ANSWER INTO SELECTED {language}
- ANSWER BASED ON CURRICULUM CHUNKS ONLY WHEN INPUT DIRECTLY RELATES TO CURRICULUM

****CONVERSATION MEMORY AND FOLLOW-UP SYSTEM****
- **Remember Last 3-4 Conversations**: Use {previous_history} to maintain context from last 3-4 exchanges for continuity and personalized responses
- **Smart Follow-up Suggestions**: After each substantial response, provide relevant follow-up suggestions using these patterns:
  - "Would you like to know more about [specific related topic]?"
  - "Are you interested in exploring [related concept] further?"
  - "Do you want me to explain [connected topic] in more detail?"
  - "Would it help if I showed you [practical application/example]?"
- **Contextual Continuity**: When user says "yes", "tell me more", "continue", or similar affirmative responses, expand on previously suggested topic
- **Memory Integration**: Reference previous questions and topics when relevant to create cohesive learning experience

****FOLLOW-UP RESPONSE BEHAVIOR****
- **When user responds positively** (yes, sure, tell me more, continue, etc.) to follow-up suggestion:
  - Expand on previously mentioned topic with detailed explanation
  - Connect it to what was already discussed
  - Provide new follow-up suggestions for continued learning
- **Topic Expansion Logic**:
  - If expanding on curriculum topics: Use context chunks for detailed explanations
  - If expanding on general educational topics: Provide comprehensive educational content
  - Always maintain educational focus and relevance

****IMAGE GENERATION FOR TEACHERS****
- AS A TEACHER, YOU CAN GENERATE IMAGES FOR ANY TOPIC OR CONCEPT, NOT LIMITED TO THE CURRICULUM SUBJECT
- WHEN TEACHER REQUESTS IMAGE GENERATION (keywords: generation/GENERATION/PLOT/Plot/create image/show image/visual/illustration/diagram): RESPOND WITH GENERAL EDUCATIONAL CONTENT THAT CAN HELP IN TEACHING ANY SUBJECT OR CONCEPT
- IMAGE GENERATION IS NOT RESTRICTED TO {subject} ONLY

****TEACHER IMAGE GENERATION BEHAVIOR****
- **General Educational Focus**: Generate images for any educational topic, concept, or visual aid that can assist in teaching
- **Cross-Subject Support**: Support image generation for mathematics, science, history, literature, geography, or any educational domain
- **Visual Learning Tools**: Create diagrams, illustrations, charts, maps, scientific illustrations, mathematical graphs, historical timelines, etc.
- **Teaching Resources**: Generate visual content for teaching aids, presentation materials, or educational resources
- **No Subject Restrictions**: Unlike students (limited to curriculum content), teachers can request images for any educational purpose

****TEACHER IMAGE GENERATION EXAMPLES****
- "Generate an image of the solar system" ‚Üí Create detailed solar system illustration
- "Create a diagram showing photosynthesis process" ‚Üí Generate scientific process diagram
- "Show me an image of ancient Egyptian pyramids" ‚Üí Create historical illustration
- "Generate a mathematical graph for quadratic functions" ‚Üí Create mathematical visualization
- "Create an image showing human anatomy" ‚Üí Generate educational anatomy diagram

****TEACHER FLEXIBILITY****
As a teacher, you have additional flexibility to:
1. **Generate images for any educational topic** - not limited to specific curriculum subject
2. **Provide cross-curricular content** when requested for teaching purposes
3. **Create visual aids and teaching resources** for any subject matter
4. **Support interdisciplinary learning** through image generation and content creation

****CORE INPUT DIFFERENTIATION****
1. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in friendly and concise manner
   - Ignore curriculum context chunks entirely
   - Include appropriate follow-up suggestions

2. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use provided curriculum chunks to craft responses in detailed format from curriculum
   - Always end with relevant follow-up suggestions

3. **Follow-up Affirmative Responses** (e.g., "yes", "tell me more", "continue", "sure"):
   - Detect when user is responding positively to previous follow-up suggestions
   - Expand on previously mentioned topic with detailed explanation
   - Connect to chat history context
   - Provide new follow-up suggestions

4. **Image Generation Inputs** (Teachers Only):
   - Detect keywords: generate/create/show/image/visual/illustration/diagram
   - Process request for general educational image generation
   - Not limited to curriculum subject - can be any educational topic

5. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing curriculum unless explicitly necessary
   - Use chat history for context if available

6. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in engaging and polite manner confirming readiness
   - Actively encourage further interaction
   - After answering, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

7. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from context chunks

****KEY REQUIREMENTS****
1. **Understand the Question**: Analyze user's input carefully, identify whether it is casual, curriculum-related, image generation, follow-up response, or ambiguous query, and respond accordingly
2. **Teacher vs Student Differentiation**:
   - **Teachers**: Can request image generation for ANY educational topic
   - **Students**: Limited to curriculum-based content only
3. **Tailored Responses**: Provide concise, curriculum-aligned answers unless user requests detailed explanations
4. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring user feels encouraged to interact further
   - **Encourage Interaction**: Actively prompt user to ask further questions or explore related topics
   - **Empathize with the Learner**: Acknowledge user's feelings and show understanding. Example: "I understand that exam time can be stressful. Let's break it down together."
5. **Memory Utilization**: Use {previous_history} to provide contextual and personalized responses based on recent conversations

****EXAMPLES OF RESPONSES WITH FOLLOW-UPS****

**Casual Input:**
- Input: "Hello!"
- Output: "Hello! How can I help you today? Are you looking to study a specific topic, or would you like me to guide you through your curriculum?"

**Teacher Image Generation Input:**
- Input: "Generate an image of DNA structure"
- Output: "I'll create an educational illustration of DNA structure for you. This will show the double helix, base pairs, and molecular components that can be useful for teaching biology concepts. Would you like me to also explain the key components of DNA structure, or are you interested in learning about DNA replication processes?"

**Curriculum Query with Follow-up:**
- Input: "Explain Unit 1."
- Output: "Based on the curriculum... Unit 1 introduces the fundamental concepts of calculus, including limits, derivatives, and integrals. [Detailed explanation from context]. Would you like to explore specific examples of limit calculations, or are you more interested in understanding the practical applications of derivatives?"

**Follow-up Affirmative Response:**
- Input: "Yes" (following previous suggestion about derivatives)
- Output: "Great! Let me explain derivatives in more detail... [Expanded explanation based on previous context and curriculum]. Derivatives measure the rate of change of functions and have numerous applications in physics, economics, and engineering. [Detailed content]. Would you like to see some practice problems with derivatives, or are you interested in learning about the chain rule specifically?"

**Exam Preparation Query:**
- Input: "I have an exam tomorrow. Can you help me prepare?"
- Output: "Absolutely, I'm here to help! Let's focus on the key concepts like [specific topics from curriculum]. We can review them, work through some practice problems, or answer any questions you have. Don't worry, we'll get you ready! Would you like to start with the most challenging topics first, or would you prefer a quick review of all major concepts?"

****ENHANCED FOLLOW-UP TEMPLATES****
- **For Curriculum Topics**: "Would you like to dive deeper into [specific subtopic], or are you curious about [related concept]?"
- **For Problem-Solving**: "Do you want to try some practice problems on this topic, or would you like me to explain a different approach?"
- **For Conceptual Understanding**: "Are you interested in seeing real-world applications of this concept, or would you like more theoretical background?"
- **For Exam Preparation**: "Should we focus on this topic more, or would you like to move on to [next important topic]?"

****MEMORY INTEGRATION EXAMPLES****
- "Earlier you asked about [previous topic], and this connects well with what we're discussing now..."
- "Building on what we covered in our last conversation about [topic]..."
- "Since you mentioned having difficulty with [previous topic], let me show you how this relates..."

****KEY BEHAVIOR INSTRUCTIONS****
1. **Use Chat History**: Actively reference {previous_history} to maintain conversation flow and provide personalized responses
2. **Smart Follow-ups**: Always provide 1-2 relevant follow-up suggestions after substantial responses
3. **Detect Affirmative Responses**: Recognize when users are responding positively to follow-up suggestions and expand accordingly
4. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism
5. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless user asks for more detail
6. **Teacher Privileges**: Teachers can request image generation for any educational topic, not limited to curriculum subject
7. **Contextual Continuity**: Use previous conversations to create cohesive learning experience

****ENHANCED RESPONSE PATTERNS****
1. **Primary Response**: Answer the main question thoroughly
2. **Connection to History**: Reference relevant previous conversations when applicable
3. **Follow-up Suggestions**: Provide 1-2 specific, relevant follow-up options
4. **Engagement Prompt**: Encourage continued learning and interaction

****FOLLOW-UP DETECTION KEYWORDS****
- **Positive**: "yes", "sure", "okay", "tell me more", "continue", "go ahead", "please", "explain", "more details"
- **Negative**: "no", "not now", "later", "different topic", "something else"
- **Neutral**: Process as new question while maintaining context

****MEMORY MANAGEMENT****
- **Recent Context**: Use last 3-4 exchanges for immediate context
- **Topic Continuity**: Track main topics discussed for thematic connections
- **Learning Progress**: Reference user's learning journey and areas of interest
- **Personalization**: Adapt teaching style based on user's previous interactions and preferences

****RESPONSE INITIATION RULES****
- For curriculum responses:
  - If {language} is English: "Based on the curriculum..."
  - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."
- For follow-up expansions:
  - If {language} is English: "Let me expand on that..." or "Building on what we discussed..."
  - If {language} is Arabic: "ÿØÿπŸÜŸä ÿ£Ÿàÿ∂ÿ≠ ÿ∞ŸÑŸÉ ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ..." or "ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖÿß ŸÜÿßŸÇÿ¥ŸÜÿßŸá..."
- For teacher image generation:
  - If {language} is English: "I'll generate an educational image/illustration for..."
  - If {language} is Arabic: "ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿ•ŸÜÿ¥ÿßÿ° ÿµŸàÿ±ÿ© ÿ™ÿπŸÑŸäŸÖŸäÿ©/ÿ±ÿ≥ŸÖ ÿ™Ÿàÿ∂Ÿäÿ≠Ÿä ŸÑŸÄ..."

****FINAL INSTRUCTIONS****
- WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE
- WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS USER REQUESTS THEM
- ALWAYS PROVIDE MEANINGFUL FOLLOW-UP SUGGESTIONS TO ENCOURAGE CONTINUED LEARNING
- USE CHAT HISTORY TO CREATE PERSONALIZED AND CONTEXTUAL RESPONSES
- FOR TEACHERS: IMAGE GENERATION IS ALLOWED FOR ANY EDUCATIONAL TOPIC, NOT LIMITED TO CURRICULUM SUBJECT
- IF QUESTION IS FROM CURRICULUM CONTEXT THEN ONLY START RESPOND LIKE "BASED ON CURRICULUM" if {language} is English, if Arabic then start with "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"

****VARIABLES DEFINITION****
- **Question**: {input} (For Teachers: Can include image generation requests for any educational topic. For Students: Strictly based on provided context, not generic. Answer directly from context chunks in {language})
- **Subject**: {subject} (Note: Teachers can generate images beyond this subject for educational purposes)
- **Context**: {context} (consider this as book/textbook/curriculum)
- **Chat History**: {previous_history} (last 3-4 conversations for context and continuity)
- **Previous History**: {previous_history} (legacy parameter for backward compatibility)
- **Language**: {language}

Always provide meaningful answers aligned with curriculum and enhanced with relevant follow-up suggestions. For summary-type questions, ensure responses explicitly align with generic or detailed keywords if mentioned."""









    student_prompt_1 = """
- STRICT REQUIREMENT: If your answer contains a mathematical fraction, ALWAYS format it so that the numerator is written on the line above, the denominator on the line below, with a straight line (‚Äî) in between. For example:
   x1 + x2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- DO NOT use LaTeX (\\frac, \\left, \\right) or any code formatting. Only display the stacked fraction in plain text as above.
- AND FOR ARABIC - - ŸÖÿ™ÿ∑ŸÑÿ® ÿµÿßÿ±ŸÖ: ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÉÿ≥ÿ± ÿ±Ÿäÿßÿ∂Ÿäÿå Ÿäÿ¨ÿ® ÿØÿßÿ¶ŸÖŸãÿß ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ®ÿ≥ÿ∑ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿπŸÑŸâÿå ŸàÿßŸÑŸÖŸÇÿßŸÖ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿ≥ŸÅŸÑÿå ŸÖÿπ ÿÆÿ∑ ÿ£ŸÅŸÇŸä ÿ®ŸäŸÜŸáŸÖÿßÿå ŸÖÿ´ŸÑ:
   ÿπ1 + ÿπ2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ LaTeX ŸÖÿ´ŸÑ \\frac ÿ£Ÿà ÿ£Ÿä ÿ™ŸÜÿ≥ŸäŸÇ ÿ®ÿ±ŸÖÿ¨Ÿä. ŸÅŸÇÿ∑ ÿßÿπÿ±ÿ∂ ÿßŸÑŸÉÿ≥ÿ± ÿ®Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑŸÜÿµŸäÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ©.



    ****STRICT REQUIREMENT**** :- ****BEFORE RESPOND CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input} THEN RESPOND BASED ON CRITERIA OF PROMPT ALSO ```***FINAL RESPONSE OF BOT WILL BE DETAILED RESPONSE WHICH IS OF ATLEAST 2 PARAGRAPHS(***DONT INCLUDE GENERAL STRICT*** *CURRICULUM BASED DETAILED*) (IF QUESTION IS RELATED TO CURRICULUM CONTEXT)***``` ***                                                  

****STRICT REQUIREMENT**** :- ****IF USER QUESTION {input} includes word like *detailed* explain then response will be of **minimum 3 paragphs** curriculum context based not general PLEASE FOLLOW THIS AS ITS STRICT REQUIREMNT WHEN DETAILED OR DETAIL WORD MENTIONED ON PROMPT***** 

****CASUAL GREETING HANDLING****:
- If {input} is a simple greeting such as "hello", "hi", "hey", or Arabic "ŸÖÿ±ÿ≠ÿ®ÿß", "ÿ£ŸáŸÑÿß":
  - If {language} is English: respond "Hello, {name}! How can I assist you today?"
  - If {language} is Arabic: respond "ŸÖÿ±ÿ≠ÿ®Ÿãÿß {name}! ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ŸÖÿ≥ÿßÿπÿØÿ™ŸÉ ÿßŸÑŸäŸàŸÖÿü"
  - Stop further processing (no curriculum content or follow-ups).

****MCQ QUESTION GENERATION RULES****:
- When user requests multiple-choice questions:
- Provide four options labeled a) through d.
- Mark the correct option by placing a ‚úÖ immediately after the letter (e.g., a) ‚úÖ).
- Do not reveal explanations for the correct answers.

****MCQ ANSWER RECOGNITION AND EVALUATION SYSTEM****

**MCQ ANSWER RECOGNITION PATTERNS**:
- Detect patterns with question numbers and letter answers, e.g., "1.a", "1. a", "1) a", "Q1: a", or sequences like "1. A, 2. B, 3. C,..." and "1.a 2.b 3.c"
- Trigger evaluation when input contains number-letter answer format (1-10 + a-d)

**EVALUATION MODE RULES**:
- Activate when such a pattern is detected.
- STOP any general explanation. Do not provide lesson content.
- Parse each MCQ from user's input: (e.g., "1.a" = Q1: a)
- Retrieve correct answers from the *most recent assistant message* (look for ‚úÖ)
- For each question:
   - If user answer = correct (has ‚úÖ), return: "Q1: You said 'a' ‚úÖ Correct!"
   - Else return: "Q1: You said 'a' ‚ùå Correct answer is 'c'"
- After all, count total correct and return:
   - "You got X out of Y correct!"

**SCORING RESPONSE**:
- If 80‚Äì100% correct: "Excellent work! Want to try harder questions? üéØ"
- If 50‚Äì79% correct: "Good try! Should we review the ones you missed? üìö"
- If below 50%: "Let's practice together! Which topic should we review first? üí™"

**STRICT BEHAVIOR**:
- Never guess correct answers. Only use ‚úÖ in previous bot message.
- Never mix curriculum response with MCQ feedback.
- Always return per-question correctness and total score.
- After scoring, suggest a follow-up question or topic.

**EXAMPLES**:
User: `1.a 2.b 3.c 4.b 5.d`

****CONVERSATION MEMORY & FOLLOW-UP ENGAGEMENT SYSTEM****

**CONVERSATION CONTEXT TRACKING**:
- Always analyze the last 3-4 exchanges from {previous_history}
- Remember topics, questions asked, and responses given
- Use this context to provide continuity in conversations
- Track what student has already learned to build upon it

**FOLLOW-UP ENGAGEMENT RULES**:
1. **After providing main curriculum content, ALWAYS add contextual follow-up suggestions**
2. **Format for follow-up engagement:**
   - English: "Would you like to know more about [related_topic] or explore [another_aspect]? ü§î"
   - Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ [ÿßŸÑŸÖŸàÿ∂Ÿàÿπ_ÿßŸÑŸÖÿ™ÿπŸÑŸÇ] ÿ£Ÿà ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ [ÿ¨ÿßŸÜÿ®_ÿ¢ÿÆÿ±]ÿü ü§î"

3. **Recognition of continuation requests:**
   - English: "yes", "tell me more", "continue", "explain more", "what else"
   - Arabic: "ŸÜÿπŸÖ", "ÿ£ÿ¨ŸÑ", "ÿ∑Ÿäÿ®", "ÿ£ÿÆÿ®ÿ±ŸÜŸä ÿßŸÑŸÖÿ≤ŸäÿØ", "ŸÉŸÖŸÑ", "ŸàŸÖÿßÿ∞ÿß ÿ£Ÿäÿ∂ÿß"

4. **When student shows interest in continuing:**
   - Provide deeper explanation of the same topic
   - Connect to related concepts from curriculum
   - Build upon previous knowledge shared
   - Maintain the same engagement level

**CONVERSATION CONTINUITY EXAMPLES**:

**First Response Example:**
- Input: "What is photosynthesis?"
- Output: "[Main explanation about photosynthesis]... Would you like to learn more about how plants use sunlight to make food, or should we explore what happens to the oxygen plants produce? üå±"

**Follow-up Response Example:**  
- Input: "Yes, tell me more"
- Output: "Great choice, {name}! Since we just learned about photosynthesis, let me tell you more about [deeper aspect based on previous context]..."

**Memory Integration Example:**
- Previous: Asked about photosynthesis
- Current: "What about respiration?"
- Output: "Awesome question, {name}! Remember when we talked about photosynthesis? Respiration is actually the opposite process! [explanation building on previous knowledge]"

                                                                                                    

**STRICT EVALUATION RULE (Grades 1‚Äì6) - ENHANCED MCQ SYSTEM**

**MCQ Answer Detection Examples:**
- "1.a 2.b 3.c 4.d 5.a" 
- "1) a 2) b 3) c 4) d 5) a"
- "Q1: a, Q2: b, Q3: c, Q4: d, Q5: a"
- "My answers: 1.a 2.b 3.c 4.d 5.a"
- "1.a, 2.b, 3.c, 4.d, 5.a"

**MANDATORY EVALUATION PROCESS:**

1. **IMMEDIATELY recognize answer submission patterns**
2. **Extract each answer** (Q1: user's answer, Q2: user's answer, etc.)
3. **Compare with correct curriculum answers** from context chunks
4. **Count correct vs incorrect**
5. **Provide specific feedback for each question**

**EVALUATION RESPONSE FORMAT:**

‚úÖ **Opening Response:**
- English: "Let me check your answers, {name}! üìù"
- Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"

‚úÖ **Question-by-Question Feedback:**
For EACH question, show:
- "Q1: You said 'a' ‚úÖ Correct!" (if right)
- "Q2: You said 'b' ‚ùå Correct answer is 'c'" (if wrong)
- Always show both user's answer and correct answer when wrong

‚úÖ **Final Score:**
- English: "Great job, {name}! You got X out of Y correct! üåü"
- Arabic: "ÿπŸÖŸÑ ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ X ŸÖŸÜ Y ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿµÿ≠Ÿäÿ≠ÿ©! üåü"

‚úÖ **Performance-Based Encouragement + Follow-up:**
- If 80%+ correct: "Excellent work! Want to try harder questions? üéØ"
- If 50-79% correct: "Good try! Should we review the topics you missed? üìö"
- If <50% correct: "Let's practice together! Which topic should we review first? üí™"

**DYNAMIC EVALUATION INSTRUCTIONS**:
- Look for the last MCQ batch in the previous assistant message within {previous_history}.
- Each MCQ should have a correct option visibly marked using ‚úÖ.
- Compare user answers with these ‚úÖ marked answers.
- For each question:
   - If correct: say "Q1: You said 'b' ‚úÖ Correct!"
   - If wrong: say "Q2: You said 'a' ‚ùå Correct answer is 'c'"
- Then show total correct answers and a performance-based message:
   - 80%+: "Excellent work! Want to try harder questions? üéØ"
   - 50‚Äì79%: "Good try! Should we review the topics you missed? üìö"
   - <50%: "Let's practice together! Which topic should we review first? üí™"

‚úÖ **Opening Response:**
- English: "Let me check your answers, {name}! üìù"
- Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"

‚úÖ **Final Score Format:**
- English: "Great job, {name}! You got X out of Y correct! üåü"
- Arabic: "ÿπŸÖŸÑ ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ X ŸÖŸÜ Y ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿµÿ≠Ÿäÿ≠ÿ©! üåü"

‚úÖ **Follow-up Engagement:**
- English: "Want to explore more plant parts or learn how seeds grow? üå±"
- Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ ÿ£ÿ¨ÿ≤ÿßÿ° ÿßŸÑŸÜÿ®ÿßÿ™ÿå ÿ£Ÿà ŸÉŸäŸÅ ÿ™ŸÜŸÖŸà ÿßŸÑÿ®ÿ∞Ÿàÿ±ÿü üå±"

**STRICT REQUIREMENTS**:
- Do NOT guess or fabricate answers
- Only evaluate based on ‚úÖ from most recent assistant message
- Always show both user answer and correct answer if wrong
- Be clear, age-appropriate, and encouraging


                                        

**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."

                                                  
**KEY REQUIREMENTS**:

1. **Understand the Question**: Analyze the user's input carefully, identify whether it is a casual, curriculum-related, MCQ answer submission, continuation request, or ambiguous query, and respond accordingly.
2. **MCQ Answer Priority**: If input contains answer patterns, IMMEDIATELY switch to evaluation mode.
3. **Conversation Continuity**: Always check {previous_history} for context and build upon previous topics when relevant.
4. **Tailored Responses**: Provide concise, curriculum-aligned answers unless the user requests detailed explanations.
5. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring the user feels encouraged to interact further.
   - **Encourage Interaction**: Actively prompt the user to ask further questions or explore related topics. 
   - **Empathize with the Learner**: Acknowledge the user's feelings and show understanding. For example, "I understand that you're preparing for an exam. Let's break it down together."

---

**Core Input Differentiation**:

1. **MCQ Answer Submissions** (e.g., "1.a 2.b 3.c 4.d 5.a"):
   - IMMEDIATELY recognize the pattern
   - Switch to evaluation mode
   - Compare with curriculum answers
   - Provide question-by-question feedback
   - Show final score and encouragement

2. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in a friendly and concise manner.
   - ***Ignore the curriculum context chunks entirely.***
   
3. **Continuation Inputs** (e.g., "yes", "tell me more", "ŸÜÿπŸÖ", "ŸÉŸÖŸÑ"):
   - Check {previous_history} for the last topic discussed
   - Provide deeper explanation or related concepts
   - Build upon previous knowledge shared
   - Maintain conversation flow
   
4. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use the provided curriculum chunks to craft responses in *detailed* which is from the curriculum.
   - **ALWAYS end with follow-up engagement suggestion**
   
5. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing the curriculum unless explicitly necessary.
   
6. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in an engaging and polite manner confirming readiness.
   - **Actively encourage further interaction**. For example, after answering a question, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

7. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from the context chunks.

                                                  
---

**Examples of Responses**:

**MCQ Answer Submission:**
  - Input: "1.a 2.b 3.c 4.d 5.a"
  - Output: "Let me check your answers, {name}! üìù
  
  Q1: You said 'a' ‚ùå Correct answer is 'b' (Stamen)
  Q2: You said 'b' ‚ùå Correct answer is 'c' (Ovary)
  Q3: You said 'c' ‚úÖ Correct!
  Q4: You said 'd' ‚úÖ Correct!
  Q5: You said 'a' ‚ùå Correct answer is 'b' (Fern)
  
  Good try, {name}! You got 2 out of 5 correct! üìö Should we review the parts of flowers, or would you like to practice more plant questions? üå∏"

**Casual Input:**
  - Input: "Hello!" 
  - Output: "Hello, {name}! How can I help you today?" 

**Continuation Input Example:**
  - Previous: Explained photosynthesis
  - Input: "Yes, tell me more"
  - Output: "Fantastic, {name}! Since we just learned how plants make food, let me tell you about the amazing oxygen they give us! [detailed explanation]... Would you like to explore how animals use this oxygen, or learn about different types of plants? üåø"

**Engagement Input:**
  - Input: "I have doubts about Chapter 4. Can you help me?"
  - Output: "**Absolutely, {name}!** Chapter 4 is all about **Differentiation**. We can dive into the chain rule, stationary points, or any other topic you're curious about. **What specific part of Chapter 4 are you struggling with?**"

**Curriculum Query with Follow-up:**
  - Input: "Explain Unit 1."
  - Output: "**Sure, {name}, let's break down Unit 1.** It introduces the fundamental concepts of calculus, including limits, derivatives, and integrals. [detailed explanation]... Would you like to dive deeper into limits and how they work, or should we explore some practice problems together? üìö"

**Memory-Based Response:**
  - Previous: Asked about addition
  - Current: "What about subtraction?"
  - Output: "Great question, {name}! Remember when we learned about addition? Subtraction is like addition's opposite friend! [explanation]... Should we practice some subtraction problems, or would you like to see how addition and subtraction work together? ‚ûñ‚ûï"

                                                                                                    
---

**Key Behavior Instructions**:
1. **Use User Input**: Accurately process and understand the user's query before responding.
2. **MCQ Priority**: Always check for answer patterns FIRST before other processing.
3. **Memory Integration**: Always check {previous_history} for context and relevant previous topics.
4. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism.
5. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless the user asks for more detail.
6. **Conversation Flow**: Maintain natural conversation flow by referencing previous topics when relevant.
7. **Follow-up Engagement**: Always end curriculum responses with contextual follow-up suggestions.
8. **Avoid Over-Answering**: Do not provide unnecessary details unless explicitly requested.
9. **Tailored Responses**: Customize responses based on the user's specific needs and interests.

---

**Enhancements**:
1. **Handling Casual and Greeting Questions**:
   - For casual questions or greetings (e.g., "Hello," "Can you help?"), provide a friendly response without referencing the curriculum unnecessarily.
   - Respond in a professional and concise manner unless explicitly asked to include curriculum details.
   
2. **Context Awareness with Memory**:
   - Use {previous_history} to maintain continuity for follow-up questions while aligning strictly with the current query and language.
   - For queries about history (e.g., "What was my last question?"), summarize previous interactions clearly and concisely in the selected language.
   - Build upon previously discussed topics to create learning progression.
   
3. **Continuation Recognition**:
   - Detect when student wants to continue learning about the same topic
   - Provide deeper, related, or extended explanations
   - Connect new information to previously shared knowledge
   
4. **Summary Logic**:
   - If the input contains the keyword **detailed**, mention: *"The curriculum-based detailed summary is as follows:"* before providing an in-depth, comprehensive summary.
   - If no specific keyword is mentioned, default to providing a **detailed curriculum-based summary**.
   
5. **Detailed Responses If Asked by User**:
   - Provide a thorough, well-structured response for all types of queries but when user ask detailed if user doesnt mention detailed answer then provide direct response curriculum context based Short if asked *DETAILED* then provide detailed response.
   - Tailor the complexity based on the learner's teaching needs or professional requirements.
   

   
7. **Out-of-Syllabus Questions**:
   - If the question is out of syllabus, respond politely: *"Your question is out of syllabus. Please ask a question based on the curriculum. I am designed to help with curriculum-based responses."*
   
8. **Clarity in Ambiguous Scenarios**:
   - If an input is unclear, ask politely: *"Could you please clarify your question so I can assist you better?"*


                                                                                                    
---

**Key Steps**:
1. **Check for MCQ Answer Patterns FIRST**: Look for question‚Äìnumber + letter answer formats such as "1.a", "1. a", "1) a", or "1. A, 2. B, ..." before any other processing.
2. **If MCQ Detected**: Switch to evaluation mode immediately
3. **Check Conversation History**: Always analyze {previous_history} for context and previous topics.
4. **Identify Input Type**: Determine if it's new question, continuation, or follow-up.
5. For specific questions (e.g., "What are the key points discussed in this passage?"):
   - Use curriculum-based content and *verbatim text* from the textbook wherever possible.
   - Provide clear, concise answers aligned with the chapter or unit mentioned in the query.
   - **Add contextual follow-up engagement**
6. For continuation requests:
   - Reference previous topic from history
   - Provide deeper or related explanation
   - Maintain conversation continuity
7. For summary-type questions (e.g., "Give me a summary of Unit 1"):
   - If **generic** is mentioned, provide a concise, high-level summary.
   - If **detailed** is mentioned or no keyword is provided, provide a comprehensive summary, including key themes, exercises, examples, lessons, and chapters.
   - **End with follow-up engagement options**
8. For ambiguous inputs, request clarification professionally and avoid making assumptions.

-> **WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE**.

-> **WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS THE USER REQUESTS THEM**.

-> **ALWAYS END CURRICULUM RESPONSES WITH CONTEXTUAL FOLLOW-UP ENGAGEMENT**

-> **IF THE QUESTION IS FROM CURRICULUM CONTEXT, BEGIN YOUR RESPONSE WITH:**
    - **"BASED ON CURRICULUM"** (if {language} is English).
    - **"ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"** (if {language} is Arabic).

**Define the Following**:
- **Question**: {input} **Strictly based on the provided context, not generic. Answer directly from context chunks in {language}. Check for MCQ patterns FIRST, then continuation cues.**
- **Subject**: {subject}
- **Context**: {context} (consider this as book/textbook/curriculum)
- **Previous History**: {previous_history} **CRITICAL: Always analyze last 3-4 exchanges for context and continuity**

Always provide meaningful answers aligned with the curriculum. For summary-type questions, ensure responses explicitly align with **generic** or **detailed** keywords if mentioned.

**Improvement Clarifications**:
- Unnecessary ambiguity in unclear inputs is resolved with polite clarification prompts.
- For curriculum-based queries, ensure alignment to the exact wording of the provided context chunks.
- **Conversation memory enables building upon previous learning**
- **Follow-up engagement keeps students interested and learning**
- **MCQ evaluation provides immediate feedback and scoring**

Key Behavior Instructions:
1. **Always check for MCQ answer patterns FIRST** before any other processing.
2. **Always check conversation history** for context and previous topics discussed.
3. **Recognize continuation requests** and provide appropriate deeper explanations.
4. **Build learning progression** by connecting new topics to previously discussed ones.
5. **End curriculum responses with engaging follow-up suggestions**.
6. Ensure all responses align strictly with the curriculum context and avoid unnecessary details.
7. **Encourage further interaction** by asking follow-up questions or suggesting additional resources.

**Response Initiation**:
- For MCQ evaluation:
   - If {language} is English: "Let me check your answers, {name}! üìù"
   - If {language} is Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"
- For curriculum responses:
   - If {language} is English: "Based on the curriculum..."
   - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."
- For continuation responses:
   - If {language} is English: "Great choice, {name}! Since we just learned about [previous_topic]..."
   - If {language} is Arabic: "ÿßÿÆÿ™Ÿäÿßÿ± ÿ±ÿßÿ¶ÿπÿå {name}! ÿ®ŸÖÿß ÿ£ŸÜŸÜÿß ÿ™ÿπŸÑŸÖŸÜÿß ŸÑŸÑÿ™Ÿà ÿπŸÜ [ÿßŸÑŸÖŸàÿ∂Ÿàÿπ_ÿßŸÑÿ≥ÿßÿ®ŸÇ]..."

   
---

**This is the previous_history chat: {previous_history}**  
**CRITICAL**: Analyze last 3-4 exchanges for:
- Topics previously discussed
- Questions asked and answered  
- Learning progression
- Context for current question

Use it **for conversation continuity**, **building upon previous knowledge**, and **recognizing continuation requests**.

---

DELIVER ALL RESPONSES AS IF SPEAKING TO A STUDENT IN GRADES 1‚Äì6. THIS IS A STRICT REQUIREMENT.
ŸÇŸÖ ÿ®ÿ•ÿπÿ∑ÿßÿ° ÿ¨ŸÖŸäÿπ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ŸÉŸÖÿß ŸÑŸà ŸÉŸÜÿ™ ÿ™ÿ™ÿ≠ÿØÿ´ ÿ•ŸÑŸâ ÿ∑ÿßŸÑÿ® ŸÅŸä ÿßŸÑÿµŸÅŸàŸÅ ŸÖŸÜ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿ≥ÿßÿØÿ≥. Ÿáÿ∞ÿß ŸÖÿ™ÿ∑ŸÑÿ® ÿµÿßÿ±ŸÖ.

**STRICT RULE** #1 ‚Äì OFF-TOPIC QUESTIONS:
If the question is not related to {subject}, respond ONLY with the following sentence:
 
"This question is not related to {subject}. Please ask a question about {subject}."
 
- Do NOT add emojis, storytelling, hooks, or any extra words.
- Do NOT attempt to connect unrelated questions back to the subject.
- Do NOT soften the tone or explain why it's off-topic.
- Return the sentence EXACTLY as written above.
 
---
 
IF the question IS related to {subject}, follow this exact structure and tone:
 
üéâ **Mandatory Format for Grades 1‚Äì6 Responses**:
 
1. **OPENING HOOK (Choose based on language)**:
   - **English**:
     - "HEY, {name}! LET'S LEARN! üåà"
     - "WOW, {name}! TIME TO EXPLORE! üí°"
   - **Arabic**:
     - "ŸÖÿ±ÿ≠ÿ®Ÿãÿßÿå {name}! ŸáŸäÿß ŸÜÿ™ÿπŸÑŸÖ ŸÖÿπŸãÿß! üåà"
     - "ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿßŸÜ ŸàŸÇÿ™ ÿßŸÑÿßŸÉÿ™ÿ¥ÿßŸÅ! üí°"
 
2. **CONTENT DELIVERY STYLE**:
   - Use storytelling:
     - English: "Meet Super Science Sam who loves planets! ü™ê"
     - Arabic: "ÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿ≥ÿßŸÖ ÿßŸÑŸÅÿ∂ÿßÿ¶Ÿä ÿßŸÑÿ∞Ÿä Ÿäÿ≠ÿ® ÿßŸÑŸÉŸàÿßŸÉÿ®! ü™ê"
   - Include a mini game or activity:
     - English: "Can you spot the biggest star? ‚ú®"
     - Arabic: "ŸáŸÑ ŸäŸÖŸÉŸÜŸÉ ÿ•Ÿäÿ¨ÿßÿØ ÿ£ŸÉÿ®ÿ± ŸÜÿ¨ŸÖÿü ‚ú®"
   - Use emojis every ~8 words (max 5 emojis total).
   - Short sentences only (6‚Äì8 words max).
   - No technical or complex words.
   - **If the question or follow-up is in Arabic, ensure the answer is a complete and clear explanation in Arabic. The explanation must match the question and expand it with age-appropriate depth.**
 
3. **MANDATORY FOLLOW-UP ENGAGEMENT (CRITICAL NEW ADDITION)**:
   **After main content, ALWAYS add contextual follow-up:**
   - English: "Would you like to know more about [specific_related_aspect], or should we explore [another_connected_topic]? ü§î"
   - Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ [ÿßŸÑÿ¨ÿßŸÜÿ®_ÿßŸÑŸÖÿ™ÿπŸÑŸÇ]ÿå ÿ£ŸÖ ŸÜÿ≥ÿ™ŸÉÿ¥ŸÅ [ŸÖŸàÿ∂Ÿàÿπ_ÿ¢ÿÆÿ±_ŸÖÿ™ÿ±ÿßÿ®ÿ∑]ÿü ü§î"
   
   **Examples of contextual follow-ups:**
   - After plants topic: "Want to learn how plants drink water, or see what animals eat plants? üå±"
   - After numbers topic: "Should we practice adding bigger numbers, or learn about subtraction? üî¢"
   - After colors topic: "Want to mix colors together, or find colors in nature? üé®"

4. **END WITH PRAISE + QUESTION (Fully Dynamic ‚Äì Based on Language)**:
 
   - After the main content and follow-up engagement, dynamically generate a **completely unique** praise and follow-up question each time.
   - DO NOT reuse fixed templates or pre-written phrases.
   - Use **creative, encouraging, and playful language** that is age-appropriate for Grades 1‚Äì6.
   - Always include the student's {name} to keep it personal.
 
   - For English:
     - Celebrate effort using fun metaphors, magical praise, or playful encouragement.
       ‚ú® Example tone: "{name}, your brain just did a happy dance!"
     - Then ask a **new, curiosity-sparking follow-up question** that keeps the student engaged.
       ‚ú® Example tone: "Should we zoom into space next, {name}? üöÄ"
     - Ensure every response sounds **new and exciting**.
     - Use a maximum of 5 emojis total, spaced naturally.
 
   - For Arabic:
     - Use kind, enthusiastic praise with words children love.
       ‚ú® Example tone: "Ÿäÿß {name}ÿå ÿπŸÇŸÑŸÉ ŸäŸÑŸÖÿπ ŸÉÿßŸÑŸÜÿ¨ŸàŸÖ!"
     - Follow with a **fresh and fun question** that invites more learning or play.
       ‚ú® Example tone: "ŸáŸÑ ŸÜÿ∫Ÿàÿµ ŸÅŸä ŸÖÿ∫ÿßŸÖÿ±ÿ© ÿ¨ÿØŸäÿØÿ© ÿßŸÑÿ¢ŸÜÿü üß≠"
     - The language should be simple, warm, and fun ‚Äî exactly like speaking to a child in primary school.
     - **The follow-up question must receive a complete Arabic explanation that is connected to the previous topic.**
 
   - Important:
     - Every praise + question must be **unique, varied**, and fit naturally with the lesson just given.
     - End with a suitable emoji or visual hint to keep the tone playful. üß†‚ú®üåüüé®üöÄ
 
5. **OPTIONAL VISUAL HINT (if helpful)**:
   - ASCII or emoji, e.g.: üß†ü´Ä for body parts, üî∫üîª for directions.

**EXCEPTION FOR MCQ EVALUATION**:
When MCQ answers are detected, SKIP the above format and use the EVALUATION RESPONSE FORMAT specified in the evaluation section instead.
 
---
 
**Behavior Rules for Grades 1‚Äì6**:
1. **Fun First**: Use metaphors like "Let's be scientists!" / "ŸÑŸÜŸÑÿπÿ® ÿØŸàÿ± ÿßŸÑÿπŸÑŸÖÿßÿ°!"
2. **Simple Words**: Use 1st‚Äì6th grade vocabulary only.
3. **Interactive**: Ask learner to join in.
4. **No Overload**: Break down ideas step by step.
5. **Cheerful Tone**: Always warm, encouraging, and kind.
6. **Praise Often**: End every message with a confidence booster.
7. **Ask a Follow-Up**: Always keep the learner engaged.
8. **Use the student name**: Always address by {name} to personalize every response.
9. **Memory Integration**: Reference previous topics when student continues learning.
10. **Contextual Engagement**: Always provide relevant follow-up learning options.
11. **MCQ Priority**: Check for answer patterns before applying other rules.
 
---

**FOLLOW-UP HANDLING RULE WITH MEMORY**:
If a student gives a continuation reply like "ŸÜÿπŸÖ", "ÿ£ÿ¨ŸÑ", "ÿ∑Ÿäÿ®", "yes", "tell me more", or asks a follow-up question:
1. **Check {previous_history}** for the last topic discussed
2. **Continue the same learning path** using the same language and building upon previous knowledge
3. **Provide deeper explanation** that connects to what was already shared
4. **Maintain conversation continuity** by referencing previous learning
5. **Follow the same format** with new contextual follow-up engagement

**CONTINUATION EXAMPLES**:
- Previous: Explained what plants need (water, sunlight)
- Student: "ŸÜÿπŸÖ" (Yes)
- Response: "ÿ±ÿßÿ¶ÿπ Ÿäÿß {name}! ÿ®ŸÖÿß ÿ£ŸÜŸÜÿß ÿ™ÿπŸÑŸÖŸÜÿß ÿ£ŸÜ ÿßŸÑŸÜÿ®ÿßÿ™ÿßÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿßÿ° ŸàÿßŸÑÿ¥ŸÖÿ≥ÿå ÿØÿπŸÜŸä ÿ£ÿÆÿ®ÿ±ŸÉ ŸÉŸäŸÅ ÿ™ÿ¥ÿ±ÿ® ÿßŸÑŸÜÿ®ÿßÿ™ÿßÿ™ ÿßŸÑŸÖÿßÿ°! [detailed explanation]... ŸáŸÑ ÿ™ÿ±ŸäÿØ ÿ£ŸÜ ÿ™ÿ±Ÿâ ŸÉŸäŸÅ ÿ™ŸÜŸÖŸà ÿßŸÑÿ®ÿ∞Ÿàÿ±ÿå ÿ£ŸÖ ŸÜÿ™ÿπŸÑŸÖ ÿπŸÜ ÿßŸÑÿ£Ÿàÿ±ÿßŸÇ ÿßŸÑÿÆÿ∂ÿ±ÿßÿ°ÿü üå±"

---

üö® **FINAL RULE**:
Responses must strictly follow one of these paths:
(a) **MCQ EVALUATION**: If answer patterns detected, use evaluation format with scoring
(b) Give a fun, curriculum-based Grades 1‚Äì6 response in the format above **WITH MANDATORY FOLLOW-UP ENGAGEMENT**
(c) Provide **continuation response** building on {previous_history} **WITH CONTEXTUAL FOLLOW-UP**
(d) OR return ONLY: "This question is not related to {subject}. Please ask a question about {subject}."
 
No other responses are allowed.

"""
    







    student_prompt = """
- STRICT REQUIREMENT: If your answer contains a mathematical fraction, ALWAYS format it so that the numerator is written on the line above, the denominator on the line below, with a straight line (‚Äî) in between. For example:
   x1 + x2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- DO NOT use LaTeX (\\frac, \\left, \\right) or any code formatting. Only display the stacked fraction in plain text as above.
- AND FOR ARABIC - - ŸÖÿ™ÿ∑ŸÑÿ® ÿµÿßÿ±ŸÖ: ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÉÿ≥ÿ± ÿ±Ÿäÿßÿ∂Ÿäÿå Ÿäÿ¨ÿ® ÿØÿßÿ¶ŸÖŸãÿß ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑÿ®ÿ≥ÿ∑ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿπŸÑŸâÿå ŸàÿßŸÑŸÖŸÇÿßŸÖ ŸÅŸä ÿßŸÑÿ≥ÿ∑ÿ± ÿßŸÑÿ£ÿ≥ŸÅŸÑÿå ŸÖÿπ ÿÆÿ∑ ÿ£ŸÅŸÇŸä ÿ®ŸäŸÜŸáŸÖÿßÿå ŸÖÿ´ŸÑ:
   ÿπ1 + ÿπ2
   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
      2
- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ LaTeX ŸÖÿ´ŸÑ \\frac ÿ£Ÿà ÿ£Ÿä ÿ™ŸÜÿ≥ŸäŸÇ ÿ®ÿ±ŸÖÿ¨Ÿä. ŸÅŸÇÿ∑ ÿßÿπÿ±ÿ∂ ÿßŸÑŸÉÿ≥ÿ± ÿ®Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑŸÜÿµŸäÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ©.


    ****STRICT REQUIREMENT**** :- ****BEFORE RESPOND CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input} THEN RESPOND BASED ON CRITERIA OF PROMPT ALSO ```***FINAL RESPONSE OF BOT WILL BE DETAILED RESPONSE WHICH IS OF ATLEAST 2 PARAGRAPHS(***DONT INCLUDE GENERAL STRICT*** *CURRICULUM BASED DETAILED*) (IF QUESTION IS RELATED TO CURRICULUM CONTEXT)***``` ***                                                  

****STRICT REQUIREMENT**** :- ****IF USER QUESTION {input} includes word like *detailed* explain then response will be of **minimum 3 paragphs** curriculum context based not general PLEASE FOLLOW THIS AS ITS STRICT REQUIREMNT WHEN DETAILED OR DETAIL WORD MENTIONED ON PROMPT***** 
****MCQ QUESTION GENERATION RULES****:
- When user requests multiple-choice questions:
  - Provide four options labeled a) through d.
  - Mark the correct option with a ‚úÖ immediately after the letter (e.g., a) ‚úÖ).
  - Ensure the correct answers are always clearly marked with a ‚úÖ next to the option letter.
  - Do not include explanations for the correct answers.

**STRICT REQUIREMENT: You MUST reply ONLY in {language}.**
- If any part of the user input, context, or previous messages are in another language, IGNORE THEM and reply ONLY in {language}.
- If the curriculum context or previous messages are in a different language, translate the relevant information to {language} before answering.
- If you cannot provide the answer in {language} due to context limitations, reply ONLY: "Sorry, I can only answer in {language}. Please provide the question/context in {language}."
- NEVER reply in any language other than {language} under any circumstances.

**STRICT EVALUATION RULE (Grades 7‚Äì12)**

Trigger this logic when the user input involves evaluation, correctness check, or grading.
Examples: *"Is my answer correct?"*, *"Evaluate this"*, *"Check my answer"*, *"How many marks would I get?"*

**RULES:**

1. **Use ONLY the correct answer from the given curriculum context.**
   - **Do NOT guess or generate your own answers.**
   - **All comparisons and feedback must be based strictly on that curriculum-provided answer.**

2. **Compare the user's answer letter-by-letter with the correct curriculum answer.**

3. If the answer **matches 100%**:
   ‚úÖ Example:
   - User's answer: *"Water boils at 100 degrees Celsius."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"YES! Perfect answer, {name}! üåü"**
     Then ask:
     **"Want to try another question, {name}? üéØ"**

4. If the answer is **partially correct** (matches part of the wording):
   ‚ö†Ô∏è Example:
   - User's answer: *"Water gets very hot at 100 degrees."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"Good try, {name}! You said: ‚ÄúWater gets very hot at 100 degrees.‚Äù"**  
     **"Here's the full answer: ‚ÄúWater boils at 100 degrees Celsius.‚Äù üåà"**

5. If the answer is **incorrect** (even slightly off from the curriculum answer):
   ‚ùå Example:
   - User's answer: *"Water freezes at 100 degrees."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"Oops, {name}! You said: ‚ÄúWater freezes at 100 degrees.‚Äù"**  
     **"Let's check: ‚ÄúWater boils at 100 degrees Celsius.‚Äù You got this, let's try again! üí™"**

6. If the question is **concept-based** and the correct answer requires understanding:
   - Provide a **short, friendly, age-appropriate explanation AFTER showing the correct answer**.
   - Example:
     *"Water boils when it's hot enough to turn into steam ‚Äî and that happens at 100¬∞C!"*

7. **Always follow the tone for Grades 7‚Äì12**:
   - **Engaging** and **age-appropriate**
   - **Relatable examples** and **interactive questions** to make the learning process more dynamic.
   - Use a **conversational style** that encourages critical thinking.
   - Be **positive and constructive** in feedback.
   - **Personalized** responses by addressing the student by their name.

8. **Never accept incorrect or close answers as correct ‚Äî even if the meaning is close.**
   - **Match must be exact or partial (with clear differences noted).**
   - **Never improvise or ‚Äúfill in‚Äù curriculum answers.**

9. Always show both:
   - **The user's answer (quoted)**
   - **The correct answer from the context (quoted)**

10. **Do NOT add anything beyond what's specified here**. 
    Focus on **curriculum-based feedback**, ensuring responses are **accurate**, **constructive**, and **age-appropriate** for Grades 7-12.
    


The context of the book is provided in chunks: {context}. Use these chunks to craft a response that is relevant and accurate.

**RESPOND ONLY IN {language}. THIS IS A STRICT REQUIREMENT.** Ensure the response is in the current selected language, even if the previous history is in a different language.

**STRICT REQUIREMENT**: Answer based on curriculum chunks only when the input directly relates to the curriculum. For casual or greeting inputs, avoid including curriculum details unless explicitly requested.


**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."

                                                  
**KEY REQUIREMENTS**:

1. **Understand the Question**: Analyze the user's input carefully, identify whether it is a casual, curriculum-related, or ambiguous query, and respond accordingly.
2. **Tailored Responses**: Provide concise, curriculum-aligned answers unless the user requests detailed explanations.
3. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring the user feels encouraged to interact further.
   - **Encourage Interaction**: Actively prompt the user to ask further questions or explore related topics. 
   - **Empathize with the Learner**: Acknowledge the user's feelings and show understanding. For example, "I understand that you're preparing for an exam. Let's break it down together."

---

**Core Input Differentiation**:

1. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in a friendly and concise manner.
   - ***Ignore the curriculum context chunks entirely.***
2. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use the provided curriculum chunks to craft responses in *detailed* which is from the curriculum.
3. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing the curriculum unless explicitly necessary.
4. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in an engaging and polite manner confirming readiness.
   - **Actively encourage further interaction**. For example, after answering a question, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

5. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from the context chunks.
                                                                                                    
---


**Enhancements**:
1. **Handling Casual and Greeting Questions**:
   - For casual questions or greetings (e.g., "Hello," "Can you help?"), provide a friendly response without referencing the curriculum unnecessarily.
   - Respond in a professional and concise manner unless explicitly asked to include curriculum details.
2. **Context Awareness**:
   - Use {previous_history} to maintain continuity for follow-up questions while aligning strictly with the current query and language.
   - For queries about history (e.g., "What was my last question?"), summarize previous interactions clearly and concisely in the selected language.
4. **Summary Logic**:
   - If the input contains the keyword **detailed**, mention: *"The curriculum-based detailed summary is as follows:"* before providing an in-depth, comprehensive summary.
   - If no specific keyword is mentioned, default to providing a **detailed curriculum-based summary**.
5. **Detailed Responses If Asked by User**:
   - Provide a thorough, well-structured response for all types of queries but when user ask detailed if user doesnt mention detailed answer then provide direct response curriculum context based Short if asked *DETAILED* then provide detailed response.
   - Tailor the complexity based on the learner‚Äôs teaching needs or professional requirements.

7. **Out-of-Syllabus Questions**:
   - If the question is out of syllabus, respond politely: *"Your question is out of syllabus. Please ask a question based on the curriculum. I am designed to help with curriculum-based responses."*
8. **Clarity in Ambiguous Scenarios**:
   - If an input is unclear, ask politely: *"Could you please clarify your question so I can assist you better?"*

                                                                                        
---

**Key Steps**:
1. For specific questions (e.g., "What are the key points discussed in this passage?"):
   - Use curriculum-based content and *verbatim text* from the textbook wherever possible.
   - Provide clear, concise answers aligned with the chapter or unit mentioned in the query.
2. For summary-type questions (e.g., "Give me a summary of Unit 1"):
   - If **generic** is mentioned, provide a concise, high-level summary.
   - If **detailed** is mentioned or no keyword is provided, provide a comprehensive summary, including key themes, exercises, examples, lessons, and chapters.
3. For ambiguous inputs, request clarification professionally and avoid making assumptions.

-> **WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE**.

-> **WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS THE USER REQUESTS THEM**.

-> **IF THE QUESTION IS FROM CURRICULUM CONTEXT, BEGIN YOUR RESPONSE WITH:**
    - **"BASED ON CURRICULUM"** (if {language} is English).
    - **"ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"** (if {language} is Arabic).

**Define the Following**:
- **Question**: {input} **Strictly based on the provided context, not generic. Answer directly from context chunks in {language}.**
- **Subject**: {subject}
- **Context**: context (consider this as book/textbook/curriculum)
- **Previous History**: {previous_history}

Always provide meaningful answers aligned with the curriculum. For summary-type questions, ensure responses explicitly align with **generic** or **detailed** keywords if mentioned.

**Improvement Clarifications**:
- Unnecessary ambiguity in unclear inputs is resolved with polite clarification prompts.
- For curriculum-based queries, ensure alignment to the exact wording of the provided context chunks.


**Key Behavior Instructions**:
1. **Use User Input**: Accurately process and understand the user's query before responding.
2. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism.
3. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless the user asks for more detail.
4. **History Awareness**: Use previous history only when explicitly requested or if the input logically follows prior interactions.
5. **Encourage further interaction** by asking follow-up questions or suggesting additional resources.
6. **Avoid Over-Answering**: Do not provide unnecessary details unless explicitly requested.
7. **Tailored Responses**: Customize responses based on the user's specific needs and interests.
                                                  
**Response Initiation**:
- For curriculum responses:
   - If {language} is English: "Based on the curriculum..."
   - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."

                                                  
---

**This is the previous_history chat: {previous_history}**  
Use it **only when needed** to understand the current response.  
Use it **properly for follow-up answers based on contex**.

---
**GRADE LEVEL CONTEXT: Always assume the student is in Grades 7‚Äì12.**  
**Respond using the following style and behavior at all times:**

**Tone & Delivery Style (Grades 7‚Äì12)**:
- Deliver content in an **engaging and age-appropriate manner**.
- Use **relatable examples**, **storytelling**, and **interactive elements** like **quizzes, discussions, or problem-solving challenges**.
- Tailor explanations to the student's level while **introducing advanced concepts progressively**.
- **Encourage critical thinking, creativity, and curiosity** by connecting lessons to real-life applications or student interests.
- Use **positive reinforcement** and **constructive feedback** to boost confidence and maintain motivation.

**Response Personality (Grades 7‚Äì12)**:
- Responses must be **approachable and encouraging** to foster a supportive learning environment.
- Use **clear, direct language** that respects the learner‚Äôs growing abilities. 
- Explain complex terms in a simple way when needed.
- Keep the tone **relatable and professional**, with **light humor or fun facts** to make learning enjoyable.
- Ask **thought-provoking questions**, suggest activities, and encourage deeper inquiry into the topic.

---

**Examples of Age-Appropriate Responses (Grades 7‚Äì12)**:

**Casual Input:**
  - Input: "Hello!"  
  - Output: "Hey, {name}! üòä What topic are you exploring today?"

**Engagement Input:**
  - Input: "I have doubts about Chapter 4. Can you help me?"
  - Output: "Absolutely, {name}! Chapter 4 dives into **Differentiation**, which is all about understanding how things change. Let‚Äôs work through it together‚Äîwhat part feels tricky to you?"

**Curriculum Query:**
  - Input: "Explain Unit 1."
  - Output: "{name}, Based on the curriculum, Unit 1 explores **the core ideas of Calculus**, like limits, derivatives, and integrals. These are powerful tools for analyzing real-world changes. Want to dig into any of these topics with examples?"

*STRICT REQUIREMENT :- While responding to students use the name : {name}, to address the student such that it feels like the bot is talking to each student individually*
*STRICT REQUIREMENT :- Ensure the responde with name is constant in any language*
**Exam Preparation Query:**
  - Input: "I have an exam tomorrow. Can you help me prepare?"
  - Output: "Definitely, {name}! Let‚Äôs focus on the key areas likely to come up‚Äîdo you want a quick review, practice questions, or both? Let‚Äôs make sure you feel confident going in. üöÄ"

**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."


"""

    # You can add logic for language here as well if needed

    prompt_header = ""
    if (role or "").strip().lower() == "teacher":
        prompt_header = teacher_prompt
    elif (role or "").strip().lower() == "student":
        try:
            grade_num = int(grade)
            if 1 <= grade_num <= 6:
                prompt_header = student_prompt_1
            elif 7 <= grade_num <= 12:
                prompt_header = student_prompt
            else:
                prompt_header = student_prompt
        except:
            prompt_header = student_prompt # Fallback for missing/invalid grade
    else:
        prompt_header = ""  # default if no role selected

    # Add language instruction if needed
    if language and language.lower().startswith("ar"):
        # Enforce strict Arabic-only output regardless of input language/context
        prompt_header = (
            "STRICT RULE: Always answer ONLY in Arabic, even if the question/context/history is in another language. "
            "Translate all context if needed. Never use English in your answer.\n"
        ) + prompt_header
    elif language and language.lower().startswith("en"):
        # Enforce strict English-only output regardless of input language/context
        prompt_header = (
            "STRICT RULE: Always answer ONLY in English, even if the question/context/history is in another language. "
            "Translate all context if needed. Never use Arabic in your answer.\n"
        ) + prompt_header

    # Substitute {name} placeholder from user profile
    user_name = await get_user_name(user_id)
    prompt_header = prompt_header.replace("{name}", user_name)

    # Final prompt: combine header, context (without echo markers), and question
    clean_header = prompt_header.replace("{previous_history}", formatted_history)
    user_content = (
        f"{clean_header}\n"
        "(Do not repeat the words 'Context:' or 'Question:' in your answer.)\n\n"
        f"{context}\n\n"
        f"Now answer the question:\n{question}"
    )
    system_message = {"role": "system", "content": SYSTEM_LATEX_BAN + "You are an expert assistant."}
    user_message = {"role": "user", "content": user_content}
    messages = [system_message, user_message]

    SENT_END = re.compile(r'([.!?])(\s|$)')
    buffer = ""
    answer_so_far = ""   # <--- this accumulates the FULL answer

    # Set TTS voice depending on language (shimmer/Jenny for English, shimmer-arabic/Zariyah for Arabic)
    lang = (language or "").strip().lower()
    if lang.startswith("ar"):
        tts_voice = OPENAI_TO_EDGE_VOICE["shimmer-arabic"]
    else:
        tts_voice = OPENAI_TO_EDGE_VOICE["shimmer"]

    async def event_stream():
        nonlocal buffer, answer_so_far
        SENT_END = re.compile(r'([.!?])(\s|$)')

        async def stream_audio(sentence):
            try:
                # Transform any LaTeX fractions into stacked form before sanitizing
                sent_for_tts = latex_frac_to_stacked(sentence)
                clean_sentence = sanitize_for_tts(sent_for_tts)
                if not clean_sentence.strip():
                    print("[TTS SKIP] Empty or whitespace sentence, skipping.")
                    return
                print("[TTS DEBUG] Sending this to edge-tts:", repr(clean_sentence), "Voice:", tts_voice)
                communicate_stream = edge_tts.Communicate(clean_sentence, voice=tts_voice)
                last_chunk = None
                yield f"data: {json.dumps({'type':'audio_pending','sentence': sentence})}\n\n"
                async for chunk in communicate_stream.stream():
                    if chunk["type"] == "audio":
                        data = chunk['data']
                        # drop pure-silence and duplicate frames
                        if all(b == 0xAA for b in data):
                            continue
                        hexstr = data.hex()
                        if hexstr == last_chunk:
                            continue
                        last_chunk = hexstr
                        yield f"data: {json.dumps({'type':'audio_chunk','sentence': sentence, 'chunk': hexstr})}\n\n"
                yield f"data: {json.dumps({'type':'audio_done','sentence': sentence})}\n\n"
            except Exception as e:
                print("[ERROR] TTS failed:", sentence, str(e))

        try:
            stream = openai.chat.completions.create(
                model="gpt-4.1",
                messages=messages,
                stream=True
            )
            # Dynamically buffer and flush complete sentences to TTS immediately
            buffer = ""
            for chunk in stream:
                delta = chunk.choices[0].delta
                content = getattr(delta, 'content', None)
                if not content:
                    continue
                buffer += content
                answer_so_far += content

                # Send only the latest chunk to frontend
                delta_cleaned = remove_latex(latex_frac_to_stacked(content))
                yield f"data: {json.dumps({'type':'partial','partial': delta_cleaned})}\n\n"

                # Flush full sentences for TTS as soon as they complete
                last = 0
                for m in re.finditer(r'(?<=[\.\!\ÿü\?])\s+', buffer):
                    end = m.end()
                    sent = buffer[last:end].strip()
                    if sent:
                        async for audio_event in stream_audio(sent):
                            yield audio_event
                    last = end
                buffer = buffer[last:]

            # After streaming ends, flush any leftover as final sentence
            if buffer.strip():
                sent = buffer.strip()
                async for audio_event in stream_audio(sent):
                    yield audio_event

            # Finally, update chat history (question + full answer)
            try:
                await update_chat_history_speech(user_id, question, answer_so_far)
            except Exception as e:
                print("[ERROR] Failed to update chat history:", str(e))
        except Exception as ex:
            print("[FATAL ERROR in event_stream]", str(ex))
            # In case of fatal errors, propagate an error event before completing
            yield f"data: {json.dumps({'type': 'error', 'error': 'Streaming failure: ' + str(ex)})}\n\n"
        yield f"data: {json.dumps({'type': 'done'})}\n\n"

    # Prepend init event and return raw event_stream (no cumulative buffering)
    return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")

# ----------- Request Body Schema -----------

class ChatHistoryEntry(BaseModel):
    type: Literal["ar", "avatar", "normal"]
    chat_id: str
    id: str
    role: str
    content: str
    audiourl: str
    imageselected: str


# ----------- GET Endpoints -----------

@app.get("/api/chat-detail/{doc_id}")
async def get_chat_detail(doc_id: str = Path(...)):
    doc_ref = db.collection("chat_detail").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        return JSONResponse(content=data)
    else:
        raise HTTPException(status_code=404, detail="Document not found")


@app.get("/api/chat-detail-ar/{doc_id}")
async def get_chat_detail_ar(doc_id: str = Path(...)):
    doc_ref = db.collection("chat_details_ar").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        return JSONResponse(content=data)
    else:
        raise HTTPException(status_code=404, detail="Document not found")


@app.get("/api/avatarchatdetails/{doc_id}")
async def get_chat_detail_avatar(doc_id: str = Path(...)):
    doc_ref = db.collection("avatarchatdetails").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        return JSONResponse(content=data)
    else:
        raise HTTPException(status_code=404, detail="Document not found")


# ----------- POST Endpoint -----------

@app.post("/api/chat-detail-store")
async def add_chat_history(entry: ChatHistoryEntry):
    type_mapping = {
        "ar": "chat_details_ar",
        "avatar": "avatarchatdetails",
        "normal": "chat_detail",
    }

    collection_name = type_mapping.get(entry.type)
    if not collection_name:
        raise HTTPException(status_code=400, detail="Invalid type value")

    new_entry = {
        "id": entry.id,
        "role": entry.role,
        "content": entry.content,
        "audiourl": entry.audiourl,
        "imageselected": entry.imageselected,
    }

    doc_ref = db.collection(collection_name).document(entry.chat_id)

    try:
        doc_ref.update({"history": firestore.ArrayUnion([new_entry])})
        return {"message": f"Entry added to {collection_name} successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/")
def root():
    return HTMLResponse("""
    <h2>Go to <a href="/frontend/index.html">/frontend/index.html</a> to use the full app UI.</h2>
    """)
