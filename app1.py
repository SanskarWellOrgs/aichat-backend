import os
import uuid
import re
import json
import string
import random
import sys  # Add this import
from fastapi import FastAPI, UploadFile, File, Request, Query, HTTPException
from fastapi.responses import StreamingResponse, HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import openai
import requests
import io
import base64
import edge_tts
from PyPDF2 import PdfReader
import tiktoken
from runware import Runware, IImageInference
import urllib.parse
import httpx
import aiohttp
import pprint
import asyncio
from werkzeug.utils import secure_filename
from PIL import Image
import pytesseract
from fastapi import FastAPI, HTTPException, Path
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Literal
from google.cloud import firestore
from datetime import datetime
from langchain_community.vectorstores import FAISS
import asyncio
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import time
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor

# ============================================================================
# GLOBAL MODEL CACHE - PRE-LOAD EVERYTHING AT STARTUP FOR OPTIMAL PERFORMANCE
# ============================================================================

class GlobalModelCache:
    """Pre-load and cache all models to eliminate startup delays"""
    
    def __init__(self):
        self.embedding_model = None
        self.tokenizer = None
        self.text_splitter = None
        self.huggingface_embeddings = None
        self.is_initialized = False
        
    async def initialize_models(self):
        """Pre-load all models at startup"""
        if self.is_initialized:
            return
            
        print("[STARTUP] üöÄ Pre-loading models for optimal performance...")
        start_time = time.time()
        
        # Pre-load embedding model (biggest bottleneck)
        print("[STARTUP] Loading SentenceTransformer model...")
        self.embedding_model = await asyncio.to_thread(
            SentenceTransformer, 'all-MiniLM-L6-v2'
        )
        print(f"[STARTUP] ‚úÖ SentenceTransformer loaded in {time.time() - start_time:.2f}s")
        
        # Pre-load HuggingFace embeddings for FAISS
        print("[STARTUP] Loading HuggingFace embeddings...")
        self.huggingface_embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("[STARTUP] ‚úÖ HuggingFace embeddings loaded")
        
        # Pre-load tokenizer
        print("[STARTUP] Loading tokenizer...")
        self.tokenizer = tiktoken.encoding_for_model("gpt-4")
        print("[STARTUP] ‚úÖ Tokenizer loaded")
        
        # Pre-initialize text splitter (optimized chunk size)
        print("[STARTUP] Initializing text splitter...")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=10000,  # Reduced from 25000 for faster processing
            chunk_overlap=2000  # Reduced from 5000
        )
        print("[STARTUP] ‚úÖ Text splitter initialized")
        
        self.is_initialized = True
        total_time = time.time() - start_time
        print(f"[STARTUP] üéâ All models pre-loaded in {total_time:.2f}s")
        
    def get_embedding_model(self):
        """Get pre-loaded embedding model"""
        if not self.is_initialized:
            raise RuntimeError("Models not initialized! Call initialize_models() first")
        return self.embedding_model
        
    def get_huggingface_embeddings(self):
        """Get pre-loaded HuggingFace embeddings"""
        if not self.is_initialized:
            raise RuntimeError("Models not initialized! Call initialize_models() first")
        return self.huggingface_embeddings
        
    def get_tokenizer(self):
        """Get pre-loaded tokenizer"""
        if not self.is_initialized:
            raise RuntimeError("Models not initialized! Call initialize_models() first")
        return self.tokenizer
        
    def get_text_splitter(self):
        """Get pre-initialized text splitter"""
        if not self.is_initialized:
            raise RuntimeError("Models not initialized! Call initialize_models() first")
        return self.text_splitter

# Global model cache instance
model_cache = GlobalModelCache()

# ============================================================================
# PRE-COMPILED REGEX PATTERNS FOR PERFORMANCE OPTIMIZATION
# ============================================================================

# Text processing patterns
LATEX_PATTERN = re.compile(r'\\[a-zA-Z]+')
WHITESPACE_PATTERN = re.compile(r'\s+')
CURLY_BRACES_PATTERN = re.compile(r'[{}]')
UNDERSCORE_PATTERN = re.compile(r'_')

# Emoji removal pattern
EMOJI_PATTERN = re.compile(
    "["
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map symbols
    "\U0001F1E0-\U0001F1FF"  # flags (iOS)
    "\U00002702-\U000027B0"
    "\U000024C2-\U0001F251"
    "]+"
)

# Sentence splitting patterns (for both English and Arabic)
SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[\.\!\ÿü\?])\s+')
SENTENCE_END_PATTERN = re.compile(r'([.!?])(\s|$)')

# Punctuation removal pattern
PUNCTUATION_PATTERN = re.compile(r'[{}]+'.format(re.escape(string.punctuation)))

# MCQ detection patterns
MCQ_ANSWER_PATTERN = re.compile(r'\d+\.\s*[a-d]', re.IGNORECASE)
MCQ_SEQUENCE_PATTERN = re.compile(r'(\d+\.\s*[a-d]\s*)+', re.IGNORECASE)

# Arabic text detection pattern
ARABIC_CHAR_PATTERN = re.compile(r'[\u0600-\u06FF]')

# LaTeX math cleanup patterns
LATEX_COMMANDS_PATTERN = re.compile(r'\\[a-zA-Z]+')
DOLLAR_SIGNS_PATTERN = re.compile(r'\$')
BACKSLASH_PATTERN = re.compile(r'\\')

print("[REGEX] Pre-compiled regex patterns loaded successfully")

# ============================================================================

# Inline RAG helper functions (formerly in backend/rag_text_response_image56.py)
try:
    import firebase_admin
    from firebase_admin import credentials, firestore, storage
except ImportError as e:
    raise RuntimeError(
        "Missing 'firebase_admin' dependency: please install via `pip install firebase-admin google-cloud-firestore google-cloud-storage`"
    ) from e
import shutil
async def generate_complete_tts(text: str, language: str) -> str:
    """Generate complete TTS audio file and return URL"""
    try:
        # Determine voice based on language
        if language.lower().startswith('ar'):
            voice = "ar-SA-ZariyahNeural"
        else:
            voice = "en-US-AriaNeural"
        
        # Clean text for TTS
        clean_text = sanitize_for_tts(text)
        if not clean_text.strip():
            return ""
        
        # Generate unique filename
        timestamp = int(time.time() * 1000)
        filename = f"tts_{timestamp}.mp3"
        filepath = os.path.join(AUDIO_DIR, filename)
        
        # Generate TTS
        communicate = edge_tts.Communicate(clean_text, voice)
        await communicate.save(filepath)
        
        # Return URL for the audio file
        audio_url = f"https://ai-assistant.myddns.me:8443/audio/{filename}"
        print(f"[TTS] Generated audio: {audio_url}")
        return audio_url
        
    except Exception as e:
        print(f"[TTS ERROR] Failed to generate audio: {e}")
        return ""

import time
import tempfile
from pathlib import Path as FilePath
import base64
import logging
from concurrent.futures import ThreadPoolExecutor
import boto3
from botocore.exceptions import ClientError
def get_firebase_secret_from_aws(secret_name, region_name):
    """Fetch Firebase credentials JSON from AWS Secrets Manager."""
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    try:
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    except ClientError as e:
        raise RuntimeError(f"Error retrieving secret: {e}")
    secret = get_secret_value_response['SecretString']
    return json.loads(secret)

# LangChain & FAISS imports for curriculum RAG
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredImageLoader
from langchain_community.vectorstores import FAISS

# Initialize logging
logging.basicConfig(level=logging.DEBUG)

# Firebase setup (ensure service account JSON is present alongside app1.py)
# Firebase setup (loads from FIREBASE_JSON env var if available, else from file)
async def vector_embedding(curriculum_id, file_url):
    """Create vector embeddings using pre-loaded models for optimal performance"""
    # Use pre-loaded models
    embeddings = model_cache.get_huggingface_embeddings()
    text_splitter = model_cache.get_text_splitter()
    
    FAISS_LOCAL_LOCATION = f'../faiss/faiss_index_{curriculum_id}'
    
    # ---- LOCAL INDEX CHECK ----
    if os.path.exists(os.path.join(FAISS_LOCAL_LOCATION, "index.faiss")) and os.path.exists(os.path.join(FAISS_LOCAL_LOCATION, "index.pkl")):
        print(f"[OPTIMIZED] ‚ö° Loading existing FAISS index for {curriculum_id}")
        try:
            # Load with allow_dangerous_deserialization=True for our own safe files
            vectors = await asyncio.to_thread(
                FAISS.load_local, 
                FAISS_LOCAL_LOCATION, 
                embeddings,
                allow_dangerous_deserialization=True
            )
        except Exception as e:
            print(f"[OPTIMIZED] ‚ö†Ô∏è FAISS loading failed: {e}, rebuilding index")
            # Fallback to building new index
            vectors = await _build_new_index_optimized(curriculum_id, file_url, embeddings, text_splitter, FAISS_LOCAL_LOCATION)
    else:
        print(f"[OPTIMIZED] üî® Building new FAISS index for {curriculum_id}")
        vectors = await _build_new_index_optimized(curriculum_id, file_url, embeddings, text_splitter, FAISS_LOCAL_LOCATION)
    
    return vectors

async def _build_new_index_optimized(curriculum_id, file_url, embeddings, text_splitter, faiss_location):
    """Build new FAISS index using pre-loaded models"""
    file_path = await download_file(file_url, curriculum_id)
    file_extension = file_path.split('.')[-1].lower()

    try:
        if file_extension == 'pdf':
            loader = PyPDFLoader(file_path)
        elif file_extension == 'docx':
            loader = Docx2txtLoader(file_path)
        else:
            raise ValueError("Unsupported file type. Only PDF and DOCX are allowed.")

        print(f"[OPTIMIZED] üìÑ Loading document: {file_path}")
        docs = await asyncio.to_thread(loader.load)
        
        print(f"[OPTIMIZED] ‚úÇÔ∏è Splitting document into optimized chunks")
        final_documents = await asyncio.to_thread(text_splitter.split_documents, docs)
        
        print(f"[OPTIMIZED] üß† Creating FAISS index from {len(final_documents)} chunks")
        vectors = await asyncio.to_thread(FAISS.from_documents, final_documents, embeddings)
        
        print(f"[OPTIMIZED] üíæ Saving FAISS index to {faiss_location}")
        await asyncio.to_thread(vectors.save_local, faiss_location)
        
        # Clean up
        if os.path.exists(file_path):
            os.remove(file_path)
            
        return vectors
        
    except Exception as e:
        # Clean up on error
        if os.path.exists(file_path):
            os.remove(file_path)
        raise e

if not firebase_admin._apps:
    FIREBASE_JSON = os.getenv("FIREBASE_JSON")
    local_cred_file = FilePath(__file__).parent / "aischool-ba7c6-firebase-adminsdk-n8tjs-669d9da038.json"
    if FIREBASE_JSON:
        print("[INFO] Loading Firebase credentials from FIREBASE_JSON environment variable.")
        cred = credentials.Certificate(json.loads(FIREBASE_JSON))
    elif local_cred_file.exists():
        print(f"[INFO] Loading Firebase credentials from local file: {local_cred_file}")
        cred = credentials.Certificate(str(local_cred_file))
    else:
        # Fetch from AWS Secrets Manager
        print("[INFO] Loading Firebase credentials from AWS Secrets Manager.")
        secret_name = "firebase-service-account-json"    # The secret name you used
        region_name = "ap-south-1"                       # Your AWS region
        firebase_creds = get_firebase_secret_from_aws(secret_name, region_name)
        cred = credentials.Certificate(firebase_creds)
    firebase_admin.initialize_app(cred, {"storageBucket": "aischool-ba7c6.appspot.com"})

# Firestore & Storage clients
db = firestore.client()
bucket = storage.bucket()

# ThreadPoolExecutor for any blocking calls
executor = ThreadPoolExecutor()

# ============================================================================
# OPTIMIZED CURRICULUM-BASED CHAT SESSION MANAGER WITH BACKGROUND PROCESSING
# ============================================================================

class OptimizedCurriculumChatSessionManager:
    """
    Optimized curriculum-based chat session manager with background processing.
    Pre-processes curriculum files when chat starts for instant responses.
    """
    def __init__(self):
        self.chat_sessions = {}  # chat_id -> {curriculum_id, vectors, last_access}
        self.url_cache = {}      # curriculum_id -> pdf_url (tiny memory)
        self.processing_queue = {}  # curriculum_id -> processing_status
        self.background_tasks = set()
    
    async def preprocess_curriculum_in_background(self, curriculum_id):
        """Process curriculum in background when chat starts"""
        if curriculum_id in self.processing_queue:
            return  # Already processing
            
        self.processing_queue[curriculum_id] = "processing"
        print(f"[BACKGROUND] üîÑ Pre-processing curriculum: {curriculum_id}")
        
        try:
            # Create background task
            task = asyncio.create_task(self._build_vectors_background(curriculum_id))
            self.background_tasks.add(task)
            task.add_done_callback(self.background_tasks.discard)
            
        except Exception as e:
            print(f"[BACKGROUND ERROR] Failed to start background processing: {e}")
            self.processing_queue[curriculum_id] = "failed"
    
    async def _build_vectors_background(self, curriculum_id):
        """Build vectors in background using pre-loaded models"""
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            idx_dir = os.path.join(base_dir, 'faiss', f'faiss_index_{curriculum_id}')
            
            # Check if already exists
            if os.path.exists(idx_dir) and os.path.exists(os.path.join(idx_dir, 'index.faiss')):
                print(f"[BACKGROUND] ‚úÖ Index already exists for {curriculum_id}")
                self.processing_queue[curriculum_id] = "completed"
                return
            
            # Get URL and build vectors
            if curriculum_id not in self.url_cache:
                self.url_cache[curriculum_id] = await get_curriculum_url(curriculum_id)
            
            pdf_url = self.url_cache[curriculum_id]
            vectors = await self._create_vectors_optimized(curriculum_id, pdf_url)
            self.processing_queue[curriculum_id] = "completed"
            print(f"[BACKGROUND] ‚úÖ Pre-processing completed for {curriculum_id}")
            
        except Exception as e:
            print(f"[BACKGROUND ERROR] Pre-processing failed for {curriculum_id}: {e}")
            self.processing_queue[curriculum_id] = "failed"
    
    async def get_vectors_for_chat(self, chat_id, curriculum_id):
        """Get vectors with optimized loading and background processing awareness"""
        
        # Check if this chat already has vectors loaded
        if chat_id in self.chat_sessions:
            session = self.chat_sessions[chat_id]
            if session['curriculum_id'] == curriculum_id:
                # Update last access time
                session['last_access'] = time.time()
                print(f"[OPTIMIZED] ‚ö° Using cached vectors for chat {chat_id}")
                return session['vectors']
            else:
                # Different curriculum - clean up old one first
                print(f"[OPTIMIZED] üîÑ Switching curriculum for chat {chat_id}")
                await self.cleanup_chat_session(chat_id)
        
        # Start background processing if not already started
        if curriculum_id not in self.processing_queue:
            asyncio.create_task(self.preprocess_curriculum_in_background(curriculum_id))
        
        # Check if background processing is complete
        if curriculum_id in self.processing_queue:
            status = self.processing_queue[curriculum_id]
            if status == "processing":
                print(f"[OPTIMIZED] ‚è≥ Waiting for background processing to complete...")
                # Wait for background processing with timeout
                wait_time = 0
                while self.processing_queue.get(curriculum_id) == "processing" and wait_time < 30:
                    await asyncio.sleep(0.5)
                    wait_time += 0.5
                
                if wait_time >= 30:
                    print(f"[OPTIMIZED] ‚ö†Ô∏è Background processing timeout, proceeding with direct processing")
        
        # Load vectors (should be fast if background processing completed)
        print(f"[OPTIMIZED] üöÄ Loading vectors for chat {chat_id}")
        vectors = await self._load_vectors_optimized(curriculum_id)
        
        # Store for this chat session
        self.chat_sessions[chat_id] = {
            'curriculum_id': curriculum_id,
            'vectors': vectors,
            'last_access': time.time()
        }
        
        print(f"[OPTIMIZED] ‚úÖ Vectors ready for chat {chat_id}")
        return vectors
    
    async def _load_vectors_optimized(self, curriculum_id):
        """Optimized vector loading using pre-loaded models - FIXED FAISS loading"""
        base_dir = os.path.dirname(os.path.abspath(__file__))
        idx_dir = os.path.join(base_dir, 'faiss', f'faiss_index_{curriculum_id}')
        
        if os.path.exists(idx_dir) and os.path.exists(os.path.join(idx_dir, 'index.faiss')):
            print(f"[OPTIMIZED] ‚ö° Loading existing FAISS index from disk")
            # Use pre-loaded embeddings
            embeddings = model_cache.get_huggingface_embeddings()
            
            try:
                # üîß FIXED: Add allow_dangerous_deserialization=True for loading our own safe files
                vectors = await asyncio.to_thread(
                    FAISS.load_local,
                    idx_dir,
                    embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"[OPTIMIZED] ‚úÖ Successfully loaded existing FAISS index")
                return vectors
            except Exception as e:
                print(f"[OPTIMIZED] ‚ö†Ô∏è FAISS loading failed: {e}")
                print(f"[OPTIMIZED] üî® Building new index instead")
                return await self._create_vectors_optimized(curriculum_id, await self._get_curriculum_url_cached(curriculum_id))
        else:
            print(f"[OPTIMIZED] üî® Building new index (background processing may have failed)")
            return await self._create_vectors_optimized(curriculum_id, await self._get_curriculum_url_cached(curriculum_id))
    
    async def _create_vectors_optimized(self, curriculum_id, pdf_url):
        """Create vectors using pre-loaded models"""
        # Use pre-loaded models
        embeddings = model_cache.get_huggingface_embeddings()
        text_splitter = model_cache.get_text_splitter()
        
        FAISS_LOCAL_LOCATION = f'faiss/faiss_index_{curriculum_id}'
        os.makedirs(os.path.dirname(FAISS_LOCAL_LOCATION), exist_ok=True)
        
        # Download file
        file_path = await download_file(pdf_url, curriculum_id)
        
        try:
            # Determine file type and load
            file_extension = file_path.split('.')[-1].lower()
            
            if file_extension == 'pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension == 'docx':
                loader = Docx2txtLoader(file_path)
            else:
                loader = PyPDFLoader(file_path)
            
            print(f"[OPTIMIZED] üìÑ Loading document: {file_path}")
            docs = await asyncio.to_thread(loader.load)
            
            print(f"[OPTIMIZED] ‚úÇÔ∏è Splitting document into optimized chunks")
            final_documents = await asyncio.to_thread(text_splitter.split_documents, docs)
            
            print(f"[OPTIMIZED] üß† Creating FAISS index from {len(final_documents)} chunks")
            vectors = await asyncio.to_thread(FAISS.from_documents, final_documents, embeddings)
            
            print(f"[OPTIMIZED] üíæ Saving FAISS index to {FAISS_LOCAL_LOCATION}")
            await asyncio.to_thread(vectors.save_local, FAISS_LOCAL_LOCATION)
            
            # Clean up downloaded file
            if os.path.exists(file_path):
                os.remove(file_path)
                
            return vectors
            
        except Exception as e:
            # Clean up on error
            if os.path.exists(file_path):
                os.remove(file_path)
            raise e
    
    async def _get_curriculum_url_cached(self, curriculum_id):
        """Get curriculum URL with caching"""
        if curriculum_id not in self.url_cache:
            print(f"[URL CACHE MISS] Fetching URL for {curriculum_id}")
            self.url_cache[curriculum_id] = await get_curriculum_url(curriculum_id)
        else:
            print(f"[URL CACHE HIT] Using cached URL for {curriculum_id}")
        
        return self.url_cache[curriculum_id]
    
    async def cleanup_chat_session(self, chat_id):
        """Delete vectors when chat ends"""
        if chat_id in self.chat_sessions:
            curriculum_id = self.chat_sessions[chat_id]['curriculum_id']
            # Delete vectors to free memory
            del self.chat_sessions[chat_id]
            print(f"[OPTIMIZED] üßπ Cleaned up chat session {chat_id}")
            return True
        return False
    
    async def cleanup_inactive_sessions(self, timeout_minutes=30):
        """Auto-cleanup sessions that haven't been used recently"""
        current_time = time.time()
        timeout_seconds = timeout_minutes * 60
        
        inactive_chats = []
        for chat_id, session in self.chat_sessions.items():
            if current_time - session['last_access'] > timeout_seconds:
                inactive_chats.append(chat_id)
        
        for chat_id in inactive_chats:
            await self.cleanup_chat_session(chat_id)
            print(f"[OPTIMIZED] üßπ Auto-cleaned inactive chat {chat_id}")
    
    def get_session_stats(self):
        """Get current session statistics"""
        return {
            "active_chats": len(self.chat_sessions),
            "chat_ids": list(self.chat_sessions.keys()),
            "curriculums_in_use": [s['curriculum_id'] for s in self.chat_sessions.values()],
            "background_processing": len([k for k, v in self.processing_queue.items() if v == "processing"]),
            "completed_processing": len([k for k, v in self.processing_queue.items() if v == "completed"]),
            "failed_processing": len([k for k, v in self.processing_queue.items() if v == "failed"]),
            "estimated_memory_mb": len(self.chat_sessions) * 150,
            "url_cache_size": len(self.url_cache),
            "approach": "optimized curriculum-based RAG with background processing and pre-loaded models"
        }

# Global optimized curriculum chat session manager
chat_session_manager = OptimizedCurriculumChatSessionManager()

# Legacy ChatSessionVectorManager for backward compatibility
ChatSessionVectorManager = OptimizedCurriculumChatSessionManager

# Chat-session based vector management for 200+ curriculums
import time
import asyncio
from contextlib import asynccontextmanager

class ChatSessionVectorManager:
    def __init__(self):
        self.chat_sessions = {}  # chat_id -> {curriculum_id, vectors, last_access}
        self.url_cache = {}      # curriculum_id -> pdf_url (tiny memory)
    
    async def get_vectors_for_chat(self, chat_id, curriculum_id):
        """Get vectors for specific chat session"""
        
        # Check if this chat already has vectors loaded
        if chat_id in self.chat_sessions:
            session = self.chat_sessions[chat_id]
            if session['curriculum_id'] == curriculum_id:
                # Update last access time
                session['last_access'] = time.time()
                print(f"[CHAT SESSION HIT] Using loaded vectors for chat {chat_id}")
                return session['vectors']
            else:
                # Different curriculum - clean up old one first
                print(f"[CHAT SESSION] Switching curriculum for chat {chat_id}")
                await self.cleanup_chat_session(chat_id)
        
        # Load vectors for this chat session
        print(f"[CHAT SESSION LOAD] Loading {curriculum_id} for chat {chat_id}")
        
        # Get URL (cache URLs only - tiny memory)
        if curriculum_id not in self.url_cache:
            print(f"[URL CACHE MISS] Fetching URL for {curriculum_id}")
            self.url_cache[curriculum_id] = await get_curriculum_url(curriculum_id)
        else:
            print(f"[URL CACHE HIT] Using cached URL for {curriculum_id}")
        
        pdf_url = self.url_cache[curriculum_id]
        
        # Load vectors
        vectors = await self._load_vectors_from_disk_or_build(curriculum_id, pdf_url)
        
        # Store for this chat session only
        self.chat_sessions[chat_id] = {
            'curriculum_id': curriculum_id,
            'vectors': vectors,
            'last_access': time.time()
        }
        
        print(f"[CHAT SESSION] Stored vectors for chat {chat_id}")
        return vectors
    
    async def cleanup_chat_session(self, chat_id):
        """Delete vectors when chat ends"""
        if chat_id in self.chat_sessions:
            curriculum_id = self.chat_sessions[chat_id]['curriculum_id']
            # Delete vectors to free memory
            del self.chat_sessions[chat_id]
            print(f"[CHAT SESSION CLEANUP] Deleted {curriculum_id} vectors for chat {chat_id}")
            return True
        return False
    
    async def cleanup_inactive_sessions(self, timeout_minutes=30):
        """Auto-cleanup sessions that haven't been used recently"""
        current_time = time.time()
        timeout_seconds = timeout_minutes * 60
        
        inactive_chats = []
        for chat_id, session in self.chat_sessions.items():
            if current_time - session['last_access'] > timeout_seconds:
                inactive_chats.append(chat_id)
        
        for chat_id in inactive_chats:
            await self.cleanup_chat_session(chat_id)
            print(f"[AUTO CLEANUP] Removed inactive chat {chat_id}")
    
    def get_session_stats(self):
        """Get current session statistics"""
        return {
            "active_chats": len(self.chat_sessions),
            "chat_ids": list(self.chat_sessions.keys()),
            "curriculums_in_use": [s['curriculum_id'] for s in self.chat_sessions.values()],
            "estimated_memory_mb": len(self.chat_sessions) * 150,  # ~150MB per curriculum
            "url_cache_size": len(self.url_cache),
            "approach": "chat-session based (perfect for 200+ curriculums)"
        }
    
    async def _load_vectors_from_disk_or_build(self, curriculum_id, pdf_url):
        """Load vectors from disk or build new index"""
        base_dir = os.path.dirname(os.path.abspath(__file__))
        idx_dir = os.path.join(base_dir, 'faiss', f'faiss_index_{curriculum_id}')
        exists = os.path.exists(idx_dir) and os.path.exists(os.path.join(idx_dir, 'index.faiss'))
        
        if exists:
            print(f"[FAISS] Loading existing index from {idx_dir}")
            vectors = await asyncio.to_thread(
                FAISS.load_local,
                idx_dir,
                HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL),
                allow_dangerous_deserialization=True,
            )
        else:
            print(f"[FAISS] Building new index for {curriculum_id}")
            vectors = await vector_embedding(curriculum_id, pdf_url)
        
        return vectors

# Global optimized curriculum chat session manager (already defined above)
# chat_session_manager = OptimizedCurriculumChatSessionManager() - already instantiated

# ============================================================================
# ENHANCED MULTI-MODAL BACKGROUND PROCESSING MANAGER
# ============================================================================

class EnhancedMultiModalBackgroundProcessor:
    """
    Enhanced background processor for curriculum, files, and images.
    Pre-processes all content types for instant responses.
    """
    def __init__(self):
        self.curriculum_processor = chat_session_manager  # Use existing curriculum processor
        self.file_processing_queue = {}  # file_hash -> processing_status
        self.image_processing_queue = {}  # image_hash -> processing_status
        self.file_cache = {}  # file_hash -> processed_content
        self.image_cache = {}  # image_hash -> processed_content
        self.background_tasks = set()
    
    async def preprocess_file_in_background(self, file_content, file_hash, file_type="pdf"):
        """Process uploaded file in background"""
        if file_hash in self.file_processing_queue:
            return  # Already processing
            
        self.file_processing_queue[file_hash] = "processing"
        print(f"[FILE-BACKGROUND] üîÑ Pre-processing uploaded file: {file_hash[:8]}...")
        
        try:
            # Create background task
            task = asyncio.create_task(self._process_file_background(file_content, file_hash, file_type))
            self.background_tasks.add(task)
            task.add_done_callback(self.background_tasks.discard)
            
        except Exception as e:
            print(f"[FILE-BACKGROUND ERROR] Failed to start file processing: {e}")
            self.file_processing_queue[file_hash] = "failed"
    
    async def preprocess_image_in_background(self, image_content, image_hash):
        """Process uploaded image in background"""
        if image_hash in self.image_processing_queue:
            return  # Already processing
            
        self.image_processing_queue[image_hash] = "processing"
        print(f"[IMAGE-BACKGROUND] üîÑ Pre-processing uploaded image: {image_hash[:8]}...")
        
        try:
            # Create background task
            task = asyncio.create_task(self._process_image_background(image_content, image_hash))
            self.background_tasks.add(task)
            task.add_done_callback(self.background_tasks.discard)
            
        except Exception as e:
            print(f"[IMAGE-BACKGROUND ERROR] Failed to start image processing: {e}")
            self.image_processing_queue[image_hash] = "failed"
    
    async def _process_file_background(self, file_content, file_hash, file_type):
        """Process file content in background using pre-loaded models"""
        try:
            # Use pre-loaded models
            embeddings = model_cache.get_huggingface_embeddings()
            text_splitter = model_cache.get_text_splitter()
            
            # Save file temporarily
            temp_file = f"temp_upload_{file_hash}.{file_type}"
            with open(temp_file, "wb") as f:
                f.write(file_content)
            
            try:
                # Load document
                if file_type.lower() == 'pdf':
                    loader = PyPDFLoader(temp_file)
                elif file_type.lower() == 'docx':
                    loader = Docx2txtLoader(temp_file)
                else:
                    loader = PyPDFLoader(temp_file)  # Default to PDF
                
                print(f"[FILE-BACKGROUND] üìÑ Loading uploaded document")
                docs = await asyncio.to_thread(loader.load)
                
                print(f"[FILE-BACKGROUND] ‚úÇÔ∏è Splitting document into chunks")
                final_documents = await asyncio.to_thread(text_splitter.split_documents, docs)
                
                print(f"[FILE-BACKGROUND] üß† Creating FAISS index from {len(final_documents)} chunks")
                vectors = await asyncio.to_thread(FAISS.from_documents, final_documents, embeddings)
                
                # Cache the processed vectors
                self.file_cache[file_hash] = {
                    'vectors': vectors,
                    'document_count': len(final_documents),
                    'processed_at': time.time()
                }
                
                self.file_processing_queue[file_hash] = "completed"
                print(f"[FILE-BACKGROUND] ‚úÖ File processing completed for {file_hash[:8]}")
                
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    
        except Exception as e:
            print(f"[FILE-BACKGROUND ERROR] Processing failed for {file_hash[:8]}: {e}")
            self.file_processing_queue[file_hash] = "failed"
    
    async def _process_image_background(self, image_content, image_hash):
        """Process image content in background"""
        try:
            # Save image temporarily
            temp_image = f"temp_image_{image_hash}.png"
            with open(temp_image, "wb") as f:
                f.write(image_content)
            
            try:
                # Process image with vision model
                print(f"[IMAGE-BACKGROUND] üñºÔ∏è Processing uploaded image")
                img = Image.open(temp_image).convert("RGB")
                
                # Extract text using OCR if needed
                try:
                    ocr_text = pytesseract.image_to_string(img)
                except:
                    ocr_text = ""
                
                # Get vision caption
                vision_caption = await vision_caption_openai(img=img)
                
                # Combine OCR and vision results
                combined_text = f"Vision Description: {vision_caption}\n\nExtracted Text: {ocr_text}".strip()
                
                # Cache the processed content
                self.image_cache[image_hash] = {
                    'vision_caption': vision_caption,
                    'ocr_text': ocr_text,
                    'combined_text': combined_text,
                    'processed_at': time.time()
                }
                
                self.image_processing_queue[image_hash] = "completed"
                print(f"[IMAGE-BACKGROUND] ‚úÖ Image processing completed for {image_hash[:8]}")
                
            finally:
                # Clean up temp file
                if os.path.exists(temp_image):
                    os.remove(temp_image)
                    
        except Exception as e:
            print(f"[IMAGE-BACKGROUND ERROR] Processing failed for {image_hash[:8]}: {e}")
            self.image_processing_queue[image_hash] = "failed"
    
    async def get_processed_file(self, file_hash, timeout=30):
        """Get processed file vectors with smart waiting"""
        # Check if already processed
        if file_hash in self.file_cache:
            print(f"[FILE-BACKGROUND] ‚ö° Using cached file processing for {file_hash[:8]}")
            return self.file_cache[file_hash]['vectors']
        
        # Wait for background processing if in progress
        if file_hash in self.file_processing_queue:
            status = self.file_processing_queue[file_hash]
            if status == "processing":
                print(f"[FILE-BACKGROUND] ‚è≥ Waiting for file processing to complete...")
                wait_time = 0
                while self.file_processing_queue.get(file_hash) == "processing" and wait_time < timeout:
                    await asyncio.sleep(0.5)
                    wait_time += 0.5
                
                if file_hash in self.file_cache:
                    return self.file_cache[file_hash]['vectors']
        
        # If not processed or failed, return None (fallback to regular processing)
        print(f"[FILE-BACKGROUND] ‚ö†Ô∏è File not ready, falling back to regular processing")
        return None
    
    async def get_processed_image(self, image_hash, timeout=30):
        """Get processed image content with smart waiting"""
        # Check if already processed
        if image_hash in self.image_cache:
            print(f"[IMAGE-BACKGROUND] ‚ö° Using cached image processing for {image_hash[:8]}")
            return self.image_cache[image_hash]
        
        # Wait for background processing if in progress
        if image_hash in self.image_processing_queue:
            status = self.image_processing_queue[image_hash]
            if status == "processing":
                print(f"[IMAGE-BACKGROUND] ‚è≥ Waiting for image processing to complete...")
                wait_time = 0
                while self.image_processing_queue.get(image_hash) == "processing" and wait_time < timeout:
                    await asyncio.sleep(0.5)
                    wait_time += 0.5
                
                if image_hash in self.image_cache:
                    return self.image_cache[image_hash]
        
        # If not processed or failed, return None (fallback to regular processing)
        print(f"[IMAGE-BACKGROUND] ‚ö†Ô∏è Image not ready, falling back to regular processing")
        return None
    
    def get_processing_stats(self):
        """Get processing statistics"""
        return {
            "curriculum_sessions": self.curriculum_processor.get_session_stats(),
            "file_processing": {
                "active": len([k for k, v in self.file_processing_queue.items() if v == "processing"]),
                "completed": len([k for k, v in self.file_processing_queue.items() if v == "completed"]),
                "failed": len([k for k, v in self.file_processing_queue.items() if v == "failed"]),
                "cached_files": len(self.file_cache)
            },
            "image_processing": {
                "active": len([k for k, v in self.image_processing_queue.items() if v == "processing"]),
                "completed": len([k for k, v in self.image_processing_queue.items() if v == "completed"]),
                "failed": len([k for k, v in self.image_processing_queue.items() if v == "failed"]),
                "cached_images": len(self.image_cache)
            },
            "background_tasks": len(self.background_tasks),
            "optimization_status": "‚úÖ MULTI-MODAL BACKGROUND PROCESSING ACTIVE"
        }
    
    async def cleanup_old_cache(self, max_age_hours=24):
        """Clean up old cached files and images"""
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        # Clean file cache
        old_files = []
        for file_hash, data in self.file_cache.items():
            if current_time - data['processed_at'] > max_age_seconds:
                old_files.append(file_hash)
        
        for file_hash in old_files:
            del self.file_cache[file_hash]
            if file_hash in self.file_processing_queue:
                del self.file_processing_queue[file_hash]
        
        # Clean image cache
        old_images = []
        for image_hash, data in self.image_cache.items():
            if current_time - data['processed_at'] > max_age_seconds:
                old_images.append(image_hash)
        
        for image_hash in old_images:
            del self.image_cache[image_hash]
            if image_hash in self.image_processing_queue:
                del self.image_processing_queue[image_hash]
        
        if old_files or old_images:
            print(f"[CLEANUP] üßπ Cleaned {len(old_files)} old files and {len(old_images)} old images")

# Global enhanced multi-modal processor
multi_modal_processor = EnhancedMultiModalBackgroundProcessor()

# ============================================================================
# OPTIMIZED LIFESPAN MANAGEMENT WITH MODEL PRE-LOADING
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    # üöÄ PRE-LOAD ALL MODELS AT STARTUP FOR OPTIMAL PERFORMANCE
    print("[STARTUP] üöÄ Initializing optimized AI assistant with pre-loaded models...")
    await model_cache.initialize_models()
    print("[STARTUP] ‚úÖ All models pre-loaded and ready for instant responses!")
    
    # Start auto-cleanup task
    cleanup_task = asyncio.create_task(auto_cleanup_sessions())
    
    yield
    
    # Cancel cleanup task on shutdown
    cleanup_task.cancel()
    print("[SHUTDOWN] üõë Shutting down optimized AI assistant")

async def auto_cleanup_sessions():
    """Enhanced background task to clean up inactive sessions and old cache"""
    while True:
        try:
            await asyncio.sleep(1800)  # Every 30 minutes
            # Clean up curriculum sessions
            await chat_session_manager.cleanup_inactive_sessions(timeout_minutes=30)
            # Clean up old file and image cache
            await multi_modal_processor.cleanup_old_cache(max_age_hours=24)
        except Exception as e:
            print(f"[AUTO CLEANUP ERROR] {e}")

async def get_or_load_vectors(curriculum, pdf_url):
    """
    DEPRECATED: This function is no longer used.
    Use chat_session_manager.get_vectors_for_chat() instead.
    """
    raise NotImplementedError("Use chat_session_manager.get_vectors_for_chat() instead")

async def get_curriculum_url(curriculum):
    """Fetch curriculum PDF URL from Firestore with enhanced error handling."""
    try:
        print(f"[RAG] Fetching curriculum document: {curriculum}")
        doc = await asyncio.to_thread(lambda: db.collection('curriculum').document(curriculum).get())
        
        if not doc.exists:
            print(f"[RAG][ERROR] Curriculum document not found: {curriculum}")
            raise ValueError(f"No curriculum found with ID: {curriculum}")
        
        data = doc.to_dict()
        print(f"[RAG] Document data: {data}")
        
        # Check both URL fields
        ocr_url = data.get('ocrfile_id')
        primary_url = data.get('url')
        print(f"[RAG] Found URLs - OCR: {ocr_url}, Primary: {primary_url}")
        
        # Prefer OCR version, fallback to primary URL
        url = ocr_url or primary_url
        
        if not url:
            print(f"[RAG][ERROR] No valid URL found in curriculum document: {curriculum}")
            raise ValueError(f"No valid URL found in curriculum document: {curriculum}")
            
        # Validate URL format
        if not url.startswith(('http://', 'https://')):
            print(f"[RAG][ERROR] Invalid URL format: {url}")
            raise ValueError(f"Invalid URL format in curriculum document: {url}")
            
        print(f"[RAG] Successfully retrieved URL for curriculum '{curriculum}': {url}")
        return url
        
    except Exception as e:
        print(f"[RAG][ERROR] Failed to fetch curriculum URL: {str(e)}")
        raise

def decode_arabic_text(text: str) -> str:
    """
    Decode potentially corrupted Arabic text back to proper Unicode.
    Handles various encoding issues including double-encoding.
    """
    if not isinstance(text, str):
        return str(text)
    
    try:
        # If text is already proper Arabic, return as is
        if any('\u0600' <= c <= '\u06FF' for c in text):
            return text
            
        # Try different encoding combinations to fix corrupted text
        encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
        for enc in encodings:
            try:
                # Try to decode assuming it was encoded with this encoding
                decoded = text.encode('latin1').decode(enc)
                # If we got Arabic text, return it
                if any('\u0600' <= c <= '\u06FF' for c in decoded):
                    return decoded
            except:
                continue
                
        # If above fails, try forcing utf-8 decode
        try:
            decoded = text.encode('latin1').decode('utf-8')
            return decoded
        except:
            pass
            
        # If all fails, return original
        return text
    except Exception as e:
        print(f"[ERROR] Arabic decode failed: {str(e)}")
        return text


async def fetch_chat_detail(chat_id):
    """Fetch and format the last 3 QA pairs of chat history."""
    chat_ref = db.collection('chat_detail').document(chat_id)
    chat_doc = chat_ref.get()
    if not chat_doc.exists:
        return ""
    data = chat_doc.to_dict()
    history = data.get('history', [])
    users = [e for e in history if e.get('role') in ['2','3']]
    bots = [e for e in history if e.get('role') == 'assistant']
    pairs = list(zip(users, bots))
    recent = pairs[-3:]
    out = ''
    for u,a in recent:
        out += f"My Question: {u.get('content','')}\nBot Response: {a.get('content','')}\n"
    return out

async def get_chat_history(chat_id):
    """Alias to fetch formatted chat history for RAG context."""
    return await fetch_chat_detail(chat_id)

async def get_user_name(user_id: str) -> str:
    """
    Fetch the user's display name from Firestore users collection.
    Returns empty string if not found or on error.
    """
    try:
        doc = await asyncio.to_thread(lambda: db.collection('users').document(user_id).get())
        if doc.exists:
            return doc.to_dict().get('name', '') or ''
    except Exception:
        pass
    return ''



async def download_file(url, curriculum):
    ext = url.split('.')[-1]
    path = f'curriculum_{curriculum}.{ext}'
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            if resp.status == 200:
                with open(path, 'wb') as f:
                    f.write(await resp.read())
            else:
                raise IOError(f"Download failed: {resp.status}")
    return path

 
        

async def retrieve_documents(vectorstore, query: str, max_tokens: int = 30000, k: int = 5):
    """Optimized document retrieval with performance tracking and pre-loaded tokenizer"""
    start_time = time.time()
    print(f"[OPTIMIZED] üöÄ Starting optimized document retrieval for query: '{query[:50]}...', k={k}")
    
    # Use pre-loaded tokenizer for faster token counting
    encoder = model_cache.get_tokenizer()
    
    # similarity_search is sync; run in thread to avoid blocking the event loop
    docs = await asyncio.to_thread(vectorstore.similarity_search, query, k=k)
    search_time = time.time() - start_time
    print(f"[OPTIMIZED] ‚ö° Vector search completed in {search_time:.3f}s, found {len(docs)} docs")
    
    if docs:
        print(f"[OPTIMIZED] üìÑ First doc snippet: {docs[0].page_content[:100]!r}")
        print(f"[OPTIMIZED] üìã First doc metadata: {docs[0].metadata}")
    else:
        print(f"[OPTIMIZED] ‚ùå No documents retrieved for query: {query}")
        return []
    
    # Optimized token counting and filtering
    token_start = time.time()
    total = 0
    out = []
    for i, d in enumerate(docs):
        nt = len(encoder.encode(d.page_content))
        if total + nt <= max_tokens:
            out.append(d)
            total += nt
        else:
            print(f"[OPTIMIZED] ‚úÇÔ∏è Stopping at doc {i} due to token limit ({total} + {nt} > {max_tokens})")
            break
    
    token_time = time.time() - token_start
    total_time = time.time() - start_time
    
    print(f"[OPTIMIZED] ‚ö° Token filtering completed in {token_time:.3f}s")
    print(f"[OPTIMIZED] ‚úÖ Final selection: {len(out)} docs, {total} total tokens")
    print(f"[OPTIMIZED] üéØ Total retrieval time: {total_time:.3f}s (target: <0.5s)")
    return out
    
    print(f"[RETRIEVE] Token filtering completed in {token_time:.3f}s")
    print(f"[RETRIEVE] Final selection: {len(out)} docs, {total} total tokens")
    print(f"[RETRIEVE] Total retrieval time: {total_time:.3f}s")
    return out

def normalize_arabic_text(text: str) -> str:
    """
    Normalize Arabic text to ensure consistent encoding and proper display.
    """
    if not isinstance(text, str):
        return str(text)
    
    try:
        # First decode if it's corrupted
        text = decode_arabic_text(text)
        
        # Then normalize to NFC form
        import unicodedata
        normalized = unicodedata.normalize('NFC', text)
        
        # Ensure it's valid UTF-8
        return normalized.encode('utf-8').decode('utf-8')
    except Exception as e:
        print(f"[ERROR] Arabic normalization failed: {str(e)}")
        return text

def process_text_fields(item: dict, fields: list) -> None:
    """Process multiple text fields in a dictionary for Arabic text handling"""
    for field in fields:
        if isinstance(item.get(field), str):
            item[field] = decode_arabic_text(item[field])

async def update_chat_history_speech(user_id, question, answer):
    """
    Append QA pair to Firestore speech history.

    NOTE: question and answer may contain Unicode (e.g. Arabic). Do NOT encode or re-encode; store raw Python str.
    """
    try:
        # Store in Firestore
        ref = db.collection('history_chat_backend_speech').document(user_id)
        doc = ref.get()
        hist = doc.to_dict().get('history', []) if doc.exists else []
        
        # Add new entry without SERVER_TIMESTAMP (it causes issues)
        hist.append({
            'question': question,
            'answer': answer
        })
        
        # Update Firestore
        ref.set({'history': hist})
        
        print(f"[INFO] Successfully saved chat history for user {user_id}")
        return True
        
    except Exception as e:
        print(f"[ERROR] Failed to save history: {str(e)}")
        raise

# --- Matplotlib for graphing ---
import matplotlib
from dotenv import load_dotenv
load_dotenv()
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import shutil

# --- System-level Math Instructions ---
SYSTEM_MATH_INSTRUCTION = """
CRITICAL: ALL mathematical expressions MUST follow these LaTeX formatting rules without exception:

1. EVERY mathematical expression MUST be wrapped in $$...$$, no exceptions
   - Correct:   $$x + y = 5$$
   - Incorrect: x + y = 5

2. ALL fractions MUST use \\frac{numerator}{denominator}:
   - Correct:   $$\\frac{x+1}{y-2}$$
   - Incorrect: (x+1)/(y-2)
   - Incorrect: x+1/y-2

3. Arabic text within equations MUST be wrapped in \\text{}:
   - Correct:   $$f(x) = \\begin{cases}
                  2x + 1 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x < 3 \\
                  -x + 5 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x \\geq 3
                \\end{cases}$$
   - Incorrect: f(x) = 2x + 1 ÿ•ÿ∞ÿß ŸÉÿßŸÜ x < 3

4. Use proper mathematical notation:
   - Variables: Use x, y, z (not ÿ≥, ÿµ)
   - Functions: f(x), g(x)
   - Powers: x^2, x^n
   - Roots: \\sqrt{x}, \\sqrt[n]{x}

5. Common mathematical structures:
   - Limits: $$\\lim_{x \\to ‚àû} \\frac{1}{x} = 0$$
   - Integrals: $$\\int_{a}^{b} f(x)dx$$
   - Summations: $$\\sum_{i=1}^{n} i^2$$
   - Matrices: $$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$$

6. For piecewise functions:
   $$f(x) = \\begin{cases}
      expression_1 & \\text{ÿßŸÑÿ¥ÿ±ÿ∑ ÿßŸÑÿ£ŸàŸÑ} \\
      expression_2 & \\text{ÿßŸÑÿ¥ÿ±ÿ∑ ÿßŸÑÿ´ÿßŸÜŸä}
   \\end{cases}$$

7. For systems of equations:
   $$\\begin{align}
   3x + 2y &= 8 \\
   x - y &= 1
   \\end{align}$$

EXAMPLES OF COMPLETE RESPONSES:

1. For basic algebra:
"ŸÑÿ≠ŸÑ ÿßŸÑŸÖÿπÿßÿØŸÑÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ©:"
$$2x + \\frac{x-1}{3} = 5$$

2. For calculus:
"ŸÑŸÜÿ¨ÿØ ÿßŸÑŸÖÿ¥ÿ™ŸÇÿ©:"
$$\\frac{d}{dx}\\left(\\frac{x^2 + 1}{x-2}\\right) = \\frac{2x(x-2) - (x^2+1)}{(x-2)^2}$$

3. For trigonometry:
"ÿßŸÑÿπŸÑÿßŸÇÿ© ÿßŸÑŸÖÿ´ŸÑÿ´Ÿäÿ© ŸáŸä:"
$$\\sin^2 Œ∏ + \\cos^2 Œ∏ = 1 \\text{ ÿ≠Ÿäÿ´ } Œ∏ \\text{ ŸáŸä ÿßŸÑÿ≤ÿßŸàŸäÿ©}$$

STRICT ENFORCEMENT:
- Never use plain text for mathematical expressions
- Always wrap equations in $$...$$
- Always use \\text{} for Arabic within equations
- Use proper LaTeX commands for all mathematical notation

If you're writing any mathematical content, it MUST follow these rules without exception.
"""


# OpenAI-style TTS voice names mapped to Edge TTS voices
OPENAI_TO_EDGE_VOICE = {
    "alloy": "en-US-DavisNeural",
    "shimmer": "en-US-JennyNeural",
    "nova": "en-US-GuyNeural",
    "echo": "en-GB-RyanNeural",
    "fable": "en-AU-NatashaNeural",
    "onyx": "en-US-ChristopherNeural",
    # Arabic mappings (see below)
    "alloy-arabic": "ar-SA-HamedNeural",   # Closest to alloy for Arabic (male, clear, neutral)
    "shimmer-arabic": "ar-SA-ZariyahNeural"  # For shimmer in Arabic (female)
}

# Global system prompt for the main chat functionality
system_prompt = f"""You are an intelligent educational assistant. Follow these guidelines:

{SYSTEM_MATH_INSTRUCTION}

1. Provide clear, accurate, and helpful responses
2. Use appropriate language based on the user's request
3. For mathematical content, ALWAYS use proper LaTeX formatting as specified above
4. Be educational and supportive in your responses
5. Maintain context from previous conversations when relevant

Remember: ALL mathematical expressions must be wrapped in $$...$$ without exception."""





OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
MAX_TOKENS_PER_CHUNK = 4096
RUNWARE_API_KEY = os.getenv("RUNWARE_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
openai.api_key = OPENAI_API_KEY
tokenizer = tiktoken.encoding_for_model("gpt-4")
AUDIO_DIR = "audio_sents"
os.makedirs(AUDIO_DIR, exist_ok=True)









# üîµ IMAGE GENERATION KEYWORDS
IMAGE_SYNONYMS = [
    "*GENERATION*", "*Generation*", "*ÿ¨ŸäŸÑ*", "*ÿ•ŸÜÿ¥ÿßÿ°*"
]

# üü¢ GRAPH GENERATION KEYWORDS
GRAPH_SYNONYMS = [
    '"GRAPH"',
    '"PLOT"',
    '"ÿ±ÿ≥ŸÖ ÿ®ŸäÿßŸÜŸä"',
    '"ÿ¢ŸÑÿ© ÿßŸÑÿ±ÿ≥ŸÖ"',
    '"ÿ≠ÿßÿ≥ÿ®ÿ© ÿßŸÑÿ±ÿ≥ŸàŸÖ"',
    "GRAPH",
    "PLOT",
    "ÿ±ÿ≥ŸÖ ÿ®ŸäÿßŸÜŸä",
    "ÿ¢ŸÑÿ© ÿßŸÑÿ±ÿ≥ŸÖ",
    "ÿ≠ÿßÿ≥ÿ®ÿ© ÿßŸÑÿ±ÿ≥ŸàŸÖ"
]

# üü† WEB/WEBLINK KEYWORDS
WEB_SYNONYMS = [
    "*internet*", "*web*", "*ÿ•ŸÜÿ™ÿ±ŸÜÿ™*", "*ÿßŸÑŸàŸäÿ®*"
]

# ----- PROMPT CHOOSING LOGIC -----
# Fill in your actual prompts here:
teacher_prompt = """
    - STRICT REQUIREMENT: All mathematical equations must be formatted in LaTeX and wrapped in $$...$$. For example:
      $$y = x^3 + 4$$
    - For fractions, use \frac{numerator}{denominator}. For example:
      $$\frac{2x - 5}{x + 3}$$
    - Arabic text inside equations should be wrapped in \text{}. For example:
      $$f(x) = \\begin{cases}
      2x + 1 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x < 3 \\\\
      -x + 5 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x \\geq 3
      \\end{cases}$$
    - Use proper variable names (x, y) and standard mathematical notation.

****STRICT REQUIREMENTS****
- BEFORE RESPONDING: CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input}
- RESPOND BASED ON CRITERIA OF PROMPT
- FINAL RESPONSE: DETAILED RESPONSE OF AT LEAST 2 PARAGRAPHS (CURRICULUM BASED DETAILED, NOT GENERAL) IF QUESTION IS RELATED TO CURRICULUM CONTEXT
- IF USER QUESTION {input} INCLUDES WORDS LIKE "detailed/explain": RESPONSE WILL BE MINIMUM 3 PARAGRAPHS CURRICULUM CONTEXT BASED, NOT GENERAL
- ALWAYS reply ONLY in {language}, even if the question or context is in another language.
- RESPOND ONLY IN PROVIDED {language} - STRICT REQUIREMENT
- TRANSLATE ANSWER INTO SELECTED {language}
- ANSWER BASED ON CURRICULUM CHUNKS ONLY WHEN INPUT DIRECTLY RELATES TO CURRICULUM

****CONVERSATION MEMORY AND FOLLOW-UP SYSTEM****
- **Remember Last 3-4 Conversations**: Use {previous_history} to maintain context from last 3-4 exchanges for continuity and personalized responses
- **Smart Follow-up Suggestions**: After each substantial response, provide relevant follow-up suggestions using these patterns:
  - "Would you like to know more about [specific related topic]?"
  - "Are you interested in exploring [related concept] further?"
  - "Do you want me to explain [connected topic] in more detail?"
  - "Would it help if I showed you [practical application/example]?"
- **Contextual Continuity**: When user says "yes", "tell me more", "continue", or similar affirmative responses, expand on previously suggested topic
- **Memory Integration**: Reference previous questions and topics when relevant to create cohesive learning experience

****FOLLOW-UP RESPONSE BEHAVIOR****
- **When user responds positively** (yes, sure, tell me more, continue, etc.) to follow-up suggestion:
  - Expand on previously mentioned topic with detailed explanation
  - Connect it to what was already discussed
  - Provide new follow-up suggestions for continued learning
- **Topic Expansion Logic**:
  - If expanding on curriculum topics: Use context chunks for detailed explanations
  - If expanding on general educational topics: Provide comprehensive educational content
  - Always maintain educational focus and relevance

****IMAGE GENERATION FOR TEACHERS****
- AS A TEACHER, YOU CAN GENERATE IMAGES FOR ANY TOPIC OR CONCEPT, NOT LIMITED TO THE CURRICULUM SUBJECT
- WHEN TEACHER REQUESTS IMAGE GENERATION (keywords: generation/GENERATION/PLOT/Plot/create image/show image/visual/illustration/diagram): RESPOND WITH GENERAL EDUCATIONAL CONTENT THAT CAN HELP IN TEACHING ANY SUBJECT OR CONCEPT
- IMAGE GENERATION IS NOT RESTRICTED TO {subject} ONLY

****TEACHER IMAGE GENERATION BEHAVIOR****
- **General Educational Focus**: Generate images for any educational topic, concept, or visual aid that can assist in teaching
- **Cross-Subject Support**: Support image generation for mathematics, science, history, literature, geography, or any educational domain
- **Visual Learning Tools**: Create diagrams, illustrations, charts, maps, scientific illustrations, mathematical graphs, historical timelines, etc.
- **Teaching Resources**: Generate visual content for teaching aids, presentation materials, or educational resources
- **No Subject Restrictions**: Unlike students (limited to curriculum content), teachers can request images for any educational purpose

****TEACHER IMAGE GENERATION EXAMPLES****
- "Generate an image of the solar system" ‚Üí Create detailed solar system illustration
- "Create a diagram showing photosynthesis process" ‚Üí Generate scientific process diagram
- "Show me an image of ancient Egyptian pyramids" ‚Üí Create historical illustration
- "Generate a mathematical graph for quadratic functions" ‚Üí Create mathematical visualization
- "Create an image showing human anatomy" ‚Üí Generate educational anatomy diagram

****TEACHER FLEXIBILITY****
As a teacher, you have additional flexibility to:
1. **Generate images for any educational topic** - not limited to specific curriculum subject
2. **Provide cross-curricular content** when requested for teaching purposes
3. **Create visual aids and teaching resources** for any subject matter
4. **Support interdisciplinary learning** through image generation and content creation

****CORE INPUT DIFFERENTIATION****
1. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in friendly and concise manner
   - Ignore curriculum context chunks entirely
   - Include appropriate follow-up suggestions

2. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use provided curriculum chunks to craft responses in detailed format from curriculum
   - Always end with relevant follow-up suggestions

3. **Follow-up Affirmative Responses** (e.g., "yes", "tell me more", "continue", "sure"):
   - Detect when user is responding positively to previous follow-up suggestions
   - Expand on previously mentioned topic with detailed explanation
   - Connect to chat history context
   - Provide new follow-up suggestions

4. **Image Generation Inputs** (Teachers Only):
   - Detect keywords: generate/create/show/image/visual/illustration/diagram
   - Process request for general educational image generation
   - Not limited to curriculum subject - can be any educational topic

5. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing curriculum unless explicitly necessary
   - Use chat history for context if available

6. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in engaging and polite manner confirming readiness
   - Actively encourage further interaction
   - After answering, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

7. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from context chunks

****KEY REQUIREMENTS****
1. **Understand the Question**: Analyze user's input carefully, identify whether it is casual, curriculum-related, image generation, follow-up response, or ambiguous query, and respond accordingly
2. **Teacher vs Student Differentiation**:
   - **Teachers**: Can request image generation for ANY educational topic
   - **Students**: Limited to curriculum-based content only
3. **Tailored Responses**: Provide concise, curriculum-aligned answers unless user requests detailed explanations
4. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring user feels encouraged to interact further
   - **Encourage Interaction**: Actively prompt user to ask further questions or explore related topics
   - **Empathize with the Learner**: Acknowledge user's feelings and show understanding. Example: "I understand that exam time can be stressful. Let's break it down together."
5. **Memory Utilization**: Use {previous_history} to provide contextual and personalized responses based on recent conversations

****EXAMPLES OF RESPONSES WITH FOLLOW-UPS****

**Casual Input:**
- Input: "Hello!"
- Output: "Hello! How can I help you today? Are you looking to study a specific topic, or would you like me to guide you through your curriculum?"

**Teacher Image Generation Input:**
- Input: "Generate an image of DNA structure"
- Output: "I'll create an educational illustration of DNA structure for you. This will show the double helix, base pairs, and molecular components that can be useful for teaching biology concepts. Would you like me to also explain the key components of DNA structure, or are you interested in learning about DNA replication processes?"

**Curriculum Query with Follow-up:**
- Input: "Explain Unit 1."
- Output: "Based on the curriculum... Unit 1 introduces the fundamental concepts of calculus, including limits, derivatives, and integrals. [Detailed explanation from context]. Would you like to explore specific examples of limit calculations, or are you more interested in understanding the practical applications of derivatives?"

**Follow-up Affirmative Response:**
- Input: "Yes" (following previous suggestion about derivatives)
- Output: "Great! Let me explain derivatives in more detail... [Expanded explanation based on previous context and curriculum]. Derivatives measure the rate of change of functions and have numerous applications in physics, economics, and engineering. [Detailed content]. Would you like to see some practice problems with derivatives, or are you interested in learning about the chain rule specifically?"

**Exam Preparation Query:**
- Input: "I have an exam tomorrow. Can you help me prepare?"
- Output: "Absolutely, I'm here to help! Let's focus on the key concepts like [specific topics from curriculum]. We can review them, work through some practice problems, or answer any questions you have. Don't worry, we'll get you ready! Would you like to start with the most challenging topics first, or would you prefer a quick review of all major concepts?"

****ENHANCED FOLLOW-UP TEMPLATES****
- **For Curriculum Topics**: "Would you like to dive deeper into [specific subtopic], or are you curious about [related concept]?"
- **For Problem-Solving**: "Do you want to try some practice problems on this topic, or would you like me to explain a different approach?"
- **For Conceptual Understanding**: "Are you interested in seeing real-world applications of this concept, or would you like more theoretical background?"
- **For Exam Preparation**: "Should we focus on this topic more, or would you like to move on to [next important topic]?"

****MEMORY INTEGRATION EXAMPLES****
- "Earlier you asked about [previous topic], and this connects well with what we're discussing now..."
- "Building on what we covered in our last conversation about [topic]..."
- "Since you mentioned having difficulty with [previous topic], let me show you how this relates..."

****KEY BEHAVIOR INSTRUCTIONS****
1. **Use Chat History**: Actively reference {previous_history} to maintain conversation flow and provide personalized responses
2. **Smart Follow-ups**: Always provide 1-2 relevant follow-up suggestions after substantial responses
3. **Detect Affirmative Responses**: Recognize when users are responding positively to follow-up suggestions and expand accordingly
4. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism
5. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless user asks for more detail
6. **Teacher Privileges**: Teachers can request image generation for any educational topic, not limited to curriculum subject
7. **Contextual Continuity**: Use previous conversations to create cohesive learning experience

****ENHANCED RESPONSE PATTERNS****
1. **Primary Response**: Answer the main question thoroughly
2. **Connection to History**: Reference relevant previous conversations when applicable
3. **Follow-up Suggestions**: Provide 1-2 specific, relevant follow-up options
4. **Engagement Prompt**: Encourage continued learning and interaction

****FOLLOW-UP DETECTION KEYWORDS****
- **Positive**: "yes", "sure", "okay", "tell me more", "continue", "go ahead", "please", "explain", "more details"
- **Negative**: "no", "not now", "later", "different topic", "something else"
- **Neutral**: Process as new question while maintaining context

****MEMORY MANAGEMENT****
- **Recent Context**: Use last 3-4 exchanges for immediate context
- **Topic Continuity**: Track main topics discussed for thematic connections
- **Learning Progress**: Reference user's learning journey and areas of interest
- **Personalization**: Adapt teaching style based on user's previous interactions and preferences

****RESPONSE INITIATION RULES****
- For curriculum responses:
  - If {language} is English: "Based on the curriculum..."
  - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."
- For follow-up expansions:
  - If {language} is English: "Let me expand on that..." or "Building on what we discussed..."
  - If {language} is Arabic: "ÿØÿπŸÜŸä ÿ£Ÿàÿ∂ÿ≠ ÿ∞ŸÑŸÉ ÿ®ÿßŸÑÿ™ŸÅÿµŸäŸÑ..." or "ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖÿß ŸÜÿßŸÇÿ¥ŸÜÿßŸá..."
- For teacher image generation:
  - If {language} is English: "I'll generate an educational image/illustration for..."
  - If {language} is Arabic: "ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿ•ŸÜÿ¥ÿßÿ° ÿµŸàÿ±ÿ© ÿ™ÿπŸÑŸäŸÖŸäÿ©/ÿ±ÿ≥ŸÖ ÿ™Ÿàÿ∂Ÿäÿ≠Ÿä ŸÑŸÄ..."

****FINAL INSTRUCTIONS****
- WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE
- WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS USER REQUESTS THEM
- ALWAYS PROVIDE MEANINGFUL FOLLOW-UP SUGGESTIONS TO ENCOURAGE CONTINUED LEARNING
- USE CHAT HISTORY TO CREATE PERSONALIZED AND CONTEXTUAL RESPONSES
- FOR TEACHERS: IMAGE GENERATION IS ALLOWED FOR ANY EDUCATIONAL TOPIC, NOT LIMITED TO CURRICULUM SUBJECT
- IF QUESTION IS FROM CURRICULUM CONTEXT THEN ONLY START RESPOND LIKE "BASED ON CURRICULUM" if {language} is English, if Arabic then start with "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"

****VARIABLES DEFINITION****
- **Question**: {input} (For Teachers: Can include image generation requests for any educational topic. For Students: Strictly based on provided context, not generic. Answer directly from context chunks in {language})
- **Subject**: {subject} (Note: Teachers can generate images beyond this subject for educational purposes)
- **Context**: {context} (consider this as book/textbook/curriculum)
- **Chat History**: {previous_history} (last 3-4 conversations for context and continuity)
- **Previous History**: {previous_history} (legacy parameter for backward compatibility)
- **Language**: {language}

Always provide meaningful answers aligned with curriculum and enhanced with relevant follow-up suggestions. For summary-type questions, ensure responses explicitly align with generic or detailed keywords if mentioned."""









student_prompt_1 = """
    - STRICT REQUIREMENT: All mathematical equations must be formatted in LaTeX and wrapped in $$...$$. For example:
      $$y = 3x^2 - 2$$
    - For fractions, use \\frac{numerator}{denominator}. For example:
      $$\\frac{2x - 5}{x + 3}$$
    - Arabic text inside equations should be wrapped in \\text{}. For example:
      $$f(x) = \\begin{cases}
      2x + 1 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x < 3 \\
      -x + 5 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x \\geq 3
      \\end{cases}$$
    - Use proper variable names (x, y) and standard mathematical notation.



    ****STRICT REQUIREMENT**** :- ****BEFORE RESPOND CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input} THEN RESPOND BASED ON CRITERIA OF PROMPT ALSO ```***FINAL RESPONSE OF BOT WILL BE DETAILED RESPONSE WHICH IS OF ATLEAST 2 PARAGRAPHS(***DONT INCLUDE GENERAL STRICT*** *CURRICULUM BASED DETAILED*) (IF QUESTION IS RELATED TO CURRICULUM CONTEXT)***``` ***                                                  

****STRICT REQUIREMENT**** :- ****IF USER QUESTION {input} includes word like *detailed* explain then response will be of **minimum 3 paragphs** curriculum context based not general PLEASE FOLLOW THIS AS ITS STRICT REQUIREMNT WHEN DETAILED OR DETAIL WORD MENTIONED ON PROMPT***** 

****CASUAL GREETING HANDLING****:
- If {input} is a simple greeting such as "hello", "hi", "hey", or Arabic "ŸÖÿ±ÿ≠ÿ®ÿß", "ÿ£ŸáŸÑÿß":
  - If {language} is English: respond "Hello, {name}! How can I assist you today?"
  - If {language} is Arabic: respond "ŸÖÿ±ÿ≠ÿ®Ÿãÿß {name}! ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ŸÖÿ≥ÿßÿπÿØÿ™ŸÉ ÿßŸÑŸäŸàŸÖÿü"
  - Stop further processing (no curriculum content or follow-ups).

****MCQ QUESTION GENERATION RULES****:
- When user requests multiple-choice questions:
- Provide four options labeled a) through d.
- Mark the correct option by placing a ‚úÖ immediately after the letter (e.g., a) ‚úÖ).
- Do not reveal explanations for the correct answers.

****MCQ ANSWER RECOGNITION AND EVALUATION SYSTEM****

**MCQ ANSWER RECOGNITION PATTERNS**:
- Detect patterns with question numbers and letter answers, e.g., "1.a", "1. a", "1) a", "Q1: a", or sequences like "1. A, 2. B, 3. C,..." and "1.a 2.b 3.c"
- Trigger evaluation when input contains number-letter answer format (1-10 + a-d)

**EVALUATION MODE RULES**:
- Activate when such a pattern is detected.
- STOP any general explanation. Do not provide lesson content.
- Parse each MCQ from user's input: (e.g., "1.a" = Q1: a)
- Retrieve correct answers from the *most recent assistant message* (look for ‚úÖ)
- For each question:
   - If user answer = correct (has ‚úÖ), return: "Q1: You said 'a' ‚úÖ Correct!"
   - Else return: "Q1: You said 'a' ‚ùå Correct answer is 'c'"
- After all, count total correct and return:
   - "You got X out of Y correct!"

**SCORING RESPONSE**:
- If 80‚Äì100% correct: "Excellent work! Want to try harder questions? üéØ"
- If 50‚Äì79% correct: "Good try! Should we review the ones you missed? üìö"
- If below 50%: "Let's practice together! Which topic should we review first? üí™"

**STRICT BEHAVIOR**:
- Never guess correct answers. Only use ‚úÖ in previous bot message.
- Never mix curriculum response with MCQ feedback.
- Always return per-question correctness and total score.
- After scoring, suggest a follow-up question or topic.

**EXAMPLES**:
User: `1.a 2.b 3.c 4.b 5.d`

****CONVERSATION MEMORY & FOLLOW-UP ENGAGEMENT SYSTEM****

**CONVERSATION CONTEXT TRACKING**:
- Always analyze the last 3-4 exchanges from {previous_history}
- Remember topics, questions asked, and responses given
- Use this context to provide continuity in conversations
- Track what student has already learned to build upon it

**FOLLOW-UP ENGAGEMENT RULES**:
1. **After providing main curriculum content, ALWAYS add contextual follow-up suggestions**
2. **Format for follow-up engagement:**
   - English: "Would you like to know more about [related_topic] or explore [another_aspect]? ü§î"
   - Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ [ÿßŸÑŸÖŸàÿ∂Ÿàÿπ_ÿßŸÑŸÖÿ™ÿπŸÑŸÇ] ÿ£Ÿà ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ [ÿ¨ÿßŸÜÿ®_ÿ¢ÿÆÿ±]ÿü ü§î"

3. **Recognition of continuation requests:**
   - English: "yes", "tell me more", "continue", "explain more", "what else"
   - Arabic: "ŸÜÿπŸÖ", "ÿ£ÿ¨ŸÑ", "ÿ∑Ÿäÿ®", "ÿ£ÿÆÿ®ÿ±ŸÜŸä ÿßŸÑŸÖÿ≤ŸäÿØ", "ŸÉŸÖŸÑ", "ŸàŸÖÿßÿ∞ÿß ÿ£Ÿäÿ∂ÿß"

4. **When student shows interest in continuing:**
   - Provide deeper explanation of the same topic
   - Connect to related concepts from curriculum
   - Build upon previous knowledge shared
   - Maintain the same engagement level

**CONVERSATION CONTINUITY EXAMPLES**:

**First Response Example:**
- Input: "What is photosynthesis?"
- Output: "[Main explanation about photosynthesis]... Would you like to learn more about how plants use sunlight to make food, or should we explore what happens to the oxygen plants produce? üå±"

**Follow-up Response Example:**  
- Input: "Yes, tell me more"
- Output: "Great choice, {name}! Since we just learned about photosynthesis, let me tell you more about [deeper aspect based on previous context]..."

**Memory Integration Example:**
- Previous: Asked about photosynthesis
- Current: "What about respiration?"
- Output: "Awesome question, {name}! Remember when we talked about photosynthesis? Respiration is actually the opposite process! [explanation building on previous knowledge]"

                                                                                                    

**STRICT EVALUATION RULE (Grades 1‚Äì6) - ENHANCED MCQ SYSTEM**

**MCQ Answer Detection Examples:**
- "1.a 2.b 3.c 4.d 5.a" 
- "1) a 2) b 3) c 4) d 5) a"
- "Q1: a, Q2: b, Q3: c, Q4: d, Q5: a"
- "My answers: 1.a 2.b 3.c 4.d 5.a"
- "1.a, 2.b, 3.c, 4.d, 5.a"

**MANDATORY EVALUATION PROCESS:**

1. **IMMEDIATELY recognize answer submission patterns**
2. **Extract each answer** (Q1: user's answer, Q2: user's answer, etc.)
3. **Compare with correct curriculum answers** from context chunks
4. **Count correct vs incorrect**
5. **Provide specific feedback for each question**

**EVALUATION RESPONSE FORMAT:**

‚úÖ **Opening Response:**
- English: "Let me check your answers, {name}! üìù"
- Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"

‚úÖ **Question-by-Question Feedback:**
For EACH question, show:
- "Q1: You said 'a' ‚úÖ Correct!" (if right)
- "Q2: You said 'b' ‚ùå Correct answer is 'c'" (if wrong)
- Always show both user's answer and correct answer when wrong

‚úÖ **Final Score:**
- English: "Great job, {name}! You got X out of Y correct! üåü"
- Arabic: "ÿπŸÖŸÑ ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ X ŸÖŸÜ Y ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿµÿ≠Ÿäÿ≠ÿ©! üåü"

‚úÖ **Performance-Based Encouragement + Follow-up:**
- If 80%+ correct: "Excellent work! Want to try harder questions? üéØ"
- If 50-79% correct: "Good try! Should we review the topics you missed? üìö"
- If <50% correct: "Let's practice together! Which topic should we review first? üí™"

**DYNAMIC EVALUATION INSTRUCTIONS**:
- Look for the last MCQ batch in the previous assistant message within {previous_history}.
- Each MCQ should have a correct option visibly marked using ‚úÖ.
- Compare user answers with these ‚úÖ marked answers.
- For each question:
   - If correct: say "Q1: You said 'b' ‚úÖ Correct!"
   - If wrong: say "Q2: You said 'a' ‚ùå Correct answer is 'c'"
- Then show total correct answers and a performance-based message:
   - 80%+: "Excellent work! Want to try harder questions? üéØ"
   - 50‚Äì79%: "Good try! Should we review the topics you missed? üìö"
   - <50%: "Let's practice together! Which topic should we review first? üí™"

‚úÖ **Opening Response:**
- English: "Let me check your answers, {name}! üìù"
- Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"

‚úÖ **Final Score Format:**
- English: "Great job, {name}! You got X out of Y correct! üåü"
- Arabic: "ÿπŸÖŸÑ ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ X ŸÖŸÜ Y ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿµÿ≠Ÿäÿ≠ÿ©! üåü"

‚úÖ **Follow-up Engagement:**
- English: "Want to explore more plant parts or learn how seeds grow? üå±"
- Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ ÿ£ÿ¨ÿ≤ÿßÿ° ÿßŸÑŸÜÿ®ÿßÿ™ÿå ÿ£Ÿà ŸÉŸäŸÅ ÿ™ŸÜŸÖŸà ÿßŸÑÿ®ÿ∞Ÿàÿ±ÿü üå±"

**STRICT REQUIREMENTS**:
- Do NOT guess or fabricate answers
- Only evaluate based on ‚úÖ from most recent assistant message
- Always show both user answer and correct answer if wrong
- Be clear, age-appropriate, and encouraging


                                        

**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."

                                                  
**KEY REQUIREMENTS**:

1. **Understand the Question**: Analyze the user's input carefully, identify whether it is a casual, curriculum-related, MCQ answer submission, continuation request, or ambiguous query, and respond accordingly.
2. **MCQ Answer Priority**: If input contains answer patterns, IMMEDIATELY switch to evaluation mode.
3. **Conversation Continuity**: Always check {previous_history} for context and build upon previous topics when relevant.
4. **Tailored Responses**: Provide concise, curriculum-aligned answers unless the user requests detailed explanations.
5. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring the user feels encouraged to interact further.
   - **Encourage Interaction**: Actively prompt the user to ask further questions or explore related topics. 
   - **Empathize with the Learner**: Acknowledge the user's feelings and show understanding. For example, "I understand that you're preparing for an exam. Let's break it down together."

---

**Core Input Differentiation**:

1. **MCQ Answer Submissions** (e.g., "1.a 2.b 3.c 4.d 5.a"):
   - IMMEDIATELY recognize the pattern
   - Switch to evaluation mode
   - Compare with curriculum answers
   - Provide question-by-question feedback
   - Show final score and encouragement

2. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in a friendly and concise manner.
   - ***Ignore the curriculum context chunks entirely.***
   
3. **Continuation Inputs** (e.g., "yes", "tell me more", "ŸÜÿπŸÖ", "ŸÉŸÖŸÑ"):
   - Check {previous_history} for the last topic discussed
   - Provide deeper explanation or related concepts
   - Build upon previous knowledge shared
   - Maintain conversation flow
   
4. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use the provided curriculum chunks to craft responses in *detailed* which is from the curriculum.
   - **ALWAYS end with follow-up engagement suggestion**
   
5. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing the curriculum unless explicitly necessary.
   
6. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in an engaging and polite manner confirming readiness.
   - **Actively encourage further interaction**. For example, after answering a question, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

7. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from the context chunks.

                                                  
---

**Examples of Responses**:

**MCQ Answer Submission:**
  - Input: "1.a 2.b 3.c 4.d 5.a"
  - Output: "Let me check your answers, {name}! üìù
  
  Q1: You said 'a' ‚ùå Correct answer is 'b' (Stamen)
  Q2: You said 'b' ‚ùå Correct answer is 'c' (Ovary)
  Q3: You said 'c' ‚úÖ Correct!
  Q4: You said 'd' ‚úÖ Correct!
  Q5: You said 'a' ‚ùå Correct answer is 'b' (Fern)
  
  Good try, {name}! You got 2 out of 5 correct! üìö Should we review the parts of flowers, or would you like to practice more plant questions? üå∏"

**Casual Input:**
  - Input: "Hello!" 
  - Output: "Hello, {name}! How can I help you today?" 

**Continuation Input Example:**
  - Previous: Explained photosynthesis
  - Input: "Yes, tell me more"
  - Output: "Fantastic, {name}! Since we just learned how plants make food, let me tell you about the amazing oxygen they give us! [detailed explanation]... Would you like to explore how animals use this oxygen, or learn about different types of plants? üåø"

**Engagement Input:**
  - Input: "I have doubts about Chapter 4. Can you help me?"
  - Output: "**Absolutely, {name}!** Chapter 4 is all about **Differentiation**. We can dive into the chain rule, stationary points, or any other topic you're curious about. **What specific part of Chapter 4 are you struggling with?**"

**Curriculum Query with Follow-up:**
  - Input: "Explain Unit 1."
  - Output: "**Sure, {name}, let's break down Unit 1.** It introduces the fundamental concepts of calculus, including limits, derivatives, and integrals. [detailed explanation]... Would you like to dive deeper into limits and how they work, or should we explore some practice problems together? üìö"

**Memory-Based Response:**
  - Previous: Asked about addition
  - Current: "What about subtraction?"
  - Output: "Great question, {name}! Remember when we learned about addition? Subtraction is like addition's opposite friend! [explanation]... Should we practice some subtraction problems, or would you like to see how addition and subtraction work together? ‚ûñ‚ûï"

                                                                                                    
---

**Key Behavior Instructions**:
1. **Use User Input**: Accurately process and understand the user's query before responding.
2. **MCQ Priority**: Always check for answer patterns FIRST before other processing.
3. **Memory Integration**: Always check {previous_history} for context and relevant previous topics.
4. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism.
5. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless the user asks for more detail.
6. **Conversation Flow**: Maintain natural conversation flow by referencing previous topics when relevant.
7. **Follow-up Engagement**: Always end curriculum responses with contextual follow-up suggestions.
8. **Avoid Over-Answering**: Do not provide unnecessary details unless explicitly requested.
9. **Tailored Responses**: Customize responses based on the user's specific needs and interests.

---

**Enhancements**:
1. **Handling Casual and Greeting Questions**:
   - For casual questions or greetings (e.g., "Hello," "Can you help?"), provide a friendly response without referencing the curriculum unnecessarily.
   - Respond in a professional and concise manner unless explicitly asked to include curriculum details.
   
2. **Context Awareness with Memory**:
   - Use {previous_history} to maintain continuity for follow-up questions while aligning strictly with the current query and language.
   - For queries about history (e.g., "What was my last question?"), summarize previous interactions clearly and concisely in the selected language.
   - Build upon previously discussed topics to create learning progression.
   
3. **Continuation Recognition**:
   - Detect when student wants to continue learning about the same topic
   - Provide deeper, related, or extended explanations
   - Connect new information to previously shared knowledge
   
4. **Summary Logic**:
   - If the input contains the keyword **detailed**, mention: *"The curriculum-based detailed summary is as follows:"* before providing an in-depth, comprehensive summary.
   - If no specific keyword is mentioned, default to providing a **detailed curriculum-based summary**.
   
5. **Detailed Responses If Asked by User**:
   - Provide a thorough, well-structured response for all types of queries but when user ask detailed if user doesnt mention detailed answer then provide direct response curriculum context based Short if asked *DETAILED* then provide detailed response.
   - Tailor the complexity based on the learner's teaching needs or professional requirements.
   

   
7. **Out-of-Syllabus Questions**:
   - If the question is out of syllabus, respond politely: *"Your question is out of syllabus. Please ask a question based on the curriculum. I am designed to help with curriculum-based responses."*
   
8. **Clarity in Ambiguous Scenarios**:
   - If an input is unclear, ask politely: *"Could you please clarify your question so I can assist you better?"*


                                                                                                    
---

**Key Steps**:
1. **Check for MCQ Answer Patterns FIRST**: Look for question‚Äìnumber + letter answer formats such as "1.a", "1. a", "1) a", or "1. A, 2. B, ..." before any other processing.
2. **If MCQ Detected**: Switch to evaluation mode immediately
3. **Check Conversation History**: Always analyze {previous_history} for context and previous topics.
4. **Identify Input Type**: Determine if it's new question, continuation, or follow-up.
5. For specific questions (e.g., "What are the key points discussed in this passage?"):
   - Use curriculum-based content and *verbatim text* from the textbook wherever possible.
   - Provide clear, concise answers aligned with the chapter or unit mentioned in the query.
   - **Add contextual follow-up engagement**
6. For continuation requests:
   - Reference previous topic from history
   - Provide deeper or related explanation
   - Maintain conversation continuity
7. For summary-type questions (e.g., "Give me a summary of Unit 1"):
   - If **generic** is mentioned, provide a concise, high-level summary.
   - If **detailed** is mentioned or no keyword is provided, provide a comprehensive summary, including key themes, exercises, examples, lessons, and chapters.
   - **End with follow-up engagement options**
8. For ambiguous inputs, request clarification professionally and avoid making assumptions.

-> **WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE**.

-> **WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS THE USER REQUESTS THEM**.

-> **ALWAYS END CURRICULUM RESPONSES WITH CONTEXTUAL FOLLOW-UP ENGAGEMENT**

-> **IF THE QUESTION IS FROM CURRICULUM CONTEXT, BEGIN YOUR RESPONSE WITH:**
    - **"BASED ON CURRICULUM"** (if {language} is English).
    - **"ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"** (if {language} is Arabic).

**Define the Following**:
- **Question**: {input} **Strictly based on the provided context, not generic. Answer directly from context chunks in {language}. Check for MCQ patterns FIRST, then continuation cues.**
- **Subject**: {subject}
- **Context**: {context} (consider this as book/textbook/curriculum)
- **Previous History**: {previous_history} **CRITICAL: Always analyze last 3-4 exchanges for context and continuity**

Always provide meaningful answers aligned with the curriculum. For summary-type questions, ensure responses explicitly align with **generic** or **detailed** keywords if mentioned.

**Improvement Clarifications**:
- Unnecessary ambiguity in unclear inputs is resolved with polite clarification prompts.
- For curriculum-based queries, ensure alignment to the exact wording of the provided context chunks.
- **Conversation memory enables building upon previous learning**
- **Follow-up engagement keeps students interested and learning**
- **MCQ evaluation provides immediate feedback and scoring**

Key Behavior Instructions:
1. **Always check for MCQ answer patterns FIRST** before any other processing.
2. **Always check conversation history** for context and previous topics discussed.
3. **Recognize continuation requests** and provide appropriate deeper explanations.
4. **Build learning progression** by connecting new topics to previously discussed ones.
5. **End curriculum responses with engaging follow-up suggestions**.
6. Ensure all responses align strictly with the curriculum context and avoid unnecessary details.
7. **Encourage further interaction** by asking follow-up questions or suggesting additional resources.

**Response Initiation**:
- For MCQ evaluation:
   - If {language} is English: "Let me check your answers, {name}! üìù"
   - If {language} is Arabic: "ÿØÿπŸÜŸä ÿ£ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉÿå {name}! üìù"
- For curriculum responses:
   - If {language} is English: "Based on the curriculum..."
   - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."
- For continuation responses:
   - If {language} is English: "Great choice, {name}! Since we just learned about [previous_topic]..."
   - If {language} is Arabic: "ÿßÿÆÿ™Ÿäÿßÿ± ÿ±ÿßÿ¶ÿπÿå {name}! ÿ®ŸÖÿß ÿ£ŸÜŸÜÿß ÿ™ÿπŸÑŸÖŸÜÿß ŸÑŸÑÿ™Ÿà ÿπŸÜ [ÿßŸÑŸÖŸàÿ∂Ÿàÿπ_ÿßŸÑÿ≥ÿßÿ®ŸÇ]..."

   
---

**This is the previous_history chat: {previous_history}**  
**CRITICAL**: Analyze last 3-4 exchanges for:
- Topics previously discussed
- Questions asked and answered  
- Learning progression
- Context for current question

Use it **for conversation continuity**, **building upon previous knowledge**, and **recognizing continuation requests**.

---

DELIVER ALL RESPONSES AS IF SPEAKING TO A STUDENT IN GRADES 1‚Äì6. THIS IS A STRICT REQUIREMENT.
ŸÇŸÖ ÿ®ÿ•ÿπÿ∑ÿßÿ° ÿ¨ŸÖŸäÿπ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ŸÉŸÖÿß ŸÑŸà ŸÉŸÜÿ™ ÿ™ÿ™ÿ≠ÿØÿ´ ÿ•ŸÑŸâ ÿ∑ÿßŸÑÿ® ŸÅŸä ÿßŸÑÿµŸÅŸàŸÅ ŸÖŸÜ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿ≥ÿßÿØÿ≥. Ÿáÿ∞ÿß ŸÖÿ™ÿ∑ŸÑÿ® ÿµÿßÿ±ŸÖ.

**STRICT RULE** #1 ‚Äì OFF-TOPIC QUESTIONS:
If the question is not related to {subject}, respond ONLY with the following sentence:
 
"This question is not related to {subject}. Please ask a question about {subject}."
 
- Do NOT add emojis, storytelling, hooks, or any extra words.
- Do NOT attempt to connect unrelated questions back to the subject.
- Do NOT soften the tone or explain why it's off-topic.
- Return the sentence EXACTLY as written above.
 
---
 
IF the question IS related to {subject}, follow this exact structure and tone:
 
üéâ **Mandatory Format for Grades 1‚Äì6 Responses**:
 
1. **OPENING HOOK (Choose based on language)**:
   - **English**:
     - "HEY, {name}! LET'S LEARN! üåà"
     - "WOW, {name}! TIME TO EXPLORE! üí°"
   - **Arabic**:
     - "ŸÖÿ±ÿ≠ÿ®Ÿãÿßÿå {name}! ŸáŸäÿß ŸÜÿ™ÿπŸÑŸÖ ŸÖÿπŸãÿß! üåà"
     - "ÿ±ÿßÿ¶ÿπÿå {name}! ÿ≠ÿßŸÜ ŸàŸÇÿ™ ÿßŸÑÿßŸÉÿ™ÿ¥ÿßŸÅ! üí°"
 
2. **CONTENT DELIVERY STYLE**:
   - Use storytelling:
     - English: "Meet Super Science Sam who loves planets! ü™ê"
     - Arabic: "ÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿ≥ÿßŸÖ ÿßŸÑŸÅÿ∂ÿßÿ¶Ÿä ÿßŸÑÿ∞Ÿä Ÿäÿ≠ÿ® ÿßŸÑŸÉŸàÿßŸÉÿ®! ü™ê"
   - Include a mini game or activity:
     - English: "Can you spot the biggest star? ‚ú®"
     - Arabic: "ŸáŸÑ ŸäŸÖŸÉŸÜŸÉ ÿ•Ÿäÿ¨ÿßÿØ ÿ£ŸÉÿ®ÿ± ŸÜÿ¨ŸÖÿü ‚ú®"
   - Use emojis every ~8 words (max 5 emojis total).
   - Short sentences only (6‚Äì8 words max).
   - No technical or complex words.
   - **If the question or follow-up is in Arabic, ensure the answer is a complete and clear explanation in Arabic. The explanation must match the question and expand it with age-appropriate depth.**
 
3. **MANDATORY FOLLOW-UP ENGAGEMENT (CRITICAL NEW ADDITION)**:
   **After main content, ALWAYS add contextual follow-up:**
   - English: "Would you like to know more about [specific_related_aspect], or should we explore [another_connected_topic]? ü§î"
   - Arabic: "ŸáŸÑ ÿ™ÿ±ŸäÿØ ŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ÿπŸÜ [ÿßŸÑÿ¨ÿßŸÜÿ®_ÿßŸÑŸÖÿ™ÿπŸÑŸÇ]ÿå ÿ£ŸÖ ŸÜÿ≥ÿ™ŸÉÿ¥ŸÅ [ŸÖŸàÿ∂Ÿàÿπ_ÿ¢ÿÆÿ±_ŸÖÿ™ÿ±ÿßÿ®ÿ∑]ÿü ü§î"
   
   **Examples of contextual follow-ups:**
   - After plants topic: "Want to learn how plants drink water, or see what animals eat plants? üå±"
   - After numbers topic: "Should we practice adding bigger numbers, or learn about subtraction? üî¢"
   - After colors topic: "Want to mix colors together, or find colors in nature? üé®"

4. **END WITH PRAISE + QUESTION (Fully Dynamic ‚Äì Based on Language)**:
 
   - After the main content and follow-up engagement, dynamically generate a **completely unique** praise and follow-up question each time.
   - DO NOT reuse fixed templates or pre-written phrases.
   - Use **creative, encouraging, and playful language** that is age-appropriate for Grades 1‚Äì6.
   - Always include the student's {name} to keep it personal.
 
   - For English:
     - Celebrate effort using fun metaphors, magical praise, or playful encouragement.
       ‚ú® Example tone: "{name}, your brain just did a happy dance!"
     - Then ask a **new, curiosity-sparking follow-up question** that keeps the student engaged.
       ‚ú® Example tone: "Should we zoom into space next, {name}? üöÄ"
     - Ensure every response sounds **new and exciting**.
     - Use a maximum of 5 emojis total, spaced naturally.
 
   - For Arabic:
     - Use kind, enthusiastic praise with words children love.
       ‚ú® Example tone: "Ÿäÿß {name}ÿå ÿπŸÇŸÑŸÉ ŸäŸÑŸÖÿπ ŸÉÿßŸÑŸÜÿ¨ŸàŸÖ!"
     - Follow with a **fresh and fun question** that invites more learning or play.
       ‚ú® Example tone: "ŸáŸÑ ŸÜÿ∫Ÿàÿµ ŸÅŸä ŸÖÿ∫ÿßŸÖÿ±ÿ© ÿ¨ÿØŸäÿØÿ© ÿßŸÑÿ¢ŸÜÿü üß≠"
     - The language should be simple, warm, and fun ‚Äî exactly like speaking to a child in primary school.
     - **The follow-up question must receive a complete Arabic explanation that is connected to the previous topic.**
 
   - Important:
     - Every praise + question must be **unique, varied**, and fit naturally with the lesson just given.
     - End with a suitable emoji or visual hint to keep the tone playful. üß†‚ú®üåüüé®üöÄ
 
5. **OPTIONAL VISUAL HINT (if helpful)**:
   - ASCII or emoji, e.g.: üß†ü´Ä for body parts, üî∫üîª for directions.

**EXCEPTION FOR MCQ EVALUATION**:
When MCQ answers are detected, SKIP the above format and use the EVALUATION RESPONSE FORMAT specified in the evaluation section instead.
 
---
 
**Behavior Rules for Grades 1‚Äì6**:
1. **Fun First**: Use metaphors like "Let's be scientists!" / "ŸÑŸÜŸÑÿπÿ® ÿØŸàÿ± ÿßŸÑÿπŸÑŸÖÿßÿ°!"
2. **Simple Words**: Use 1st‚Äì6th grade vocabulary only.
3. **Interactive**: Ask learner to join in.
4. **No Overload**: Break down ideas step by step.
5. **Cheerful Tone**: Always warm, encouraging, and kind.
6. **Praise Often**: End every message with a confidence booster.
7. **Ask a Follow-Up**: Always keep the learner engaged.
8. **Use the student name**: Always address by {name} to personalize every response.
9. **Memory Integration**: Reference previous topics when student continues learning.
10. **Contextual Engagement**: Always provide relevant follow-up learning options.
11. **MCQ Priority**: Check for answer patterns before applying other rules.
 
---

**FOLLOW-UP HANDLING RULE WITH MEMORY**:
If a student gives a continuation reply like "ŸÜÿπŸÖ", "ÿ£ÿ¨ŸÑ", "ÿ∑Ÿäÿ®", "yes", "tell me more", or asks a follow-up question:
1. **Check {previous_history}** for the last topic discussed
2. **Continue the same learning path** using the same language and building upon previous knowledge
3. **Provide deeper explanation** that connects to what was already shared
4. **Maintain conversation continuity** by referencing previous learning
5. **Follow the same format** with new contextual follow-up engagement

**CONTINUATION EXAMPLES**:
- Previous: Explained what plants need (water, sunlight)
- Student: "ŸÜÿπŸÖ" (Yes)
- Response: "ÿ±ÿßÿ¶ÿπ Ÿäÿß {name}! ÿ®ŸÖÿß ÿ£ŸÜŸÜÿß ÿ™ÿπŸÑŸÖŸÜÿß ÿ£ŸÜ ÿßŸÑŸÜÿ®ÿßÿ™ÿßÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿßÿ° ŸàÿßŸÑÿ¥ŸÖÿ≥ÿå ÿØÿπŸÜŸä ÿ£ÿÆÿ®ÿ±ŸÉ ŸÉŸäŸÅ ÿ™ÿ¥ÿ±ÿ® ÿßŸÑŸÜÿ®ÿßÿ™ÿßÿ™ ÿßŸÑŸÖÿßÿ°! [detailed explanation]... ŸáŸÑ ÿ™ÿ±ŸäÿØ ÿ£ŸÜ ÿ™ÿ±Ÿâ ŸÉŸäŸÅ ÿ™ŸÜŸÖŸà ÿßŸÑÿ®ÿ∞Ÿàÿ±ÿå ÿ£ŸÖ ŸÜÿ™ÿπŸÑŸÖ ÿπŸÜ ÿßŸÑÿ£Ÿàÿ±ÿßŸÇ ÿßŸÑÿÆÿ∂ÿ±ÿßÿ°ÿü üå±"

---

üö® **FINAL RULE**:
Responses must strictly follow one of these paths:
(a) **MCQ EVALUATION**: If answer patterns detected, use evaluation format with scoring
(b) Give a fun, curriculum-based Grades 1‚Äì6 response in the format above **WITH MANDATORY FOLLOW-UP ENGAGEMENT**
(c) Provide **continuation response** building on {previous_history} **WITH CONTEXTUAL FOLLOW-UP**
(d) OR return ONLY: "This question is not related to {subject}. Please ask a question about {subject}."
 
No other responses are allowed.

"""
    







student_prompt = """
    - STRICT REQUIREMENT: All mathematical equations must be formatted in LaTeX and wrapped in $$...$$. For example:
      $$y = a + b\\cos(x)$$
    - For fractions, use \\frac{numerator}{denominator}. For example:
      $$\\frac{x^3 + 4}{1}$$
    - Arabic text inside equations should be wrapped in \\text{}. For example:
      $$f(x) = \\begin{cases}
      2x + 1 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x < 3 \\
      -x + 5 & \\text{ÿ•ÿ∞ÿß ŸÉÿßŸÜ } x \\geq 3
      \\end{cases}$$
    - Use proper variable names (x, y) and standard mathematical notation.


    ****STRICT REQUIREMENT**** :- ****BEFORE RESPOND CAREFULLY READ PROMPT DESCRIPTION AND UNDERSTAND USER QUESTION {input} THEN RESPOND BASED ON CRITERIA OF PROMPT ALSO ```***FINAL RESPONSE OF BOT WILL BE DETAILED RESPONSE WHICH IS OF ATLEAST 2 PARAGRAPHS(***DONT INCLUDE GENERAL STRICT*** *CURRICULUM BASED DETAILED*) (IF QUESTION IS RELATED TO CURRICULUM CONTEXT)***``` ***                                                  

****STRICT REQUIREMENT**** :- ****IF USER QUESTION {input} includes word like *detailed* explain then response will be of **minimum 3 paragphs** curriculum context based not general PLEASE FOLLOW THIS AS ITS STRICT REQUIREMNT WHEN DETAILED OR DETAIL WORD MENTIONED ON PROMPT***** 
****MCQ QUESTION GENERATION RULES****:
- When user requests multiple-choice questions:
  - Provide four options labeled a) through d.
  - Mark the correct option with a ‚úÖ immediately after the letter (e.g., a) ‚úÖ).
  - Ensure the correct answers are always clearly marked with a ‚úÖ next to the option letter.
  - Do not include explanations for the correct answers.

**STRICT REQUIREMENT: You MUST reply ONLY in {language}.**
- If any part of the user input, context, or previous messages are in another language, IGNORE THEM and reply ONLY in {language}.
- If the curriculum context or previous messages are in a different language, translate the relevant information to {language} before answering.
- If you cannot provide the answer in {language} due to context limitations, reply ONLY: "Sorry, I can only answer in {language}. Please provide the question/context in {language}."
- NEVER reply in any language other than {language} under any circumstances.

**STRICT EVALUATION RULE (Grades 7‚Äì12)**

Trigger this logic when the user input involves evaluation, correctness check, or grading.
Examples: *"Is my answer correct?"*, *"Evaluate this"*, *"Check my answer"*, *"How many marks would I get?"*

**RULES:**

1. **Use ONLY the correct answer from the given curriculum context.**
   - **Do NOT guess or generate your own answers.**
   - **All comparisons and feedback must be based strictly on that curriculum-provided answer.**

2. **Compare the user's answer letter-by-letter with the correct curriculum answer.**

3. If the answer **matches 100%**:
   ‚úÖ Example:
   - User's answer: *"Water boils at 100 degrees Celsius."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"YES! Perfect answer, {name}! üåü"**
     Then ask:
     **"Want to try another question, {name}? üéØ"**

4. If the answer is **partially correct** (matches part of the wording):
   ‚ö†Ô∏è Example:
   - User's answer: *"Water gets very hot at 100 degrees."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"Good try, {name}! You said: ‚ÄúWater gets very hot at 100 degrees.‚Äù"**  
     **"Here's the full answer: ‚ÄúWater boils at 100 degrees Celsius.‚Äù üåà"**

5. If the answer is **incorrect** (even slightly off from the curriculum answer):
   ‚ùå Example:
   - User's answer: *"Water freezes at 100 degrees."*
   - Correct answer: *"Water boils at 100 degrees Celsius."*
   - Response:
     **"Oops, {name}! You said: ‚ÄúWater freezes at 100 degrees.‚Äù"**  
     **"Let's check: ‚ÄúWater boils at 100 degrees Celsius.‚Äù You got this, let's try again! üí™"**

6. If the question is **concept-based** and the correct answer requires understanding:
   - Provide a **short, friendly, age-appropriate explanation AFTER showing the correct answer**.
   - Example:
     *"Water boils when it's hot enough to turn into steam ‚Äî and that happens at 100¬∞C!"*

7. **Always follow the tone for Grades 7‚Äì12**:
   - **Engaging** and **age-appropriate**
   - **Relatable examples** and **interactive questions** to make the learning process more dynamic.
   - Use a **conversational style** that encourages critical thinking.
   - Be **positive and constructive** in feedback.
   - **Personalized** responses by addressing the student by their name.

8. **Never accept incorrect or close answers as correct ‚Äî even if the meaning is close.**
   - **Match must be exact or partial (with clear differences noted).**
   - **Never improvise or ‚Äúfill in‚Äù curriculum answers.**

9. Always show both:
   - **The user's answer (quoted)**
   - **The correct answer from the context (quoted)**

10. **Do NOT add anything beyond what's specified here**. 
    Focus on **curriculum-based feedback**, ensuring responses are **accurate**, **constructive**, and **age-appropriate** for Grades 7-12.
    


The context of the book is provided in chunks: {context}. Use these chunks to craft a response that is relevant and accurate.

**RESPOND ONLY IN {language}. THIS IS A STRICT REQUIREMENT.** Ensure the response is in the current selected language, even if the previous history is in a different language.

**STRICT REQUIREMENT**: Answer based on curriculum chunks only when the input directly relates to the curriculum. For casual or greeting inputs, avoid including curriculum details unless explicitly requested.


**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."

                                                  
**KEY REQUIREMENTS**:

1. **Understand the Question**: Analyze the user's input carefully, identify whether it is a casual, curriculum-related, or ambiguous query, and respond accordingly.
2. **Tailored Responses**: Provide concise, curriculum-aligned answers unless the user requests detailed explanations.
3. **Engaging Style**: Respond with warmth, clarity, and conversational tone, ensuring the user feels encouraged to interact further.
   - **Encourage Interaction**: Actively prompt the user to ask further questions or explore related topics. 
   - **Empathize with the Learner**: Acknowledge the user's feelings and show understanding. For example, "I understand that you're preparing for an exam. Let's break it down together."

---

**Core Input Differentiation**:

1. **Casual Inputs** (e.g., "Hello," "Hi," "How are you?"):
   - Respond in a friendly and concise manner.
   - ***Ignore the curriculum context chunks entirely.***
2. **Curriculum-Related Inputs** (e.g., "Explain Unit 1," "What are the key points?"):
   - Use the provided curriculum chunks to craft responses in *detailed* which is from the curriculum.
3. **Ambiguous Inputs**:
   - Politely ask for clarification without referencing the curriculum unless explicitly necessary.
4. **Engagement Inputs** (e.g., "I have one question regarding...", "Are you ready to answer?"):
   - Respond in an engaging and polite manner confirming readiness.
   - **Actively encourage further interaction**. For example, after answering a question, ask "Do you have any other questions?" or "Would you like to explore this topic further?"

5. **Focus on Accuracy**:
   - Ensure all curriculum-related responses use exact wording from the context chunks.
                                                                                                    
---


**Enhancements**:
1. **Handling Casual and Greeting Questions**:
   - For casual questions or greetings (e.g., "Hello," "Can you help?"), provide a friendly response without referencing the curriculum unnecessarily.
   - Respond in a professional and concise manner unless explicitly asked to include curriculum details.
2. **Context Awareness**:
   - Use {previous_history} to maintain continuity for follow-up questions while aligning strictly with the current query and language.
   - For queries about history (e.g., "What was my last question?"), summarize previous interactions clearly and concisely in the selected language.
4. **Summary Logic**:
   - If the input contains the keyword **detailed**, mention: *"The curriculum-based detailed summary is as follows:"* before providing an in-depth, comprehensive summary.
   - If no specific keyword is mentioned, default to providing a **detailed curriculum-based summary**.
5. **Detailed Responses If Asked by User**:
   - Provide a thorough, well-structured response for all types of queries but when user ask detailed if user doesnt mention detailed answer then provide direct response curriculum context based Short if asked *DETAILED* then provide detailed response.
   - Tailor the complexity based on the learner‚Äôs teaching needs or professional requirements.

7. **Out-of-Syllabus Questions**:
   - If the question is out of syllabus, respond politely: *"Your question is out of syllabus. Please ask a question based on the curriculum. I am designed to help with curriculum-based responses."*
8. **Clarity in Ambiguous Scenarios**:
   - If an input is unclear, ask politely: *"Could you please clarify your question so I can assist you better?"*

                                                                                        
---

**Key Steps**:
1. For specific questions (e.g., "What are the key points discussed in this passage?"):
   - Use curriculum-based content and *verbatim text* from the textbook wherever possible.
   - Provide clear, concise answers aligned with the chapter or unit mentioned in the query.
2. For summary-type questions (e.g., "Give me a summary of Unit 1"):
   - If **generic** is mentioned, provide a concise, high-level summary.
   - If **detailed** is mentioned or no keyword is provided, provide a comprehensive summary, including key themes, exercises, examples, lessons, and chapters.
3. For ambiguous inputs, request clarification professionally and avoid making assumptions.

-> **WHEN EXPLAINING TOPIC OR GIVING ANY ANSWER USE WORD-FOR-WORD TEXT FROM CONTEXT WHEN AVAILABLE**.

-> **WHILE GENERATING ANSWERS, DO NOT ADD UNNECESSARY DETAILS UNLESS THE USER REQUESTS THEM**.

-> **IF THE QUESTION IS FROM CURRICULUM CONTEXT, BEGIN YOUR RESPONSE WITH:**
    - **"BASED ON CURRICULUM"** (if {language} is English).
    - **"ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨"** (if {language} is Arabic).

**Define the Following**:
- **Question**: {input} **Strictly based on the provided context, not generic. Answer directly from context chunks in {language}.**
- **Subject**: {subject}
- **Context**: context (consider this as book/textbook/curriculum)
- **Previous History**: {previous_history}

Always provide meaningful answers aligned with the curriculum. For summary-type questions, ensure responses explicitly align with **generic** or **detailed** keywords if mentioned.

**Improvement Clarifications**:
- Unnecessary ambiguity in unclear inputs is resolved with polite clarification prompts.
- For curriculum-based queries, ensure alignment to the exact wording of the provided context chunks.


**Key Behavior Instructions**:
1. **Use User Input**: Accurately process and understand the user's query before responding.
2. **Professional, Yet Engaging Tone**: Respond with warmth, clarity, and professionalism. Use subtle emojis to add friendliness without compromising professionalism.
3. **Default to Conciseness**: Provide concise, curriculum-aligned responses unless the user asks for more detail.
4. **History Awareness**: Use previous history only when explicitly requested or if the input logically follows prior interactions.
5. **Encourage further interaction** by asking follow-up questions or suggesting additional resources.
6. **Avoid Over-Answering**: Do not provide unnecessary details unless explicitly requested.
7. **Tailored Responses**: Customize responses based on the user's specific needs and interests.
                                                  
**Response Initiation**:
- For curriculum responses:
   - If {language} is English: "Based on the curriculum..."
   - If {language} is Arabic: "ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑŸÖŸÜŸáÿ¨..."

                                                  
---

**This is the previous_history chat: {previous_history}**  
Use it **only when needed** to understand the current response.  
Use it **properly for follow-up answers based on contex**.

---
**GRADE LEVEL CONTEXT: Always assume the student is in Grades 7‚Äì12.**  
**Respond using the following style and behavior at all times:**

**Tone & Delivery Style (Grades 7‚Äì12)**:
- Deliver content in an **engaging and age-appropriate manner**.
- Use **relatable examples**, **storytelling**, and **interactive elements** like **quizzes, discussions, or problem-solving challenges**.
- Tailor explanations to the student's level while **introducing advanced concepts progressively**.
- **Encourage critical thinking, creativity, and curiosity** by connecting lessons to real-life applications or student interests.
- Use **positive reinforcement** and **constructive feedback** to boost confidence and maintain motivation.

**Response Personality (Grades 7‚Äì12)**:
- Responses must be **approachable and encouraging** to foster a supportive learning environment.
- Use **clear, direct language** that respects the learner‚Äôs growing abilities. 
- Explain complex terms in a simple way when needed.
- Keep the tone **relatable and professional**, with **light humor or fun facts** to make learning enjoyable.
- Ask **thought-provoking questions**, suggest activities, and encourage deeper inquiry into the topic.

---

**Examples of Age-Appropriate Responses (Grades 7‚Äì12)**:

**Casual Input:**
  - Input: "Hello!"  
  - Output: "Hey, {name}! üòä What topic are you exploring today?"

**Engagement Input:**
  - Input: "I have doubts about Chapter 4. Can you help me?"
  - Output: "Absolutely, {name}! Chapter 4 dives into **Differentiation**, which is all about understanding how things change. Let‚Äôs work through it together‚Äîwhat part feels tricky to you?"

**Curriculum Query:**
  - Input: "Explain Unit 1."
  - Output: "{name}, Based on the curriculum, Unit 1 explores **the core ideas of Calculus**, like limits, derivatives, and integrals. These are powerful tools for analyzing real-world changes. Want to dig into any of these topics with examples?"

*STRICT REQUIREMENT :- While responding to students use the name : {name}, to address the student such that it feels like the bot is talking to each student individually*
*STRICT REQUIREMENT :- Ensure the responde with name is constant in any language*
**Exam Preparation Query:**
  - Input: "I have an exam tomorrow. Can you help me prepare?"
  - Output: "Definitely, {name}! Let‚Äôs focus on the key areas likely to come up‚Äîdo you want a quick review, practice questions, or both? Let‚Äôs make sure you feel confident going in. üöÄ"

**STRICT REQUIREMENT**: If the question is not related to {subject}, respond:
"This question is not related to {subject}. Please ask a question about {subject}."


"""



app = FastAPI(lifespan=lifespan)

# üîß Add exception handlers for better error debugging
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handle parameter validation errors with detailed logging"""
    print(f"[VALIDATION ERROR] Request: {request.url}")
    print(f"[VALIDATION ERROR] Query params: {dict(request.query_params)}")
    print(f"[VALIDATION ERROR] Validation errors: {exc.errors()}")
    
    # Return detailed error for debugging
    return JSONResponse(
        status_code=400,
        content={
            "error": "Parameter validation failed",
            "details": exc.errors(),
            "url": str(request.url),
            "query_params": dict(request.query_params)
        }
    )

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    """Handle HTTP exceptions with detailed logging"""
    print(f"[HTTP ERROR] {exc.status_code}: {exc.detail}")
    print(f"[HTTP ERROR] Request: {request.url}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail, "url": str(request.url)}
    )

@app.on_event("shutdown")
async def cleanup_on_shutdown():
    """Clean up all chat sessions on shutdown"""
    print("[SHUTDOWN] Cleaning up all chat sessions...")
    chat_session_manager.chat_sessions.clear()
    print("[SHUTDOWN] Cleanup complete")

# --- CORS middleware setup ---
from fastapi.middleware.cors import CORSMiddleware

# CORS middleware setup
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://ai-school-postsse.web.app",    # Production frontend
        "https://ai-assistant.myddns.me",       # No-IP domain
        "http://localhost:3000",                # Local development
        "http://localhost:8000",                # Local testing
        "*"                                     # Allow all origins during development
    ],
    allow_credentials=True,
    allow_methods=["*"],                        # Allow all methods
    allow_headers=["*"],                        # Allow all headers
    expose_headers=[
        "Content-Length",
        "Content-Range"
    ],
    max_age=3600                               # Cache preflight requests for 1 hour
)

app.mount("/audio", StaticFiles(directory=AUDIO_DIR), name="audio")
app.mount("/frontend", StaticFiles(directory="frontend", html=True), name="frontend")
# --- Graphs directory setup and static mount ---
GRAPHS_DIR = "graphs"
os.makedirs(GRAPHS_DIR, exist_ok=True)
app.mount("/graphs", StaticFiles(directory=GRAPHS_DIR), name="graphs")
# --- Uploads directory setup and static mount for serving user-uploaded files ---
os.makedirs("uploads", exist_ok=True)
app.mount("/uploads", StaticFiles(directory="uploads"), name="uploads")
# Base URL where uploaded files (PDF, image, audio) will be served
UPLOADS_BASE_URL = "https://ai-assistant.myddns.me:8443/uploads"
# Base URL where generated graphs will be served
GRAPHS_BASE_URL = "https://ai-assistant.myddns.me:8443/graphs"

def local_path_from_image_url(image_url):
    """
    If image_url points under our /uploads/ static mount, return the local file path;
    otherwise return None.
    """
    if image_url and image_url.startswith(UPLOADS_BASE_URL + "/"):
        filename = image_url.split(UPLOADS_BASE_URL + "/", 1)[1]
        local_path = os.path.join("uploads", filename)
        if os.path.exists(local_path):
            return local_path
    return None

@app.get("/chat-session-stats")
async def get_chat_session_stats():
    """Get statistics about active chat sessions"""
    return chat_session_manager.get_session_stats()

@app.post("/end-chat-session/{chat_id}")
async def end_chat_session(chat_id: str):
    """Call this when user exits chat to free memory"""
    success = await chat_session_manager.cleanup_chat_session(chat_id)
    if success:
        return {"message": f"Chat session {chat_id} cleaned up successfully"}
    else:
        return {"message": f"Chat session {chat_id} was not active"}

@app.post("/cleanup-inactive-sessions")
async def cleanup_inactive_sessions(timeout_minutes: int = 30):
    """Manually trigger cleanup of inactive sessions"""
    await chat_session_manager.cleanup_inactive_sessions(timeout_minutes)
    return {"message": f"Cleaned up sessions inactive for >{timeout_minutes} minutes"}

@app.post("/clear-cache")
async def clear_all_chat_sessions():
    """Clear all active chat sessions (use with caution)"""
    chat_session_manager.chat_sessions.clear()
    return {"message": "All chat sessions cleared successfully"}
async def curriculum_url(
    curriculum: str = Query(..., description="Curriculum document ID to fetch its PDF URL from Firestore")
):
    """
    Return the PDF URL for the specified curriculum ID (fetched from Firestore).
    Raises a 404 if the curriculum document does not exist.
    """
    try:
        url = await get_curriculum_url(curriculum)
        return {"curriculum": curriculum, "url": url}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
def generate_matplotlib_graph(prompt):
    """
    Tries to extract a function or equation from the prompt, plots it with matplotlib,
    saves to a file in graphs/ with a UUID, and returns the URL to the static file.
    """
    import re
    import uuid
    import numpy as np
    import matplotlib.pyplot as plt
    import os

    # Basic function extraction: look for y = ... or plot f(x) = ...
    expr = None
    # Try to find y = ... or f(x) = ... or "plot ..." or "draw ..."
    patterns = [
        r'y\s*=\s*([^\s,;]+)',             # y = ...
        r'f\s*\(\s*x\s*\)\s*=\s*([^\s,;]+)', # f(x) = ...
        r'plot\s+([^\s,;]+)',              # plot ...
        r'draw\s+([^\s,;]+)',              # draw ...
        r'graph\s+([^\s,;]+)',             # graph ...
    ]
    for pat in patterns:
        m = re.search(pat, prompt, re.IGNORECASE)
        if m:
            expr = m.group(1)
            break

    # If not found, look for something that looks like a math expr
    if expr is None:
        # Try to find something like "sin(x)", "x^2+3*x-2", etc.
        m = re.search(r'([a-zA-Z0-9_\+\-\*/\^\.]+\(x\)[^\s,;]*)', prompt)
        if m:
            expr = m.group(1)
    if expr is None:
        # fallback: try to find something like "x^2", "sin(x)", etc.
        m = re.search(r'([a-zA-Z0-9_\+\-\*/\^\.]+)', prompt)
        if m:
            expr = m.group(1)

    # Clean up expr, replace ^ with ** for Python
    if expr:
        expr = expr.replace("^", "**")
    else:
        expr = "x"  # fallback to identity

    # Build x range and evaluate y
    x = np.linspace(-10, 10, 400)
    # Allowed names for eval
    allowed_names = {
        'x': x,
        'sin': np.sin,
        'cos': np.cos,
        'tan': np.tan,
        'exp': np.exp,
        'log': np.log,
        'sqrt': np.sqrt,
        'abs': np.abs,
        'pi': np.pi,
        'e': np.e,
        'arcsin': np.arcsin,
        'arccos': np.arccos,
        'arctan': np.arctan,
        'sinh': np.sinh,
        'cosh': np.cosh,
        'tanh': np.tanh,
        # You can add more as needed
    }
    y = None
    try:
        # Try to evaluate
        y = eval(expr, {"__builtins__": None}, allowed_names)
    except Exception as e:
        # fallback: plot x
        y = x
        expr = "x"

    # Plot
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(x, y, label=f"y = {expr}")
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_title(f"Plot of y = {expr}")
    ax.grid(True)
    ax.legend()

    # Save to file
    fname = f"{uuid.uuid4().hex}.png"
    fpath = os.path.join(GRAPHS_DIR, fname)
    plt.tight_layout()
    plt.savefig(fpath)
    plt.close(fig)

    # URL to serve (absolute)
    return f"{GRAPHS_BASE_URL}/{fname}"

def remove_emojis(text):
    """Remove emojis using pre-compiled pattern - MUCH faster"""
    return EMOJI_PATTERN.sub('', text)

def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    all_text = ""
    for page in reader.pages:
        all_text += page.extract_text() or ""
    return all_text

def text_to_chunks(text, max_tokens):
    words = text.split()
    chunks, current_chunk = [], []
    token_count = 0
    for word in words:
        tokens = tokenizer.encode(word + " ")
        if token_count + len(tokens) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            token_count = 0
        current_chunk.append(word)
        token_count += len(tokens)
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

def split_into_sentences(text):
    """Split text into sentences using pre-compiled pattern - MUCH faster"""
    return [s.strip() for s in SENTENCE_SPLIT_PATTERN.split(text) if s.strip()]

def smart_chunker(text, min_length=120):
    """Smart text chunking using pre-compiled sentence pattern - MUCH faster"""
    sentences = [s.strip() for s in SENTENCE_SPLIT_PATTERN.split(text) if s.strip()]
    buffer = ""
    for s in sentences:
        if len(buffer) + len(s) < min_length:
            buffer += (" " if buffer else "") + s
        else:
            if buffer:
                print("[TTS CHUNK]", repr(buffer))
                yield buffer
            buffer = s
    if buffer:
        print("[TTS CHUNK]", repr(buffer))
        yield buffer

def is_youtube_request(query):
    """Check if the user specifically requested YouTube content"""
    youtube_keywords = ['youtube', 'video', 'ŸÅŸäÿØŸäŸà', 'ŸäŸàÿ™ŸäŸàÿ®', 'ŸÖŸÇÿ∑ÿπ', 'watch']
    query_lower = query.lower()
    return any(keyword in query_lower for keyword in youtube_keywords)

def filter_youtube_links(links, allow_youtube=False):
    """Filter out YouTube links unless specifically requested"""
    if allow_youtube:
        return links
    
    filtered_links = []
    youtube_domains = ['youtube.com', 'youtu.be', 'm.youtube.com']
    
    for link in links:
        url = link.get('url', '')
        if url and not any(domain in url.lower() for domain in youtube_domains):
            filtered_links.append(link)
    
    return filtered_links

def is_arabic_content(url, title, snippet=""):
    """Check if content is likely Arabic based on URL patterns and text"""
    if not url:
        return False
    
    # Arabic domain indicators
    arabic_domains = ['.sa', '.ae', '.eg', '.jo', '.lb', '.sy', '.iq', '.ye', '.om', '.kw', '.qa', '.bh']
    arabic_subdomains = ['ar.', 'arabic.', 'ÿπÿ±ÿ®Ÿä.']
    arabic_paths = ['/ar/', '/arabic/', '/ÿπÿ±ÿ®Ÿä/']
    
    url_lower = url.lower()
    
    # Check domain
    if any(domain in url_lower for domain in arabic_domains):
        return True
    
    # Check subdomains
    if any(subdomain in url_lower for subdomain in arabic_subdomains):
        return True
    
    # Check paths
    if any(path in url_lower for path in arabic_paths):
        return True
    
    # Check if title or snippet contains Arabic characters
    text_to_check = f"{title} {snippet}"
    if any('\u0600' <= char <= '\u06FF' for char in text_to_check):
        return True
    
    return False

def validate_and_clean_links(links):
    """Validate links and remove null/empty ones"""
    cleaned_links = []
    
    for link in links:
        if not isinstance(link, dict):
            continue
            
        url = link.get('url', '').strip()
        title = link.get('title', '').strip()
        
        # Skip if URL is null, empty, or invalid
        if not url or url.lower() in ['null', 'none', '']:
            continue
            
        # Skip if URL doesn't start with http
        if not url.startswith(('http://', 'https://')):
            continue
            
        # Clean up the link
        cleaned_link = {
            'url': url,
            'title': title if title else 'Untitled',
            'summary': link.get('summary', ''),
            'desc': link.get('desc', '')
        }
        
        cleaned_links.append(cleaned_link)
    
    return cleaned_links

async def generate_weblink_perplexity(query, language="en"):
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Check if YouTube is specifically requested
    allow_youtube = is_youtube_request(query)
    
    # Adjust system prompt based on language and requirements
    if language.lower().startswith('ar'):
        system_prompt = (
            "Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸÇÿ∑. ŸÇÿØŸÖ ÿ±Ÿàÿßÿ®ÿ∑ ŸÑŸÖŸàÿßŸÇÿπ ÿπÿ±ÿ®Ÿäÿ© ŸÖŸàÿ´ŸàŸÇÿ© ŸÅŸÇÿ∑. "
            "ÿ™ÿ¨ŸÜÿ® ÿßŸÑŸÖŸàÿßŸÇÿπ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ŸàÿßŸÑŸÖŸàÿßŸÇÿπ ÿ∫Ÿäÿ± ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. "
            "ŸÑÿß ÿ™ŸÇÿØŸÖ ÿ±Ÿàÿßÿ®ÿ∑ ŸäŸàÿ™ŸäŸàÿ® ÿ•ŸÑÿß ÿ•ÿ∞ÿß ÿ∑ŸèŸÑÿ® ÿ∞ŸÑŸÉ ÿµÿ±ÿßÿ≠ÿ©. "
            "ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ£ŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ±Ÿàÿßÿ®ÿ∑ ÿµÿßŸÑÿ≠ÿ© ŸàŸÑŸäÿ≥ÿ™ ŸÅÿßÿ±ÿ∫ÿ©."
        )
    else:
        system_prompt = (
            "Provide reliable, educational links. Avoid YouTube links unless specifically requested. "
            "Ensure all links are valid and not null. Focus on authoritative sources."
        )
    
    payload = {
        "model": "sonar-pro",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ],
        "max_tokens": 400,
        "temperature": 0.5
    }

    print(f"[DEBUG] Perplexity query: {query}, Language: {language}, Allow YouTube: {allow_youtube}")

    async with httpx.AsyncClient(timeout=20) as client:
        try:
            resp = await client.post(
                "https://api.perplexity.ai/chat/completions",
                headers=headers,
                json=payload
            )
            print("DEBUG - Status Code:", resp.status_code)
            resp.raise_for_status()
            data = resp.json()

            # Look for actual Perplexity researched web links
            if "search_results" in data and data["search_results"]:
                # Process all search results, not just the first one
                raw_links = []
                for result in data["search_results"]:
                    url = result.get("url", "")
                    title = result.get("title", "")
                    snippet = result.get("snippet", "")
                    
                    if url:  # Only add if URL exists
                        raw_links.append({
                            "url": url,
                            "title": title,
                            "summary": f"{title}: {snippet}" if snippet else title,
                            "desc": snippet
                        })
                
                # Clean and validate links
                cleaned_links = validate_and_clean_links(raw_links)
                
                # Filter YouTube if not requested
                filtered_links = filter_youtube_links(cleaned_links, allow_youtube)
                
                # Filter for Arabic content if Arabic language
                if language.lower().startswith('ar'):
                    arabic_links = []
                    for link in filtered_links:
                        if is_arabic_content(link['url'], link['title'], link.get('desc', '')):
                            arabic_links.append(link)
                    filtered_links = arabic_links if arabic_links else filtered_links[:2]  # Fallback to first 2 if no Arabic found
                
                # Return the best link or fallback
                if filtered_links:
                    return {"url": filtered_links[0]["url"], "desc": filtered_links[0]["summary"]}
                else:
                    # No valid links found after filtering
                    fallback_url = f"https://www.perplexity.ai/search?q={urllib.parse.quote(query)}"
                    return {"url": fallback_url, "desc": "No suitable links found after filtering."}
                    
            elif "citations" in data and data["citations"]:
                # Process citations
                citations = data["citations"]
                valid_citations = [url for url in citations if url and url.strip() and url.startswith(('http://', 'https://'))]
                
                if valid_citations:
                    # Filter YouTube from citations too
                    if not allow_youtube:
                        youtube_domains = ['youtube.com', 'youtu.be', 'm.youtube.com']
                        valid_citations = [url for url in valid_citations 
                                         if not any(domain in url.lower() for domain in youtube_domains)]
                    
                    if valid_citations:
                        return {"url": valid_citations[0], "desc": "See the cited resource."}
                
            # Fallback if no valid results
            fallback_url = f"https://www.perplexity.ai/search?q={urllib.parse.quote(query)}"
            return {"url": fallback_url, "desc": "No summary available."}
            
        except Exception as e:
            print("[Perplexity API ERROR]", str(e))
            fallback_url = f"https://www.perplexity.ai/search?q={urllib.parse.quote(query)}"
            return {
                "url": fallback_url,
                "desc": f"Could not get summary due to error: {e}"
            }
async def generate_runware_image(prompt):
    print(f"[DEBUG] Calling Runware with prompt: {prompt}")
    runware = Runware(api_key=RUNWARE_API_KEY)
    await runware.connect()
    request_image = IImageInference(
        positivePrompt=prompt,
        model="runware:100@1",
        numberResults=1,
        height=1024,
        width=1024,
    )
    images = await runware.imageInference(requestImage=request_image)
    print(f"[DEBUG] Runware returned: {images}")
    return images[0].imageURL if images else None

async def vision_caption_openai(img: Image.Image = None, image_url: str = None) -> str:
    """
    Caption an image using OpenAI GPT-4o Vision.
    Supports both in-memory PIL.Image (local files) and remote URLs.
    """
    if img is not None:
        buf = io.BytesIO()
        img.save(buf, format="JPEG")
        img_b64 = base64.b64encode(buf.getvalue()).decode()
        image_content = {"url": f"data:image/jpeg;base64,{img_b64}"}
    elif image_url is not None:
        image_content = {"url": image_url}
    else:
        raise ValueError("Must provide either img or image_url.")

    messages = [
        {"role": "user", "content": [
            {"type": "text", "text": "Describe this image in detail."},
            {"type": "image_url", "image_url": image_content}
        ]}
    ]
    resp = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        max_tokens=256,
        temperature=0.5
    )
    return resp.choices[0].message.content.strip()



def remove_punctuation(text):
    """Remove punctuation using pre-compiled pattern - MUCH faster"""
    return PUNCTUATION_PATTERN.sub('', text)



def sanitize_for_tts(text):
    """Clean text for TTS using pre-compiled patterns - MUCH faster"""
    # Remove LaTeX commands (\frac, \left, \right, etc.)
    text = LATEX_COMMANDS_PATTERN.sub('', text)
    text = CURLY_BRACES_PATTERN.sub('', text)
    text = DOLLAR_SIGNS_PATTERN.sub('', text)
    text = BACKSLASH_PATTERN.sub('', text)
    text = text.replace('^', ' ÿ£ÿ≥ ')  # Say "power" in Arabic
    text = UNDERSCORE_PATTERN.sub(' ', text)   # Say "sub"
    text = WHITESPACE_PATTERN.sub(' ', text)
    return text.strip()



def remove_latex(text):
    """Remove remaining LaTeX commands and curly braces using pre-compiled patterns - MUCH faster"""
    text = LATEX_COMMANDS_PATTERN.sub("", text)
    text = CURLY_BRACES_PATTERN.sub("", text)
    return text


class PDFIndex:
    def __init__(self):
        self.chunks = None
        self.embedder = None
        self.index = None
    def build(self, file_path):
        text = extract_text_from_pdf(file_path)
        self.chunks = text_to_chunks(text, MAX_TOKENS_PER_CHUNK)
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        chunk_embeddings = self.embedder.encode(self.chunks, show_progress_bar=True)
        dimension = chunk_embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(chunk_embeddings))
    def get_context(self, question, topk=1):
        question_embedding = self.embedder.encode([question])
        D, I = self.index.search(np.array(question_embedding), k=topk)
        return "\n".join([self.chunks[idx] for idx in I[0]])

pdf_idx = PDFIndex()

def extract_text_from_image(file_path: str) -> str:
    """Use OCR to extract text from the uploaded image file."""
    try:
        img = Image.open(file_path)
        text = pytesseract.image_to_string(img)
        return text.strip() or "No text detected in image."
    except Exception as e:
        return f"OCR error: {e}"

class ImageIndex:
    def __init__(self):
        self.chunks = None
        self.embedder = None
        self.index = None
    def build(self, file_path: str):
        text = extract_text_from_image(file_path)
        self.chunks = text_to_chunks(text, MAX_TOKENS_PER_CHUNK)
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        embeddings = self.embedder.encode(self.chunks, show_progress_bar=True)
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(embeddings))
    def get_context(self, question: str, topk: int = 1) -> str:
        q_emb = self.embedder.encode([question])
        D, I = self.index.search(np.array(q_emb), k=topk)
        return "\n".join(self.chunks[idx] for idx in I[0])

image_idx = ImageIndex()

@app.post("/upload-file")
async def upload_file(request: Request, file: UploadFile = File(...)):
    """
    üöÄ OPTIMIZED: Save uploaded PDF and immediately start background processing.
    Uploaded files are served from:
      https://ai-assistant.myddns.me:8443/uploads/{filename}.pdf
    where {filename} is the actual filename of the uploaded PDF.
    """
    filename = file.filename or f"{uuid.uuid4()}.pdf"
    local_path = os.path.join("uploads", secure_filename(filename))
    
    # Ensure uploads directory exists
    os.makedirs("uploads", exist_ok=True)
    
    content = await file.read()
    with open(local_path, "wb") as f:
        f.write(content)
    url = f"{UPLOADS_BASE_URL}/{filename}"
    
    # üöÄ IMMEDIATELY start background processing for instant responses
    import hashlib
    file_hash = hashlib.md5(content).hexdigest()
    file_type = filename.split('.')[-1].lower() if '.' in filename else 'pdf'
    
    # Start background processing (don't wait for it)
    asyncio.create_task(
        multi_modal_processor.preprocess_file_in_background(content, file_hash, file_type)
    )
    
    print(f"[UPLOAD] ‚úÖ File uploaded and background processing started: {url}")
    
    return {
        "pdf_url": url,
        "file_hash": file_hash,
        "message": f"File uploaded and processing started in background for instant responses",
        "background_processing": True,
        "optimization": "enabled"
    }

@app.post("/upload-image")
async def upload_image(request: Request, file: UploadFile = File(...)):
    """
    üöÄ OPTIMIZED: Save uploaded image and immediately start background processing.
    Uploaded files are served from:
      https://ai-assistant.myddns.me:8443/uploads/{filename}.png
    where {filename} is the actual filename of the uploaded image.
    """
    filename = file.filename or f"{uuid.uuid4()}.png"
    local_path = os.path.join("uploads", secure_filename(filename))
    
    # Ensure uploads directory exists
    os.makedirs("uploads", exist_ok=True)
    
    content = await file.read()
    with open(local_path, "wb") as f:
        f.write(content)
    url = f"{UPLOADS_BASE_URL}/{filename}"
    
    # üöÄ IMMEDIATELY start background processing for instant responses
    import hashlib
    image_hash = hashlib.md5(content).hexdigest()
    
    # Start background processing (don't wait for it)
    asyncio.create_task(
        multi_modal_processor.preprocess_image_in_background(content, image_hash)
    )
    
    print(f"[UPLOAD] ‚úÖ Image uploaded and background processing started: {url}")
    
    return {
        "image_url": url,
        "image_hash": image_hash,
        "message": f"Image uploaded and processing started in background for instant responses",
        "background_processing": True,
        "optimization": "enabled"
    }

@app.post("/upload-audio")
async def upload_audio(request: Request,
                       file: UploadFile = File(None),
                       audio_url: str = Query(None),
                       language: str = Query(None)):
    """
    Transcribe audio to text using Whisper.
    Accepts either a multipart file upload or an audio_url pointing to a hosted file.
    """
    if not file and not audio_url:
        raise HTTPException(status_code=400, detail="Must provide audio file or audio_url")

    # Determine transcription language hint
    lang_lower = (language or "").strip().lower()
    if lang_lower.startswith("ar"):
        whisper_lang = "ar"
    elif lang_lower.startswith("en"):
        whisper_lang = "en"
    else:
        whisper_lang = None

    # Read audio from upload or remote URL and transcribe
    if file:
        filename = file.filename or f"{uuid.uuid4()}.wav"
        secure_name = secure_filename(filename)
        local_path = os.path.join("uploads", secure_name)
        content = await file.read()
        with open(local_path, "wb") as f:
            f.write(content)
        try:
            af = open(local_path, "rb")
            result = openai.audio.transcriptions.create(
                file=af,
                model="whisper-1",
                language=whisper_lang
            )
            transcription = result.text.strip()
        finally:
            af.close()
            try:
                os.remove(local_path)
            except OSError:
                pass
    else:
        # Fetch remote audio from provided URL
        async with aiohttp.ClientSession() as session:
            async with session.get(audio_url) as resp:
                if resp.status != 200:
                    raise HTTPException(status_code=400, detail=f"Failed to fetch audio: {resp.status}")
                audio_bytes = await resp.read()
        buf = io.BytesIO(audio_bytes)
        buf.name = os.path.basename(urllib.parse.urlparse(audio_url).path) or f"{uuid.uuid4()}.wav"
        result = openai.audio.transcriptions.create(
            file=buf,
            model="whisper-1",
            language=whisper_lang
        )
        transcription = result.text.strip()

    return {"text": transcription}


async def generate_weblink_and_summary(prompt, language="en"):
    # Use the improved Perplexity API function to get web link and summary
    result = await generate_weblink_perplexity(prompt, language)
    return {
        "url": result.get("url", "https://perplexity.ai/search?q=" + prompt.replace(' ', '+')),
        "summary": result.get("desc", "No summary available.")
    }

# Helper function for streaming responses
async def prepend_init(stream):
    """Prepend initial SSE event to inform client about RAG mode"""
    async for evt in stream:
        yield evt







@app.get("/stream-answer")
async def stream_answer(
    request: Request,
    role: str = Query(...),
    user_id: str = Query(...),
    grade: str = Query(None),
    curriculum: str = Query(...),
    language: str = Query(...),
    subject: str = Query(None),
    question: str = Query(None),
    chat_id: str = Query(...),
    activity: str = Query(None),
    file_url: str = Query(None),
    image_url: str = Query(None),
    file: str = Query(None),
    image: str = Query(None),
    file_provided: bool = Query(None),
    pdf_provided: bool = Query(None),
    image_provided: bool = Query(None),
):
    """
    üöÄ CLEAN VERSION: Stream an answer with robust parameter handling.
    All structural issues have been fixed and FAISS optimizations applied.
    """
    
    print(f"[CLEAN-DEBUG] Function started with question: {question}")
    print(f"[CLEAN-DEBUG] Chat ID: {chat_id}, Curriculum: {curriculum}")
    
    try:
        # üîß Parameter validation and cleanup
        if not curriculum or curriculum.strip() == "":
            raise HTTPException(status_code=400, detail="Missing required parameter: curriculum")
        
        if not question or question.strip() == "":
            question = "Hello"
        
        if not chat_id or chat_id.strip() == "":
            raise HTTPException(status_code=400, detail="Missing required parameter: chat_id")
        
        # Clean up parameters
        def safe_bool(value):
            if value is None:
                return None
            if isinstance(value, bool):
                return value
            if isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'on')
            return bool(value)
        
        file_provided = safe_bool(file_provided)
        image_provided = safe_bool(image_provided)
        
        # Determine processing flags
        file_flag = bool(file_provided or file_url or file)
        image_flag = bool(image_provided or image_url or image)
        
        print(f"[CLEAN-DEBUG] Flags - file: {file_flag}, image: {image_flag}")
        
        # Initialize context
        context = ""
        formatted_history = ""
        
        # Process based on flags
        if file_flag:
            print(f"[CLEAN-DEBUG] Processing file-based request")
            # Use optimized chat-session vectors
            vectors = await chat_session_manager.get_vectors_for_chat(chat_id, curriculum)
            docs = await retrieve_documents(vectors, question)
            context = "\n\n".join(doc.page_content for doc in docs)
            formatted_history = await get_chat_history(chat_id)
            
        elif image_flag:
            print(f"[CLEAN-DEBUG] Processing image-based request")
            # Use optimized chat-session vectors
            vectors = await chat_session_manager.get_vectors_for_chat(chat_id, curriculum)
            docs = await retrieve_documents(vectors, question)
            context = "\n\n".join(doc.page_content for doc in docs)
            formatted_history = await get_chat_history(chat_id)
            
        else:
            # Default: curriculum-based RAG with chat-session management
            print(f"[CLEAN-DEBUG] Using chat-session based RAG for chat {chat_id}")
            formatted_history = await get_chat_history(chat_id)
            
            # üöÄ OPTIMIZED: Use chat-session vectors (with FAISS fixes applied)
            vectors = await chat_session_manager.get_vectors_for_chat(chat_id, curriculum)
            docs = await retrieve_documents(vectors, question)
            context = "\n\n".join(doc.page_content for doc in docs)
            
        print(f"[CLEAN-DEBUG] Context length: {len(context)} chars")
        
        # Get user name for personalization
        user_name = await get_user_name(user_id)
        
        # ---- IMAGE GENERATION (Teachers only) ----
        if (role or "").strip().lower() == "teacher":
            # Check for image generation keywords
            img_keywords = ["generation", "GENERATION", "PLOT", "Plot", "create image", "show image", "visual", "illustration", "diagram"]
            if any(keyword in question for keyword in img_keywords):
                # Extract description for image generation
                prompt_desc = question
                
                # IMAGE GENERATION for teacher
                img_url = await generate_runware_image(prompt_desc or context)
                if not img_url:
                    async def fail_stream():
                        yield f"data: {json.dumps({'type':'error','error':'Image generation failed or no image returned.'})}\n\n"
                        yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    return StreamingResponse(fail_stream(), media_type="text/event-stream")
                
                async def event_stream():
                    yield f"data: {json.dumps({'type': 'image', 'url': img_url, 'desc': prompt_desc or 'Generated.'})}\n\n"
                    yield f"data: {json.dumps({'type':'done'})}\n\n"
                
                return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")

        # ---- GRAPH GENERATION (Block out-of-context for ALL) ----
        graph_keywords = ["graph", "plot", "chart", "diagram"]
        if any(keyword in question.lower() for keyword in graph_keywords):
            # Check if request is curriculum-related
            if not any(word in context.lower() for word in question.lower().split() if len(word) > 3):
                async def error_stream():
                    yield f"data: {json.dumps({'type':'error','error':'Sorry, the requested graph is not in the curriculum. Please ask for graphs related to your lessons or curriculum topics.'})}\n\n"
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                return StreamingResponse(error_stream(), media_type="text/event-stream")
            
            # --- GRAPH GENERATION: use Matplotlib, not Runware ---
            url = generate_matplotlib_graph(question)
            async def event_stream():
                yield f"data: {json.dumps({'type': 'image', 'url': url, 'desc': 'Generated graph.'})}\n\n"
                yield f"data: {json.dumps({'type':'done'})}\n\n"
            
            return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")

        # ---- PERPLEXITY WEBLINK ----
        weblink_keywords = ["weblink", "web link", "website", "url", "link", "online", "internet"]
        if any(keyword in question.lower() for keyword in weblink_keywords):
            # Check if request is curriculum-related for students
            if (role or "").strip().lower() == "student":
                if not any(word in context.lower() for word in question.lower().split() if len(word) > 3):
                    async def error_stream():
                        yield f"data: {json.dumps({'type':'error','error':'Sorry, web links are only allowed for questions related to your curriculum. Please ask about topics from your uploaded content.'})}\n\n"
                        yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    return StreamingResponse(error_stream(), media_type="text/event-stream")

            # Generate weblink using Perplexity
            try:
                result = await generate_weblink_perplexity(question, language)
                
                async def weblink_stream():
                    # Send the weblink result
                    yield f"data: {json.dumps({'type': 'weblink', 'title': 'Web Resource', 'url': result.get('url', ''), 'description': result.get('desc', '')})}\n\n"
                    yield f"data: {json.dumps({'type':'done'})}\n\n"
                
                return StreamingResponse(prepend_init(weblink_stream()), media_type="text/event-stream")
            except Exception as e:
                async def error_stream():
                    yield f"data: {json.dumps({'type': 'error', 'error': f'Weblink generation failed: {str(e)}'})}\n\n"
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                return StreamingResponse(error_stream(), media_type="text/event-stream")



        
        # Build prompt
        prompt_header = teacher_prompt if (role or "").strip().lower() == "teacher" else student_prompt
        
        user_content = f"""{prompt_header}

{context}

Now answer the question:
{question}"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        # üöÄ STREAMING RESPONSE
        answer_so_far = ""
        current_sentence = ""
        SENT_END = re.compile(r'([.!?])(\s|$)')
        
        # Determine voice
        voice = "ar-SA-ZariyahNeural" if (language or "").strip().lower().startswith("ar") else "en-US-JennyNeural"
        
        async def stream_audio(sentence):
            try:
                clean_sentence = sanitize_for_tts(sentence)
                if not clean_sentence.strip():
                    return None
                
                audio_url = await generate_complete_tts(clean_sentence, language)
                if audio_url:
                    return f"data: {json.dumps({'type': 'audio_pending', 'sentence': clean_sentence, 'audio_url': audio_url})}\n\n"
                return None
            except Exception as e:
                print(f"[TTS ERROR] {e}")
                return None
        
        try:
            stream = openai.chat.completions.create(
                model="gpt-4.1",
                messages=messages,
                stream=True,
                max_tokens=2000,
                temperature=0.7
            )
            
            async def event_stream():
                nonlocal answer_so_far, current_sentence
                
                # Send init event
                yield f"data: {json.dumps({'type':'init','image_provided': image_flag, 'file_provided': file_flag})}\n\n"
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        answer_so_far += content
                        current_sentence += content
                        
                        # Send partial content
                        yield f"data: {json.dumps({'type': 'partial', 'partial': content})}\n\n"
                        
                        # Check for sentence end
                        if SENT_END.search(current_sentence):
                            audio_event = await stream_audio(current_sentence.strip())
                            if audio_event:
                                yield audio_event
                            current_sentence = ""
                
                # Handle remaining sentence
                if current_sentence.strip():
                    audio_event = await stream_audio(current_sentence.strip())
                    if audio_event:
                        yield audio_event
                
                # Update chat history
                try:
                    await update_chat_history_speech(user_id, question, answer_so_far)
                except Exception as e:
                    print(f"[HISTORY ERROR] {e}")
                
                yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
            return StreamingResponse(prepend_init(event_stream()), media_type="text/event-stream")
            
        except Exception as ex:
            print(f"[STREAMING ERROR] {ex}")
            async def error_stream():
                yield f"data: {json.dumps({'type': 'error', 'error': f'Streaming failure: {str(ex)}'})}\n\n"
                yield f"data: {json.dumps({'type': 'done'})}\n\n"
            return StreamingResponse(error_stream(), media_type="text/event-stream")
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"[FUNCTION ERROR] {e}")
        async def error_stream():
            yield f"data: {json.dumps({'type': 'error', 'error': f'Server error: {str(e)}'})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        return StreamingResponse(error_stream(), media_type="text/event-stream")

# ============================================================================

@app.get("/test-params")
async def test_params(
    request: Request,
    role: str = Query(...),
    curriculum: str = Query(...),
    language: str = Query(...),
    question: str = Query(None),
    chat_id: str = Query(...),
    activity: str = Query(None),
    image: str = Query(None),
    pdf: str = Query(None)
):
    """Simple test endpoint to verify parameter parsing"""
    return {
        "status": "success",
        "received_params": {
            "role": role,
            "curriculum": curriculum,
            "language": language,
            "question": question,
            "chat_id": chat_id,
            "activity": activity,
            "image": image,
            "pdf": pdf
        },
        "raw_query_params": dict(request.query_params),
        "url": str(request.url)
    }

@app.get("/debug-params")
async def debug_params(request: Request):
    """Debug endpoint to check parameter parsing"""
    query_params = dict(request.query_params)
    
    return {
        "query_params": query_params,
        "param_count": len(query_params),
        "required_params_present": {
            "role": "role" in query_params,
            "user_id": "user_id" in query_params,
            "curriculum": "curriculum" in query_params,
            "language": "language" in query_params,
            "chat_id": "chat_id" in query_params,
            "question": "question" in query_params
        },
        "empty_params": [k for k, v in query_params.items() if v == ""],
        "malformed_params": [k for k, v in query_params.items() if v in ["null", "undefined", "None"]]
    }

@app.get("/answer")
async def get_full_answer(
    role: str = Query(...),
    user_id: str = Query(...),
    grade: str = Query(None),
    curriculum: str = Query(...),
    language: str = Query(...),
    subject: str = Query(None),
    question: str = Query(...),
    chat_id: str = Query(...),
):
    """
    Return the full, non-streamed answer along with context for debugging.
    """
    # Build the same prompt_header logic as in stream_answer
    prompt_header = ""
    if (role or "").strip().lower() == "teacher":
        prompt_header = teacher_prompt
    elif (role or "").strip().lower() == "student":
        try:
            grade_num = int(grade)
            prompt_header = student_prompt_1 if 1 <= grade_num <= 6 else student_prompt
        except:
            prompt_header = student_prompt
    # Language enforcement
    if language and language.lower().startswith("ar"):
        prompt_header = ("STRICT RULE: Always answer ONLY in Arabic, even if the question/context/history is in another language. "
                         "Translate all context if needed. Never use English in your answer.\n") + prompt_header
    elif language and language.lower().startswith("en"):
        prompt_header = ("STRICT RULE: Always answer ONLY in English, even if the question/context/history is in another language. "
                         "Translate all context if needed. Never use Arabic in your answer.\n") + prompt_header
    # Fill in user name
    user_name = await get_user_name(user_id)
    prompt_header = prompt_header.replace("{name}", user_name)
    # Fetch history and vectors with chat-session management
    formatted_history = await get_chat_history(chat_id)
    vectors = await chat_session_manager.get_vectors_for_chat(chat_id, curriculum)
    docs = await retrieve_documents(vectors, question)
    context = "\n\n".join(doc.page_content for doc in docs)

    clean_header = prompt_header.replace("{previous_history}", formatted_history)
    user_content = (
        f"{clean_header}\n"
        "(Do not repeat the words 'Context:' or 'Question:' in your answer.)\n\n"
        f"{context}\n\n"
        f"Now answer the question:\n{question}"
    )
    # Build system message with language and math instructions
    system_content = SYSTEM_MATH_INSTRUCTION
    if language and language.lower().startswith("ar"):
        system_content += "\nSTRICT RULE: You MUST ALWAYS respond in Arabic only, regardless of input language. "
        system_content += "Translate any English content to Arabic in your response. Never use English.\n"
    elif language and language.lower().startswith("en"):
        system_content += "\nSTRICT RULE: You MUST ALWAYS respond in English only, regardless of input language. "
        system_content += "Translate any Arabic content to English in your response. Never use Arabic.\n"

    system_message = {"role": "system", "content": system_content}
    user_message = {"role": "user", "content": user_content}
    messages = [system_message, user_message]

    resp = openai.chat.completions.create(
        model="gpt-4.1",
        messages=messages,
        temperature=0.5,
        max_tokens=2000,
    )
    full_answer = resp.choices[0].message.content.strip()
    return {
        "curriculum": curriculum,
        "question": question,
        "context": context,
        "answer": full_answer,
    }

# ----------- Request Body Schema -----------

class ChatHistoryEntry(BaseModel):
    type: Literal["ar", "avatar", "normal"]
    chat_id: str
    id: str
    role: str
    content: str
    audiourl: str
    imageselected: str


# ----------- GET Endpoints -----------

@app.get("/api/chat-detail/{doc_id}")
async def get_chat_detail(doc_id: str = Path(...)):
    """Get chat detail with improved Arabic text handling"""
    doc_ref = db.collection("chat_detail").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        
        # Handle Arabic text in history items
        if "history" in data:
            for item in data["history"]:
                process_text_fields(item, ["content", "question", "answer"])
        
        # Return with proper encoding headers
        return JSONResponse(
            content=data,
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Type": "application/json; charset=utf-8",
                "X-Content-Encoding": "utf-8"
            }
        )
    else:
        raise HTTPException(status_code=404, detail="Document not found")


@app.get("/api/chat-detail-ar/{doc_id}")
async def get_chat_detail_ar(doc_id: str = Path(...)):
    """Get Arabic chat detail with improved Arabic text handling"""
    doc_ref = db.collection("chat_details_ar").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        
        # Handle Arabic text in history items
        if "history" in data:
            for item in data["history"]:
                process_text_fields(item, ["content", "question", "answer"])
        
        # Return with proper encoding headers
        return JSONResponse(
            content=data,
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Type": "application/json; charset=utf-8",
                "X-Content-Encoding": "utf-8"
            }
        )
    else:
        raise HTTPException(status_code=404, detail="Document not found")


@app.get("/api/avatarchatdetails/{doc_id}")
async def get_chat_detail_avatar(doc_id: str = Path(...)):
    """Get avatar chat detail with improved Arabic text handling"""
    doc_ref = db.collection("avatarchatdetails").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        
        # Handle Arabic text in history items
        if "history" in data:
            for item in data["history"]:
                process_text_fields(item, ["content", "question", "answer"])
        
        # Return with proper encoding headers
        return JSONResponse(
            content=data,
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Type": "application/json; charset=utf-8",
                "X-Content-Encoding": "utf-8"
            }
        )
    else:
        raise HTTPException(status_code=404, detail="Document not found")


# ----------- POST Endpoint -----------

@app.post("/api/chat-detail-store")
async def add_chat_history(entry: ChatHistoryEntry):
    """
    Append a chat history entry to Firestore with improved Arabic text handling.
    """
    type_mapping = {
        "ar": "chat_details_ar",
        "avatar": "avatarchatdetails",
        "normal": "chat_detail",
    }
    
    collection_name = type_mapping.get(entry.type)
    if not collection_name:
        raise HTTPException(status_code=400, detail=f"Invalid type: {entry.type}")
    
    try:
        # First decode if the input is somehow corrupted
        content = decode_arabic_text(entry.content)
        
        # Get current timestamp
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Create entry with decoded content
        entry_dict = {
            "id": entry.id,
            "role": entry.role,
            "content": content,
            "audiourl": entry.audiourl,
            "imageselected": entry.imageselected,
            "timestamp": current_time
        }
        
        # Get existing history or create new
        doc_ref = db.collection(collection_name).document(entry.chat_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            history = data.get("history", [])
            history.append(entry_dict)
            doc_ref.update({
                "history": history,
                "last_updated": current_time
            })
        else:
            doc_ref.set({
                "history": [entry_dict],
                "created_at": current_time
            })
            
        return JSONResponse(
            content={"status": "success", "message": "Chat history updated"},
            media_type="application/json; charset=utf-8",
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to store chat history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
    
    collection_name = type_mapping.get(entry.type)
    if not collection_name:
        raise HTTPException(status_code=400, detail=f"Invalid type: {entry.type}")
    
    try:
        # First decode if the input is somehow corrupted
        content = decode_arabic_text(entry.content)
        
        # Create entry with decoded content
        entry_dict = {
            "id": entry.id,
            "role": entry.role,
            "content": content,
            "audiourl": entry.audiourl,
            "imageselected": entry.imageselected,
            "timestamp": datetime.now().isoformat(),
            "encoding": "utf-8"
        }
        
        # Get existing history or create new
        doc_ref = db.collection(collection_name).document(entry.chat_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            history = data.get("history", [])
            history.append(entry_dict)
            doc_ref.update({
                "history": history,
                "last_updated": datetime.now().isoformat()
            })
        else:
            doc_ref.set({
                "history": [entry_dict],
                "created_at": datetime.now().isoformat()
            })
            
        return JSONResponse(
            content={"status": "success", "message": "Chat history updated"},
            media_type="application/json; charset=utf-8",
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to store chat history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
    
    collection_name = type_mapping.get(entry.type)
    if not collection_name:
        raise HTTPException(status_code=400, detail=f"Invalid type: {entry.type}")
    
    try:
        # First decode if the input is somehow corrupted
        content = decode_arabic_text(entry.content)
        
        # Create entry with decoded content
        entry_dict = {
            "id": entry.id,
            "role": entry.role,
            "content": content,
            "audiourl": entry.audiourl,
            "imageselected": entry.imageselected,
            "timestamp": datetime.now().isoformat(),
            "encoding": "utf-8"
        }
        
        # Get existing history or create new
        doc_ref = db.collection(collection_name).document(entry.chat_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            history = data.get("history", [])
            history.append(entry_dict)
            doc_ref.update({
                "history": history,
                "last_updated": datetime.now().isoformat()
            })
        else:
            doc_ref.set({
                "history": [entry_dict],
                "created_at": datetime.now().isoformat()
            })
            
        return JSONResponse(
            content={"status": "success", "message": "Chat history updated"},
            media_type="application/json; charset=utf-8",
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to store chat history: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))



@app.get("/api/chat-detail/{doc_id}")
async def get_chat_detail(doc_id: str = Path(...)):
    """Get chat detail with improved Arabic text handling"""
    doc_ref = db.collection("chat_detail").document(doc_id)
    doc = doc_ref.get()

    if doc.exists:
        data = doc.to_dict()
        data["id"] = doc.id
        
        # Handle Arabic text in history items
        if "history" in data:
            for item in data["history"]:
                # Process all text fields that might contain Arabic
                for field in ["content", "question", "answer"]:
                    if isinstance(item.get(field), str):
                        item[field] = decode_arabic_text(item[field])
        
        # Return with proper encoding headers
        return JSONResponse(
            content=data,
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Type": "application/json; charset=utf-8",
                "X-Content-Encoding": "utf-8"
            }
        )
    else:
        raise HTTPException(status_code=404, detail="Document not found")


@app.get("/check-local-faiss/{curriculum_id}")
async def check_local_faiss(curriculum_id: str):
    """Check if FAISS index exists in AWS EC2 local storage."""
    local_path = f'faiss/faiss_index_{curriculum_id}'
    faiss_file = os.path.join(local_path, 'index.faiss')
    pkl_file = os.path.join(local_path, 'index.pkl')
    
    return {
        "location": "AWS EC2",
        "path": local_path,
        "faiss_exists": os.path.exists(faiss_file),
        "pkl_exists": os.path.exists(pkl_file),
        "faiss_size": os.path.getsize(faiss_file) if os.path.exists(faiss_file) else None,
        "pkl_size": os.path.getsize(pkl_file) if os.path.exists(pkl_file) else None
    }

@app.get("/check-gcp-faiss/{curriculum_id}")
async def check_gcp_faiss(curriculum_id: str):
    """Check if FAISS index exists in GCP Firebase Storage with detailed error handling."""
    try:
        gcp_path = f'users/KnowledgeBase/faiss_index_{curriculum_id}'
        faiss_blob = bucket.blob(f'{gcp_path}/index.faiss')
        pkl_blob = bucket.blob(f'{gcp_path}/index.pkl')
        
        # Check existence with error handling
        try:
            faiss_exists = faiss_blob.exists()
            print(f"[DEBUG] FAISS blob exists check: {faiss_exists}")
        except Exception as e:
            print(f"[ERROR] Failed to check FAISS blob existence: {str(e)}")
            faiss_exists = False
            
        try:
            pkl_exists = pkl_blob.exists()
            print(f"[DEBUG] PKL blob exists check: {pkl_exists}")
        except Exception as e:
            print(f"[ERROR] Failed to check PKL blob existence: {str(e)}")
            pkl_exists = False
            
        # Get metadata and sizes
        faiss_metadata = None
        pkl_metadata = None
        faiss_size = None
        pkl_size = None

        if faiss_exists:
            try:
                faiss_blob.reload()  # Refresh metadata
                faiss_size = faiss_blob.size
                faiss_metadata = {
                    'size': faiss_size,
                    'updated': faiss_blob.updated,
                    'md5_hash': faiss_blob.md5_hash,
                    'content_type': faiss_blob.content_type
                }
            except Exception as e:
                print(f"[ERROR] Failed to get FAISS metadata: {str(e)}")
                
        if pkl_exists:
            try:
                pkl_blob.reload()  # Refresh metadata
                pkl_size = pkl_blob.size
                pkl_metadata = {
                    'size': pkl_size,
                    'updated': pkl_blob.updated,
                    'md5_hash': pkl_blob.md5_hash,
                    'content_type': pkl_blob.content_type
                }
            except Exception as e:
                print(f"[ERROR] Failed to get PKL metadata: {str(e)}")
        
        return {
            "location": "GCP Firebase Storage",
            "path": gcp_path,
            "faiss_exists": faiss_exists,
            "pkl_exists": pkl_exists,
            "faiss_size": faiss_size,
            "pkl_size": pkl_size,
            "faiss_metadata": faiss_metadata,
            "pkl_metadata": pkl_metadata,
            "bucket_info": {
                "name": bucket.name,
                "path": f"gs://{bucket.name}/{gcp_path}"
            }
        }
        
    except Exception as e:
        print(f"[ERROR] Top-level error in check_gcp_faiss: {str(e)}")
        return {
            "error": str(e),
            "location": "GCP Firebase Storage",
            "path": gcp_path if 'gcp_path' in locals() else None
        }

@app.get("/check-backend-dirs")
async def check_backend_dirs():
    """Check if critical backend directories exist and are writable."""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dirs_to_check = ['uploads', 'faiss', 'graphs', 'audio_sents']
    
    results = {}
    for dir_name in dirs_to_check:
        dir_path = os.path.join(base_dir, dir_name)
        exists = os.path.exists(dir_path)
        is_dir = os.path.isdir(dir_path) if exists else False
        is_writable = os.access(dir_path, os.W_OK) if exists else False
        
        # Try to list contents if directory exists
        contents = []
        if exists and is_dir:
            try:
                contents = os.listdir(dir_path)[:5]  # List up to 5 items
            except Exception as e:
                contents = [f"Error listing contents: {str(e)}"]
        
        results[dir_name] = {
            "exists": exists,
            "is_directory": is_dir,
            "is_writable": is_writable,
            "path": dir_path,
            "sample_contents": contents
        }
    
    return {
        "server_url": "https://ai-assistant.myddns.me:8443",
        "base_directory": base_dir,
        "directories": results
    }


@app.get("/check-faiss-content/{curriculum_id}")
async def check_faiss_content(curriculum_id: str):
    """Check the content of a FAISS index to verify it's working correctly."""
    try:
        # Initialize embeddings
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # Get the FAISS index path
        base_dir = os.path.dirname(os.path.abspath(__file__))
        idx_dir = os.path.join(base_dir, 'faiss', f'faiss_index_{curriculum_id}')
        
        if not os.path.exists(idx_dir):
            return {
                "status": "error",
                "message": f"FAISS index directory not found: {idx_dir}",
                "curriculum_id": curriculum_id
            }
            
        # Try to load the index
        try:
            vectors = FAISS.load_local(idx_dir, embeddings, allow_dangerous_deserialization=True)
            
            # Test search with a simple query
            test_query = "introduction"
            docs = vectors.similarity_search(test_query, k=2)
            
            # Extract sample content
            samples = []
            for doc in docs:
                samples.append({
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                })
            
            return {
                "status": "success",
                "curriculum_id": curriculum_id,
                "index_path": idx_dir,
                "vector_count": vectors.index.ntotal if hasattr(vectors, 'index') else None,
                "sample_query": test_query,
                "sample_results": samples
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Failed to load/query FAISS index: {str(e)}",
                "curriculum_id": curriculum_id,
                "index_path": idx_dir
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"Top-level error: {str(e)}",
            "curriculum_id": curriculum_id
        }

# ============================================================================
# PERFORMANCE MONITORING ENDPOINTS
# ============================================================================

@app.get("/performance-stats")
async def get_performance_stats():
    """Get detailed performance statistics including multi-modal processing"""
    return {
        "model_cache": {
            "initialized": model_cache.is_initialized,
            "models_loaded": [
                "SentenceTransformer" if model_cache.embedding_model else None,
                "HuggingFaceEmbeddings" if model_cache.huggingface_embeddings else None,
                "Tokenizer" if model_cache.tokenizer else None,
                "TextSplitter" if model_cache.text_splitter else None
            ]
        },
        "multi_modal_processing": multi_modal_processor.get_processing_stats(),
        "optimization_status": "‚úÖ FULLY OPTIMIZED - MULTI-MODAL CURRICULUM-BASED",
        "expected_performance": {
            "first_response": "1-2 seconds (was 10-18s)",
            "subsequent_responses": "0.5-1 seconds (was 3-5s)",
            "curriculum_processing": "background processing when chat starts",
            "file_upload_processing": "background processing immediately after upload",
            "image_upload_processing": "background processing immediately after upload"
        }
    }

@app.get("/curriculum-session-stats")
async def get_curriculum_session_stats():
    """Get statistics about active curriculum chat sessions"""
    return chat_session_manager.get_session_stats()

@app.post("/end-curriculum-chat/{chat_id}")
async def end_curriculum_chat_session(chat_id: str):
    """Call this when user exits chat to free memory"""
    success = await chat_session_manager.cleanup_chat_session(chat_id)
    if success:
        return {"message": f"Curriculum chat session {chat_id} cleaned up successfully"}
    else:
        return {"message": f"Curriculum chat session {chat_id} was not active"}

@app.post("/initialize-chat-session")
async def initialize_chat_session(
    curriculum_id: str = Query(..., description="Curriculum document ID"),
    chat_id: str = Query(..., description="Chat session identifier"),  # ‚Üê REQUIRED
    grade: str = Query(None, description="Grade level (e.g., Grade 10)"),
    subject: str = Query(None, description="Subject name (e.g., English, Math)"),
    activity: str = Query(None, description="Activity type"),
    user_id: str = Query(None, description="User identifier"),
    role: str = Query(None, description="User role (teacher/student)")
):
    """
    üöÄ FAST INITIALIZATION: Pre-load curriculum vectors for instant responses.
    Call this when chat starts, then all subsequent requests will be super fast!
    """
    print(f"[FAST-INIT] üöÄ Pre-initializing chat session:")
    print(f"  Chat ID: {chat_id}")
    print(f"  Curriculum: {curriculum_id}")
    print(f"  User: {user_id} ({role})")
    
    try:
        # Start background processing immediately
        start_time = time.time()
        
        # This will load vectors in background if not already loaded
        asyncio.create_task(
            chat_session_manager.preprocess_curriculum_in_background(curriculum_id)
        )
        
        # Pre-load vectors for this specific chat session (SYNCHRONOUS)
        vectors = await chat_session_manager.get_vectors_for_chat(chat_id, curriculum_id)
        
        initialization_time = time.time() - start_time
        
        # Store session context for future use
        session_context = {
            "curriculum_id": curriculum_id,
            "chat_id": chat_id,
            "grade": grade,
            "subject": subject,
            "activity": activity,
            "user_id": user_id,
            "role": role,
            "initialized_at": time.time(),
            "initialization_time": initialization_time,
            "vectors_loaded": True,
            "vector_count": vectors.index.ntotal if hasattr(vectors, 'index') else None
        }
        
        print(f"[FAST-INIT] ‚úÖ Chat session initialized in {initialization_time:.3f}s")
        
        return {
            "status": "initialized",
            "message": f"Chat session pre-loaded! Next responses will be instant.",
            "session_context": session_context,
            "performance": {
                "initialization_time": f"{initialization_time:.3f}s",
                "expected_response_time": "0.1-0.3s (was 0.5-1.5s)",
                "optimization": "vectors pre-loaded"
            }
        }
        
    except Exception as e:
        print(f"[FAST-INIT] ‚ùå Initialization failed: {e}")
        return {
            "status": "failed",
            "error": str(e),
            "message": "Initialization failed, responses will use fallback loading"
        }

@app.post("/preprocess-curriculum/{curriculum_id}")
async def preprocess_curriculum_background(curriculum_id: str):
    """Manually trigger background preprocessing of a curriculum"""
    asyncio.create_task(
        chat_session_manager.preprocess_curriculum_in_background(curriculum_id)
    )
    return {"message": f"Background preprocessing started for curriculum {curriculum_id}"}

@app.get("/multi-modal-stats")
async def get_multi_modal_stats():
    """Get detailed multi-modal processing statistics"""
    return multi_modal_processor.get_processing_stats()

@app.post("/preprocess-file")
async def preprocess_file_manual(
    file_hash: str = Query(..., description="MD5 hash of the file content"),
    file_type: str = Query("pdf", description="File type (pdf, docx)")
):
    """Manually trigger background preprocessing of an uploaded file"""
    # This would require the file content to be available
    return {"message": "Use upload endpoints for automatic background processing"}

@app.post("/preprocess-image")
async def preprocess_image_manual(
    image_hash: str = Query(..., description="MD5 hash of the image content")
):
    """Manually trigger background preprocessing of an uploaded image"""
    # This would require the image content to be available
    return {"message": "Use upload endpoints for automatic background processing"}

@app.post("/cleanup-old-cache")
async def cleanup_old_cache(max_age_hours: int = Query(24, description="Maximum age in hours")):
    """Clean up old cached files and images"""
    await multi_modal_processor.cleanup_old_cache(max_age_hours)
    return {"message": f"Cleaned up cache older than {max_age_hours} hours"}

@app.get("/")
async def root():
    """Root endpoint with links to available tools."""
    return HTMLResponse("""
    <h2>üöÄ AI Assistant API - FULLY OPTIMIZED Multi-Modal Curriculum-Based RAG</h2>
    <h3>‚ö° Performance Status</h3>
    <ul>
        <li><strong>‚úÖ FULLY OPTIMIZED</strong> - Models pre-loaded at startup</li>
        <li><strong>üîÑ Multi-Modal Background Processing</strong> - Curriculums, files, and images processed instantly</li>
        <li><strong>‚ö° Response Time</strong> - 1-2s (was 10-18s)</li>
        <li><strong>üß† Memory Efficient</strong> - Smart caching for all content types</li>
        <li><strong>üìö Curriculum Support</strong> - Perfect for 200+ curriculums</li>
        <li><strong>üìÑ File Processing</strong> - PDF/DOCX background processing</li>
        <li><strong>üñºÔ∏è Image Processing</strong> - Vision + OCR background processing</li>
    </ul>
    <h3>üí¨ Multi-Modal Session Management</h3>
    <ul>
        <li><a href="/performance-stats">üìä Complete Performance Statistics</a></li>
        <li><a href="/multi-modal-stats">üéØ Multi-Modal Processing Stats</a></li>
        <li><a href="/curriculum-session-stats">Active Curriculum Chat Sessions</a></li>
        <li><a href="/cleanup-inactive-sessions" onclick="return confirm('Clean up inactive sessions?')">Cleanup Inactive Sessions (POST)</a></li>
        <li><a href="/cleanup-old-cache" onclick="return confirm('Clean up old cache?')">Cleanup Old Cache (POST)</a></li>
    </ul>
    <h3>üîß System Management</h3>
    <ul>
        <li><a href="/clear-cache" onclick="return confirm('Are you sure? This will clear all active chat sessions.')">Clear All Sessions (POST)</a></li>
    </ul>
    <h3>üìö Content Processing</h3>
    <ul>
        <li><a href="/docs#/default/upload_file_upload_file_post">üìÑ Upload File (with background processing)</a></li>
        <li><a href="/docs#/default/upload_image_upload_image_post">üñºÔ∏è Upload Image (with background processing)</a></li>
        <li><a href="/docs#/default/curriculum_url_curriculum_url_get">Get Curriculum URL</a></li>
        <li><a href="/docs#/default/preprocess_curriculum_background_preprocess_curriculum__curriculum_id__post">Pre-process Curriculum</a></li>
    </ul>
    <h3>üõ†Ô∏è System Tools</h3>
    <ul>
        <li><a href="/frontend/index.html">Frontend UI</a></li>
        <li><a href="/check-backend-dirs">Check Backend Directories</a></li>
        <li><a href="/check-faiss-content/Dcul12T4b7uTG5xGqtEp">Check FAISS Index Content (Example)</a></li>
    </ul>
    <h3>üìä Optimization Status</h3>
    <p><strong>‚úÖ Chat-Session Based RAG:</strong> Perfect for 200+ curriculums</p>
    <p><strong>‚ö° Performance:</strong> Fast responses within each chat session</p>
    <p><strong>üß† Memory Efficient:</strong> Only active chats use RAM (~150MB each)</p>
    <p><strong>üîÑ Auto-Cleanup:</strong> Inactive sessions cleaned every 30 minutes</p>
    """)

@app.options("/{path:path}")
async def options_route(path: str):
    """Handle OPTIONS requests for CORS preflight."""
    response = JSONResponse(content={"status": "ok"})
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization"
    return response
@app.post("/test-arabic-storage")
async def test_arabic_storage(request: Request):
    """
    Test endpoint to verify that Arabic text is stored and retrieved correctly.
    
    POST body should be JSON with:
    {
        "user_id": "test_user_id",
        "arabic_text": "ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ŸÑŸÑÿßÿÆÿ™ÿ®ÿßÿ±"
    }
    """
    try:
        data = await request.json()
        user_id = data.get("user_id", f"test_user_{uuid.uuid4().hex[:8]}")
        arabic_text = data.get("arabic_text", "Ÿáÿ∞ÿß ŸÜÿµ ÿπÿ±ÿ®Ÿä ŸÑŸÑÿßÿÆÿ™ÿ®ÿßÿ±")
        
        # Log the input
        print(f"[ARABIC TEST] Input text: {arabic_text}")
        print(f"[ARABIC TEST] Input text type: {type(arabic_text)}")
        
        # Create a test document with the Arabic text
        test_doc_id = f"arabic_test_{uuid.uuid4().hex[:8]}"
        test_ref = db.collection("arabic_test_collection").document(test_doc_id)
        
        # Store the text directly
        test_ref.set({
            "user_id": user_id,
            "arabic_text": arabic_text
        })
        
        # Retrieve the document to verify
        doc = test_ref.get()
        retrieved_data = doc.to_dict()
        retrieved_text = retrieved_data.get("arabic_text", "")
        
        # Log the retrieved text
        print(f"[ARABIC TEST] Retrieved text: {retrieved_text}")
        print(f"[ARABIC TEST] Retrieved text type: {type(retrieved_text)}")
        
        # Check if the text matches
        is_match = arabic_text == retrieved_text
        
        # Return the results
        return {
            "success": True,
            "original_text": arabic_text,
            "retrieved_text": retrieved_text,
            "is_match": is_match,
            "test_doc_id": test_doc_id
        }
    except Exception as e:
        print(f"[ARABIC TEST ERROR] {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }
@app.get("/verify-arabic-data/{collection}/{document_id}")
async def verify_arabic_data(collection: str, document_id: str):
    """
    Verify that Arabic data in a specific document is stored correctly.
    
    Parameters:
    - collection: The Firestore collection name (e.g., "chat_details_ar", "history_chat_backend_speech")
    - document_id: The document ID to check
    
    Returns the document data with any Arabic text found.
    """
    try:
        # Get the document
        doc_ref = db.collection(collection).document(document_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            return {
                "success": False,
                "error": f"Document {document_id} not found in collection {collection}"
            }
        
        data = doc.to_dict()
        
        # For chat history collections, extract the history array
        if "history" in data:
            # Get the last 5 entries or fewer if there are less
            history = data.get("history", [])
            recent_entries = history[-5:] if len(history) > 5 else history
            
            # Extract content from each entry
            entries_with_content = []
            for entry in recent_entries:
                if isinstance(entry, dict) and "content" in entry:
                    entries_with_content.append({
                        "role": entry.get("role", "unknown"),
                        "content": entry.get("content", ""),
                        "content_type": type(entry.get("content", "")).__name__
                    })
            
            return {
                "success": True,
                "collection": collection,
                "document_id": document_id,
                "recent_entries": entries_with_content,
                "total_entries": len(history)
            }
        
        # For speech history collection
        elif collection == "history_chat_backend_speech":
            history = data.get("history", [])
            recent_entries = history[-5:] if len(history) > 5 else history
            
            entries_with_qa = []
            for entry in recent_entries:
                if isinstance(entry, dict):
                    entries_with_qa.append({
                        "question": entry.get("question", ""),
                        "question_type": type(entry.get("question", "")).__name__,
                        "answer": entry.get("answer", ""),
                        "answer_type": type(entry.get("answer", "")).__name__
                    })
            
            return {
                "success": True,
                "collection": collection,
                "document_id": document_id,
                "recent_entries": entries_with_qa,
                "total_entries": len(history)
            }
        
        # For other collections, return the raw data
        return {
            "success": True,
            "collection": collection,
            "document_id": document_id,
            "data": data
        }
    except Exception as e:
        print(f"[VERIFY ERROR] {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }
if __name__ == "__main__":
    import uvicorn
    print("Starting server with HTTPS on https://0.0.0.0:8443")
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8443,
        ssl_keyfile="/etc/letsencrypt/live/ai-assistant.myddns.me/privkey.pem",
        ssl_certfile="/etc/letsencrypt/live/ai-assistant.myddns.me/fullchain.pem"
    )
def fix_text_encoding(text: str) -> str:
    """Try to fix incorrectly encoded text, especially Arabic."""
    try:
        # Try to fix double-encoded text
        return text.encode('latin1').decode('utf-8')
    except:
        try:
            # Try direct UTF-8 decoding
            return text.encode('utf-8').decode('utf-8')
        except:
            # Return original if both attempts fail
            return text

@app.get("/check-faiss-content/{curriculum_id}")
async def check_faiss_content(curriculum_id: str):
    """Check the content of a FAISS index to verify it's working correctly."""
    try:
        # Initialize embeddings
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # Get the FAISS index path
        base_dir = os.path.dirname(os.path.abspath(__file__))
        idx_dir = os.path.join(base_dir, 'faiss', f'faiss_index_{curriculum_id}')
        
        if not os.path.exists(idx_dir):
            return {
                "status": "error",
                "message": f"FAISS index directory not found: {idx_dir}",
                "curriculum_id": curriculum_id
            }
            
        # Try to load the index
        try:
            vectors = FAISS.load_local(idx_dir, embeddings, allow_dangerous_deserialization=True)
            
            # Test searches with different queries
            test_queries = ["introduction", "ŸÖŸÇÿØŸÖÿ©", "ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™", "mathematics"]
            all_samples = {}
            
            for query in test_queries:
                docs = vectors.similarity_search(query, k=2)
                samples = []
                for doc in docs:
                    # Try to fix encoding
                    content = fix_text_encoding(doc.page_content)
                    
                    samples.append({
                        "content": content[:200] + "...",
                        "content_bytes": content[:200].encode('utf-8').hex(),
                        "metadata": doc.metadata
                    })
                all_samples[query] = samples
            
            return {
                "status": "success",
                "curriculum_id": curriculum_id,
                "index_path": idx_dir,
                "vector_count": vectors.index.ntotal if hasattr(vectors, 'index') else None,
                "sample_results": all_samples,
                "encoding_info": {
                    "python_default": sys.getdefaultencoding(),
                    "file_system": sys.getfilesystemencoding()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Failed to load/query FAISS index: {str(e)}",
                "curriculum_id": curriculum_id,
                "index_path": idx_dir
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"Top-level error: {str(e)}",
            "curriculum_id": curriculum_id
        }

@app.get("/test-cors")
async def test_cors():
    """Test endpoint for CORS."""
    return {"message": "CORS is working!"}

@app.get("/test-weblink-filtering")
async def test_weblink_filtering(
    query: str = Query("machine learning", description="Search query to test"),
    language: str = Query("en", description="Language (en/ar)"),
    include_youtube: bool = Query(False, description="Whether to include YouTube in query")
):
    """Test endpoint to verify weblink filtering works correctly"""
    try:
        # Modify query to include YouTube request if needed
        test_query = f"{query} youtube video" if include_youtube else query
        
        result = await generate_weblink_perplexity(test_query, language)
        
        # Also test the filtering functions directly
        test_links = [
            {"url": "https://youtube.com/watch?v=123", "title": "YouTube Video", "summary": "A video"},
            {"url": "https://wikipedia.org/wiki/test", "title": "Wikipedia", "summary": "Wiki article"},
            {"url": "", "title": "Empty URL", "summary": "Should be filtered"},
            {"url": "null", "title": "Null URL", "summary": "Should be filtered"},
            {"url": "https://ar.wikipedia.org/wiki/test", "title": "Arabic Wiki", "summary": "ŸÖŸÇÿßŸÑ ÿπÿ±ÿ®Ÿä"}
        ]
        
        # Test filtering functions
        cleaned = validate_and_clean_links(test_links)
        youtube_filtered = filter_youtube_links(cleaned, include_youtube)
        
        # Test Arabic detection
        arabic_results = []
        for link in cleaned:
            is_arabic = is_arabic_content(link['url'], link['title'], link.get('summary', ''))
            arabic_results.append({
                "url": link['url'],
                "is_arabic": is_arabic
            })
        
        return {
            "query": test_query,
            "language": language,
            "include_youtube": include_youtube,
            "perplexity_result": result,
            "test_filtering": {
                "original_count": len(test_links),
                "after_cleaning": len(cleaned),
                "after_youtube_filter": len(youtube_filtered),
                "arabic_detection": arabic_results
            }
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "query": query,
            "language": language
        }

@app.get("/test-tts")
async def test_tts(text: str = Query("Hello, this is a test"), language: str = Query("en")):
    """Test TTS generation - returns audio URL"""
    try:
        audio_url = await generate_complete_tts(text, language)
        return {
            "status": "success",
            "text": text,
            "language": language,
            "audio_url": audio_url
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }


